Automatically generated by Mendeley Desktop 1.19.8
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{Katt2018,
abstract = {The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BAPOMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BAPOMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.},
author = {Katt, Sammie and Oliehoek, Frans A and Amato, Christopher},
booktitle = {Proceedings of The 34th International Conference on Machine Learning},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Katt, Oliehoek, Amato - 2018 - Learning in POMDPs with monte carlo tree search.pdf:pdf},
issn = {23318422},
title = {{Learning in POMDPs with monte carlo tree search}},
year = {2018}
}
@misc{Bedi2009,
author = {Bedi, P and Gandotra, V and Singhal, A and Vats, V and Mishra, N},
doi = {10.1007/978-3-642-04441-0_62},
pages = {709--719},
publisher = {Springer, Berlin, Heidelberg},
title = {{Avoiding Threats Using Multi Agent System Planning for Web Based Systems}},
year = {2009}
}
@article{Mizrahi2021,
abstract = {In tacit coordination games people manage to converge on prominent solutions, which are known as focal points. There is still no accepted explanation of how players manage to converge on the same solution. It could be that the limited explanatory power arises from the fact that existing theories rely on pure strategies to describe behaviour. The aim of the current study is to construct a cognitive model that more accurately describes human behaviour in tacit coordination games. To this end we constructed individual strategic profiles that take into account the subjective preferences of individual players regarding the prominent selection rules. Subsequently, the individual profiles were clustered to gain insights regarding different types of coordinators. By using machine learning and statistical methods we were able to demonstrate, for the first time, the relationship between different types of strategic profiles and coordination ability. The results of this study demonstrate the importance of constructing a descriptive behavioural model that may potentially improve prediction of human decision-making in the context of human-machine interaction.},
author = {Mizrahi, Dor and Laufer, Ilan and Zuckerman, Inon},
doi = {10.1080/0952813X.2020.1721572},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Mizrahi, Laufer, Zuckerman - 2021 - Individual strategic profiles in tacit coordination games.pdf:pdf},
issn = {13623079},
journal = {Journal of Experimental and Theoretical Artificial Intelligence},
keywords = {Decision-making,focal points,level-k,tacit coordination games},
month = {jan},
number = {1},
pages = {63--78},
publisher = {Taylor and Francis Ltd.},
title = {{Individual strategic profiles in tacit coordination games}},
url = {https://www.tandfonline.com/doi/full/10.1080/0952813X.2020.1721572},
volume = {33},
year = {2021}
}
@inproceedings{Baker_bayesian_theories_of_mind,
author = {Baker, C and Saxe, R and Tenenbaum, J},
booktitle = {Proceedings of the 33th Annual Meeting of the Cognitive Science Society, CogSci 2011, Boston, Massachusetts, USA, July 20--23},
title = {{Bayesian Theory of Mind: Modeling Joint Belief-Desire Attribution}},
year = {2011}
}
@inproceedings{Shvo2020,
abstract = {Theory of Mind is commonly defined as the ability to attribute mental states (e.g., beliefs, goals) to oneself, and to others. A large body of previous work—from the social sciences to artificial intelligence—has observed that Theory of Mind capabilities are central to providing an explanation to another agent or when explaining that agent's behaviour. In this paper, we build and expand upon previous work by providing an account of explanation in terms of the beliefs of agents and the mechanism by which agents revise their beliefs given possible explanations. We further identify a set of desiderata for explanations that utilize Theory of Mind. These desiderata inform our belief-based account of explanation.},
archivePrefix = {arXiv},
arxivId = {2005.02963},
author = {Shvo, Maayan and Klassen, Toryn Q. and McIlraith, Sheila A.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-51924-7_5},
eprint = {2005.02963},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Shvo, Klassen, McIlraith - 2020 - Towards the Role of Theory of Mind in Explanation.pdf:pdf},
isbn = {9783030519230},
issn = {16113349},
pages = {75--93},
publisher = {Springer},
title = {{Towards the Role of Theory of Mind in Explanation}},
volume = {12175 LNAI},
year = {2020}
}
@inproceedings{Alzetta2020a,
abstract = {In the race for automation, distributed systems are required to perform increasingly complex reasoning to deal with dynamic tasks, often not controlled by humans. On the one hand, systems dealing with strict-timing constraints in safety-critical applications mainly focused on predictability, leaving little room for complex planning and decision-making processes. Indeed, real-time techniques are very efficient in predetermined, constrained, and controlled scenarios. Nevertheless, they lack the necessary flexibility to operate in evolving settings, where the software needs to adapt to the changes of the environment. On the other hand, Intelligent Systems (IS) increasingly adopted Machine Learning (ML) techniques (e.g., subsymbolic predictors such as Neural Networks). The seminal application of those IS started in zero-risk domains producing revolutionary results. However, the ever-increasing exploitation of ML-based approaches generated opaque systems, which are nowadays no longer socially acceptable—calling for eXplainable AI (XAI). Such a problem is exacerbated when IS tend to approach safety-critical scenarios. This paper highlights the need for on-time explainability. In particular, it proposes to embrace the Real-Time Beliefs Desires Intentions (RT-BDI) framework as an enabler of eXplainable Multi-Agent Systems (XMAS) in time-critical XAI.},
author = {Alzetta, F and Giorgini, P and Najjar, Amro and Schumacher, M I and Calvaresi, D},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-51924-7_3},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Alzetta et al. - 2020 - In-Time Explainability in Multi-Agent Systems Challenges, Opportunities, and Roadmap.pdf:pdf},
isbn = {9783030519230},
issn = {16113349},
keywords = {Multi-Agent Systems,Real-Time Systems,eXplainable BDI model,eXplainable autonomous agents},
pages = {39--53},
publisher = {Springer},
title = {{In-Time Explainability in Multi-Agent Systems: Challenges, Opportunities, and Roadmap}},
volume = {12175 LNAI},
year = {2020}
}
@techreport{Ramirez2009,
abstract = {In this work we aim to narrow the gap between plan recognition and planning by exploiting the power and generality of recent planning algorithms for recognizing the set G * of goals G that explain a sequence of observations given a domain theory. After providing a crisp definition of this set, we show by means of a suitable problem transformation that a goal G belongs to G * if there is an action sequence $\pi$ that is an optimal plan for both the goal G and the goal G extended with extra goals representing the observations. Exploiting this result, we show how the set G * can be computed exactly and approximately by minor modifications of existing optimal and suboptimal planning algorithms, and existing polynomial heuristics. Experiments over several domains show that the suboptimal planning algorithms and the polynomial heuristics provide good approximations of the optimal goal set G * while scaling up as well as state-of-the-art planning algorithms and heuristics.},
author = {Ram{\'{i}}rez, M and Geffner, H},
booktitle = {cs.engr.uky.edu},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Ram{\'{i}}rez, Geffner - 2009 - Plan Recognition as Planning.pdf:pdf},
keywords = {Plan Recognition,Planning,Planning and Scheduling},
title = {{Plan Recognition as Planning}},
url = {http://cs.engr.uky.edu/$\sim$sgware/reading/papers/ramirez2009plan.pdf},
year = {2009}
}
@inproceedings{Anthony2017,
abstract = {Sequential decision making problems, such as structured prediction, robotic control, and game playing, require a combination of planning policies and generalisation of those plans. In this paper, we present Expert Iteration (EXIT), a novel reinforcement learning algorithm which decomposes the problem into separate planning and generalisation tasks. Planning new policies is performed by tree search, while a deep neural network generalises those plans. Subsequently, tree search is improved by using the neural network policy to guide search, increasing the strength of new plans. In contrast, standard deep Reinforcement Learning algorithms rely on a neural network not only to generalise plans, but to discover them too. We show that EXIT outperforms REINFORCE for training a neural network to play the board game Hex, and our final tree search agent, trained tabula rasa, defeats MOHEX 1.0, the most recent Olympiad Champion player to be publicly released.},
archivePrefix = {arXiv},
arxivId = {1705.08439},
author = {Anthony, Thomas and Tian, Zheng and Barber, David},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1705.08439},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Anthony, Tian, Barber - 2017 - Thinking fast and slow with deep learning and tree search.pdf:pdf},
issn = {10495258},
pages = {5361--5371},
title = {{Thinking fast and slow with deep learning and tree search}},
volume = {2017-Decem},
year = {2017}
}
@techreport{Ossowski2007,
abstract = {One of the major challenges in the multi-agent systems field is to build systems capable of taking decisions in an autonomous and flexible way, and to cooperate with other systems inside a virtual organization. If this virtual organization is to be applied successfully to real-world problems, it needs to be flexible enough to cope with the openness of many domains. In particular, a variety of problems need to be addressed such as: large-scale a priori distribution, constant evolution of the environment, admission and departure of members, management and adaptation of the organizational structure, requirements and limitations imposed by the (mobile) devices that support (part of) the execution of the virtual organization and its members, etc. This paper identifies and analyses open research issues that need to be addressed to develop truly open multi-agent systems based on virtual organizations, through the analysis of a real-world case study. Moreover, it puts forward an abstract architecture proposal aimed at integrating methods and techniques in order to address these challenges.},
author = {Ossowski, S and Juli{\'{a}}n, V and Bajo, J and Billhardt, H and Botti, V and Corchado, J M},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Ossowski et al. - 2007 - Open MAS for Real World Applications An Abstract Architecture Proposal.pdf:pdf},
keywords = {Multi-agent systems},
title = {{Open MAS for Real World Applications: An Abstract Architecture Proposal}},
url = {https://gredos.usal.es/handle/10366/135020},
year = {2007}
}
@techreport{ferguson1992touringmachines,
author = {Ferguson, I A},
institution = {University of Cambridge, Computer Laboratory},
title = {{TouringMachines: An architecture for dynamic, rational, mobile agents}},
year = {1992}
}
@phdthesis{RicciCo-supervisor2019,
author = {{Ricci Co-supervisor}, Alessandro and {Pau Author}, Giovanni and Bosello, Michael},
booktitle = {amslaurea.unibo.it},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Ricci Co-supervisor, Pau Author, Bosello - 2019 - Integrating BDI and Reinforcement Learning the Case Study of Autonomous Driving.pdf:pdf},
title = {{Integrating BDI and Reinforcement Learning: the Case Study of Autonomous Driving}},
url = {https://amslaurea.unibo.it/21467/1/master-thesis.pdf},
year = {2019}
}
@article{Cog_arch_AGI,
author = {Lieto, A and Bhatt, M and Oltramari, A and Vernon, D},
doi = {10.1016/j.cogsys.2017.08.003},
journal = {Cognitive Systems Research},
title = {{The Role of Cognitive Architectures in General Artificial Intelligence}},
year = {2017}
}
@article{GOERTZEL201030,
abstract = {A number of leading cognitive architectures that are inspired by the human brain, at various levels of granularity, are reviewed and compared, with special attention paid to the way their internal structures and dynamics map onto neural processes. Four categories of Biologically Inspired Cognitive Architectures (BICAs) are considered, with multiple examples of each category briefly reviewed, and selected examples discussed in more depth: primarily symbolic architectures (e.g. ACT-R), emergentist architectures (e.g. DeSTIN), developmental robotics architectures (e.g. IM-CLEVER), and our central focus, hybrid architectures (e.g. LIDA, CLARION, 4D/RCS, DUAL, MicroPsi, and OpenCog). Given the state of the art in BICA, it is not yet possible to tell whether emulating the brain on the architectural level is going to be enough to allow rough emulation of brain function; and given the state of the art in neuroscience, it is not yet possible to connect BICAs with large-scale brain simulations in a thoroughgoing way. However, it is nonetheless possible to draw reasonably close function connections between various components of various BICAs and various brain regions and dynamics, and as both BICAs and brain simulations mature, these connections should become richer and may extend further into the domain of internal dynamics as well as overall behavior.},
annote = {Artificial Brains},
author = {Goertzel, B and Lian, R and Arel, I and Garis, De and Chen, S},
doi = {https://doi.org/10.1016/j.neucom.2010.08.012},
issn = {0925-2312},
journal = {Neurocomputing},
keywords = {Artificial brains,Cognitive architectures},
number = {1},
pages = {30--49},
title = {{A world survey of artificial brain projects, Part II: Biologically inspired cognitive architectures}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231210003498},
volume = {74},
year = {2010}
}
@misc{Gray2020,
abstract = {Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via external regret minimization. External regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and achieves a rank of 23 out of 1,128 human players when playing anonymous games on a popular Diplomacy website.},
archivePrefix = {arXiv},
arxivId = {2010.02923},
author = {Gray, Jonathan and Lerer, Adam and Bakhtin, Anton and Brown, Noam},
booktitle = {arXiv},
eprint = {2010.02923},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Gray et al. - 2020 - Human-level performance in no-press diplomacy via equilibrium search.pdf:pdf},
isbn = {2010.02923v2},
issn = {23318422},
title = {{Human-level performance in no-press diplomacy via equilibrium search}},
year = {2020}
}
@article{Yang2012BehaviorBL,
author = {Yang, S and Paddrik, M and Hayes, R and Todd, A and Kirilenko, A and Beling, P and Scherer, W},
journal = {2012 IEEE Conference on Computational Intelligence for Financial Engineering & Economics (CIFEr)},
pages = {1--8},
title = {{Behavior based learning in identifying High Frequency Trading strategies}},
year = {2012}
}
@misc{Turchetta2019,
abstract = {In Interactive Machine Learning (IML), we iteratively make decisions and obtain noisy observations of an unknown function. While IML methods, e.g., Bayesian optimization and active learning, have been successful in applications, on realworld systems they must provably avoid unsafe decisions. To this end, safe IML algorithms must carefully learn about a priori unknown constraints without making unsafe decisions. Existing algorithms for this problem learn about the safety of all decisions to ensure convergence. This is sample-inefficient, as it explores decisions that are not relevant for the original IML objective. In this paper, we introduce a novel framework that renders any existing unsafe IML algorithm safe. Our method works as an add-on that takes suggested decisions as input and exploits regularity assumptions in terms of a Gaussian process prior in order to efficiently learn about their safety. As a result, we only explore the safe set when necessary for the IML problem. We apply our framework to safe Bayesian optimization and to safe exploration in deterministic Markov Decision Processes (MDP), which have been analyzed separately before. Our method outperforms other algorithms empirically.},
archivePrefix = {arXiv},
arxivId = {1910.13726},
author = {Turchetta, Matteo and Berkenkamp, Felix and Krause, Andreas},
booktitle = {arXiv},
eprint = {1910.13726},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Turchetta, Berkenkamp, Krause - 2019 - Safe exploration for interactive machine learning.pdf:pdf},
issn = {23318422},
title = {{Safe exploration for interactive machine learning}},
year = {2019}
}
@inproceedings{Helgason2012,
abstract = {Many existing AGI architectures are based on the assumption of infinite computational resources, as researchers ignore the fact that real-world tasks have time limits, and managing these is a key part of the role of intelligence. In the domain of intelligent systems the management of system resources is typically called "attention". Attention mechanisms are necessary because all moderately complex environments are likely to be the source of vastly more information than could be processed in realtime by an intelligence's available cognitive resources. Even if sufficient resources were available, attention could help make better use of them. We argue that attentional mechanisms are not only nice to have, for AGI architectures they are an absolute necessity. We examine ideas and concepts from cognitive psychology for creating intelligent resource management mechanisms and how these can be applied to engineered systems. We present a design for a general attention mechanism intended for implementation in AGI architectures. {\textcopyright} 2012 Springer-Verlag.},
annote = {Attention: cognitive resource management. 

Top down (goal oriented) and bottom up (reactive) --- cocktail party effect (Cherry 1953).

Knudson attnetion framework: (cognitive psychology): working memory, top down sensitivity control, bottom up filtering and competetive selection. 

AGIS THAT IMPLIMENT ATTENTION: NARS, LIDA AND CLARION. 
NARS: meant for general purpose reasoning, with insufficient reources and time as a given. Has a work prioritisation mechanism: urgency and durability. 

LIDA has a competetive selection mechanism, in which tasks, called codelets compete for resources (attention). It is learnable (i.e. rewards from attending to things), and hence it can improve over time. 

Highlights that predictive capabilities are necessary for attention (i.e. the future of a task requires it ot have attention now). 

Also argue for internal as well as external control: i.e. resource allocation. 

Their mechanism is dependent on activation and salience: activation refers to a process (highly active requirements etc) and salience with resepect to the importance of data. They adjust activation of processes and the salience of data to perform optimally....

Looking at AGI- attention is important in systems, but needs to be built for it- retrofitting an existing AGI might not be feasible (constructivist area).},
author = {Helgason, H and Nivel, E and Th{\'{o}}risson, K R},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-35506-6_10},
isbn = {9783642355059},
issn = {03029743},
keywords = {architecture,artificial intelligence,attention,cognition,resource management,system design},
pages = {89--98},
publisher = {Springer, Berlin, Heidelberg},
title = {{On attention mechanisms for AGI architectures: A design proposal}},
volume = {7716 LNAI},
year = {2012}
}
@inproceedings{Albrecht_stone_2019,
abstract = {Agents can achieve effective interaction with previously unknown other agents by maintaining beliefs over a set of hypothetical behaviours, or types, that these agents may have. A current limitation in this method is that it does not recognise parameters within type specifications, because types are viewed as blackbox mappings from interaction histories to probability distributions over actions. In this work, we propose a general method which allows an agent to reason about both the relative likelihood of types and the values of any bounded continuous parameters within types. The method maintains individual parameter estimates for each type and selectively updates the estimates for some types after each observation. We propose different methods for the selection of types and the estimation of parameter values. The proposed methods are evaluated in detailed experiments, showing that updating the parameter estimates of a single type after each observation can be sufficient to achieve good performance.},
annote = {Addresses a weakness in type-based reasoing: that types do not contain parameters.

This causes problems: you need n different types and various parameters within, and need to update the parameters. 
The black-box nature of opponent modelling means that evaluating the likelihood is very difficult: how does one know if the type is wrong or the parameters within a type are wrong? 

The goal is to specify types, and the ability but not the need to specify parameter updates, and selectively update types.... emphasis on selectively... 

Generic type-based reasoning: types are pre-specified to the modelling agent. 

Markovian parameters are commonly assumed: this doesn't hold. E.g. Q learnin agents internal-states aren't memoryless: they update based on internal parameters, hence to be updated properly, if an internal parameter has been historically incorrect, updating the internal state is horrific and needs to be recomputed from scratch. 

So, they way it works: it maintains parameter estimates for all types. Then, based on observations, it selectively updates parameter estimates for the most likely types. (i.e. limiting only the necessary updates... if i think that a car is driven by an old lady I don't need to update the internal hypothetical parameter for what an old lady mifht do on a scooter...). 

They select types by 1) sampling approach based on beliefs. 
2) Bandit Selection: Selecting types that will require the largest leaps to converge: it is assumed that parameter updates will be correct (gradient ascent), so basically, it works best to explore and update types which allow for the most widespread and largest convergence to the correct values... (i.e. a more exploration-style approach). Basically frame it as a mutli-armed bandit approach. 

-------
They update parameter types by 1) Approximate gradient ascent, 
2) Approximate Bayesian updating, 
3) Exact Global optimisation (gaussian process estimator). 
---

They use the level-based foraging domain with one other agent, of 4 different hypothetical types, each with a simple template, allowign for the same number of parameters, where they differ bases on which targets (i.e. items) they choose to hit first: first two are leaders, the second two are followers (i.e. they take more dominant strategies, and assume the other agent will play along, or assume the other agent to be more intelligent, and attempt to play along with their strategy respectively). 

They tested a range of algorithms for each component (type selection and parameter update), which in turn allowed for various possible sets of results. 

Action selection was then computed using MCTS rollouts to evaluate the best possible options.},
archivePrefix = {arXiv},
arxivId = {1906.11064},
author = {Albrecht, S V and Stone, P},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
eprint = {1906.11064},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Albrecht, Stone - 2017 - Reasoning about hypothetical agent behaviours and their parameters.pdf:pdf},
isbn = {9781510855076},
issn = {15582914},
keywords = {Ad hoc teamwork,Agent types,Parameter learning},
pages = {547--555},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)},
title = {{Reasoning about hypothetical agent behaviours and their parameters}},
volume = {1},
year = {2017}
}
@article{HRI-emotionPrediction,
author = {Gunes, H and Celiktutan, O and Sariyanidi, E},
journal = {Philosophical Transactions of the Royal Society B},
number = {1771},
pages = {20180026},
publisher = {The Royal Society},
title = {{Live human--robot interactive public demonstrations with automatic emotion and personality prediction}},
volume = {374},
year = {2019}
}
@inproceedings{Ganzfried2011,
abstract = {We develop an algorithm for opponent modeling in large extensive-form games of imperfect information. It works by observing the opponent's action frequencies and building an opponent model by combining information from a precom- puted equilibrium strategy with the observations. It then computes and plays a best response to this opponent model; the opponent model and best response are both updated continually in real time. The approach combines game- theoretic reasoning and pure opponent modeling, yielding a hybrid that can effectively exploit opponents after only a small number of interactions. Unlike prior opponent modeling approaches, ours is fundamentally game theoretic and takes advantage of recent algorithms for automated abstraction and equilibrium computation rather than relying on domain-specific prior distributions, historical data, or a handcrafted set of features. Experiments show that our algorithm leads to significantly higher win rates (than an approximate- equilibrium strategy) against several opponents in limit Texas Hold'em - the most studied imperfect-information game in computer science - including competitors from recent AAAI computer poker competitions. Copyright {\textcopyright} 2011, International Foundation for Autonomous Agents and Multiagent Systems.},
annote = {Makes note that often in OM, a large amount of work has gone into finding equilibria --- i.e. unexploitable points against a near-optimal opponent. 

The problem is that often opponents aren't optimal, and better results can be achieved against non-optimal opponents.

Meets the criteria of swift: little to no infromation about the opponent prior to playing, and computing strategies online (efficiently). 

They suggest an equilibrium based-strategy, which precomputes an approximate equilibrium solution, and then deviates from it depending on the opponents strategy profile. 

N.B. the concept of abstraction: many real-world games have no easily-computable equilibrium: state spaces are too large and or complex. 
The standrard approach involves computing an abstraction algorithm: a smaller game that is similar to the original game is contstructed. The smaller game is solved, and it's solution is mapped to a strategy profile in a larger game. 

Many ab algos work by coursening the game of chance, collapsing several information sets into single information sets in the abstracted game. This is sometimes referred to as bucketing. Basically using this method, one can change a computation in 10^18 states, which usually takjes about 2-3 weeks to compute an equilibrium for a reasonably small e. Or, can be used to get solutions in minutes or even seconds, and can be used as an adaptive subroutine in adaptive real-time algos. 

Deviating from an equilibrium has risks: basically there is no safe exploration. 

So... 1) play at a computed equilibrium strategy (unexploitable --- guaranteed minimum level)
2) After t timesteps, compute OM
3) Compute best response given OM

Repeat steps 2 and 3 every k iterations to allow for a change in dynamics (running every time something changes is too slow and infeasible)},
author = {Ganzfried, S and Sandholm, T},
booktitle = {10th International Conference on Autonomous Agents and Multiagent Systems 2011, AAMAS 2011},
keywords = {Game theory,Multiagent learning},
pages = {497--504},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)},
title = {{Game theory-based opponent modeling in large imperfect-information games}},
volume = {1},
year = {2011}
}
@inproceedings{OM_in_auction_markets2019,
author = {Mahfouz, M and Filos, A and Chtourou, C and Lockhart, J and Assefa, S and Veloso, M and Mandic, D and Balch, T},
booktitle = {Proceedings of the 33rd Conference on Neural Information Processing Systems (NeurIPS'19) : Workshop on Robust AI in Financial Services},
month = {dec},
title = {{On the Importance of Opponent Modeling in Auction Markets}},
year = {2019}
}
@techreport{Ramirez2010,
abstract = {Plan recognition is the problem of inferring the goals and plans of an agent after observing its behavior. Recently, it has been shown that this problem can be solved efficiently, without the need of a plan library, using slightly modified planning algorithms. In this work, we extend this approach to the more general problem of probabilistic plan recognition where a probability distribution over the set of goals is sought under the assumptions that actions have deterministic effects and both agent and observer have complete information about the initial state. We show that this problem can be solved efficiently using classical planners provided that the probability of a partially observed execution given a goal is defined in terms of the cost difference of achieving the goal under two conditions: complying with the observations, and not complying with them. This cost, and hence the posterior goal probabilities, are computed by means of two calls to a classical planner that no longer has to be modified in any way. A number of examples is considered to illustrate the quality, flexibility, and scalability of the approach.},
author = {Ram{\'{i}}rez, M and Geffner, H},
booktitle = {Citeseer},
title = {{Probabilistic Plan Recognition using off-the-shelf Classical Planners}},
url = {www.aaai.org},
year = {2010}
}
@inproceedings{Barrett_2013,
annote = {So, challenge of working with unknown agents in the simulated pursuit domain. 

Challenging environment, even with only four predators and one prey on the standard size matrix there are approximately 10^13 states... so... complex. 

POMDP planners don't scale, so use a sampling method --- upper confidence for trees, which is a MCTS that uses confidence bounds to balance exploitation vs exploration. (basically, exploring when unsure, and exploiting when it is sure, and has seen a state before). 

UCT requies a model of the opponents behaviour to direct the search. 


Computes (conservative) bayesian updates over distribution of specified opponent models (over types).

Builds these types offline. Use decision trees (supervised learning problem in action-prediction given a state). 

So, learns in short term using a form of transfer-learning. Basically it bootstraps previos encounters based on a similarity metric, and using a weighting between each (i.e. the suggestion from one which is less similar has less weight). It is efficient, but dones't guarantee optimal learning. 

All learning is done offline, and at run-time only model selection is used.},
author = {Barrett, S and Stone, P and Kraus, S and Rosenfeld, A},
booktitle = {Proceedings of the Twenty-Seventh {AAAI} Conference on Artificial Intelligence, July 14-18, Bellevue, Washington, {USA}},
title = {{Teamwork with Limited Knowledge of Teammates}},
year = {2013}
}
@inproceedings{Erdogan2011,
abstract = {The RoboCup robot soccer Small Size League has been running since 1997 with many teams successfully competing and very effectively playing the games. Teams of five robots, with a combined autonomous centralized perception and control, and distributed actuation, move at high speeds in the field space, actuating a golf ball by passing and shooting it to aim at scoring goals. Most teams run their own pre-defined team strategies, unknown to the other teams, with flexible game-state dependent assignment of robot roles and positioning. However, in this fast-paced noisy real robot league, recognizing the opponent team strategies and accordingly adapting one's own play has proven to be a considerable challenge. In this work, we analyze logged data of real games gathered by the CMDragons team, and contribute several results in learning and responding to opponent strategies. We define episodes as segments of interest in the logged data, and introduce a representation that captures the spatial and temporal data of the multi-robot system as instances of geometrical trajectory curves. We then learn a model of the team strategies through a variant of agglomerative hierarchical clustering. Using the learned cluster model, we are able to classify a team behavior incrementally as it occurs. Finally, we define an algorithm that autonomously generates counter tactics, in a simulation based on the real logs, showing that it can recognize and respond to opponent strategies.},
annote = {Robot soccer: small size. 

Singular coach-style agent. Input recieved from an overhead camera. 

So, they isolate logs of play of interest (i.e. opponent attacks). 
They extract a trajectory (a series of time-stamped points in 2D space). So basically images or directed graphs. 

They then compare trajectories using a similarity metric to form hierarchical clustering. 
Use the Hausdorf method... 

They make the assumption of non-responsiveness (i.e. the actions they take wont change the course of the attack...

SO... refined the attacks to geomertical curves, performed hierarchical clustering, and then learned (in a simulator) appropriate counter-measures (i.e. intercepting the ball).},
author = {Erdogan, Can and Veloso, Manuela},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.5591/978-1-57735-516-8/IJCAI11-043},
isbn = {9781577355120},
issn = {10450823},
pages = {192--197},
title = {{Action selection via learning behavior patterns in multi-robot domains}},
year = {2011}
}
@misc{Dosovitskiy2017,
abstract = {We introduce CARLA, an open-source simulator for autonomous driving research. CARLA has been developed from the ground up to support development, training, and validation of autonomous urban driving systems. In addition to open-source code and protocols, CARLA provides open digital assets (urban layouts, buildings, vehicles) that were created for this purpose and can be used freely. The simulation platform supports flexible specification of sensor suites and environmental conditions. We use CARLA to study the performance of three approaches to autonomous driving: a classic modular pipeline, an end-to-end model trained via imitation learning, and an end-to-end model trained via reinforcement learning. The approaches are evaluated in controlled scenarios of increasing difficulty, and their performance is examined via metrics provided by CARLA, illustrating the platform's utility for autonomous driving research.},
archivePrefix = {arXiv},
arxivId = {1711.03938},
author = {Dosovitskiy, A and Ros, G and Codevilla, F and L{\'{o}}pez, A and Koltun, V},
booktitle = {arXiv},
eprint = {1711.03938},
issn = {23318422},
keywords = {Autonomous driving,Sensorimotor control,Simulation},
month = {nov},
publisher = {arXiv},
title = {{CARLA: An open urban driving simulator}},
year = {2017}
}
@article{Hernandez-Leal_MAL_Survey,
archivePrefix = {arXiv},
arxivId = {1707.09183},
author = {Hernandez-Leal, P and Kaisers, M and Baarslag, T and de Cote, E},
eprint = {1707.09183},
journal = {CoRR},
title = {{A Survey of Learning in Multiagent Environments: Dealing with Non-Stationarity}},
url = {http://arxiv.org/abs/1707.09183},
volume = {abs/1707.0},
year = {2017}
}
@book{craik1937,
author = {Craik, K J W},
publisher = {CUP Archive},
title = {{The nature of explanation}},
volume = {445},
year = {1952}
}
@article{Hu2020,
abstract = {We consider the problem of zero-shot coordination - constructing AI agents that can coordinate with novel partners they have not seen before (e.g. humans). Standard Multi-Agent Reinforcement Learning (MARL) methods typically focus on the self-play (SP) setting where agents construct strategies by playing the game with themselves repeatedly. Unfortunately, applying SP naively to the zero-shot coordination problem can produce agents that establish highly specialized conventions that do not carry over to novel partners they have not been trained with. We introduce a novel learning algorithm called other-play (OP), that enhances self-play by looking for more robust strategies, exploiting the presence of known symmetries in the underlying problem. We characterize OP theoretically as well as experimentally. We study the cooperative card game Hanabi and show that OP agents achieve higher scores when paired with independently trained agents. In preliminary results we also show that our OP agents obtains higher average scores when paired with human players, compared to state-of-the-art SP agents.},
archivePrefix = {arXiv},
arxivId = {2003.02979},
author = {Hu, Hengyuan and Lerer, Adam and Peysakhovich, Alex and Foerster, Jakob},
eprint = {2003.02979},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Hu et al. - 2020 - Other-Play for Zero-Shot Coordination.pdf:pdf},
month = {mar},
title = {{"Other-Play" for Zero-Shot Coordination}},
url = {http://arxiv.org/abs/2003.02979},
year = {2020}
}
@techreport{Hansen,
abstract = {We develop an exact dynamic programming algorithm for partially observable stochastic games (POSGs). The algorithm is a synthesis of dynamic programming for partially observable Markov decision processes (POMDPs) and iterated elimination of dominated strategies in normal form games. We prove that when applied to finite-horizon POSGs, the algorithm iteratively eliminates very weakly dominated strategies without first forming a normal form representation of the game. For the special case in which agents share the same payoffs, the algorithm can be used to find an optimal solution. Preliminary empirical results are presented.},
author = {Hansen, E A and Bernstein, D S and Zilberstein, S},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Hansen, Bernstein, Zilberstein - Unknown - Dynamic Programming for Partially Observable Stochastic Games.pdf:pdf},
title = {{Dynamic Programming for Partially Observable Stochastic Games}},
url = {www.aaai.org}
}
@article{Gold_complexity_automaton,
author = {Gold, E M},
doi = {10.1016/S0019-9958(78)90562-4},
journal = {Inf. Control.},
number = {3},
pages = {302--320},
title = {{Complexity of Automaton Identification from Given Data}},
url = {https://doi.org/10.1016/S0019-9958(78)90562-4},
volume = {37},
year = {1978}
}
@inproceedings{Barrett2015,
abstract = {Many scenarios require that robots work together as a team in order to effectively accomplish their tasks. However, pre-coordinating these teams may not always be possible given the growing number of companies and research labs creating these robots. Therefore, it is desirable for robots to be able to reason about ad hoc teamwork and adapt to new teammates on the fly. Past research on ad hoc teamwork has focused on relatively simple domains, but this paper demonstrates that agents can reason about ad hoc teamwork in complex scenarios. To handle these complex scenarios, we introduce a new algorithm, PLASTIC-Policy, that builds on an existing ad hoc teamwork approach. Specifically, PLASTIC-Policy learns policies to cooperate with past teammates and reuses these policies to quickly adapt to new teammates. This approach is tested in the 2D simulation soccer league of RoboCup using the half field offense task.},
annote = {This work defines plastic policy for robo-soccer ad hoc teamwork. #

In effect it moves from a model-based selection to a policy-based action selection algorithm. 

The basic idea is that it learns a policy for certain tyupes of teammates. 
This policy doesn't predict how the agent should act (i.e. no model represtnatoin). 

It does however cluster opponents using a nearest neighbour algorithm to swiftly type teammates.},
author = {Barrett, S and Stone, P},
booktitle = {Proceedings of the National Conference on Artificial Intelligence},
isbn = {9781577357018},
month = {jun},
pages = {2010--2016},
publisher = {AI Access Foundation},
title = {{Cooperating with unknown teammates in complex domains: A robot soccer case study of ad hoc teamwork}},
volume = {3},
year = {2015}
}
@techreport{Booch2020,
abstract = {This paper proposes a research direction to advance AI which draws inspiration from cognitive theories of human decision making. The premise is that if we gain insights about the causes of some human capabilities that are still lacking in AI (for instance, adaptability, generalizability, common sense, and causal reasoning), we may obtain similar capabilities in an AI system by embedding these causal components. We hope that the high-level description of our vision included in this paper, as well as the several research questions that we propose to consider, can stimulate the AI research community to define, try and evaluate new methodologies, frameworks, and evaluation metrics, in the spirit of achieving a better understanding of both human and machine intelligence.},
archivePrefix = {arXiv},
arxivId = {2010.06002v2},
author = {Booch, G and Fabiano, F and Horesh, L and Kate, K and Lenchner, J and Linck, N and Loreggia, A and Murugesan, K and Mattei, N and Rossi, F and Srivastava, B},
eprint = {2010.06002v2},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Booch et al. - 2020 - Thinking Fast and Slow in AI Motivation and Overall Vision.pdf:pdf},
title = {{Thinking Fast and Slow in AI Motivation and Overall Vision}},
url = {www.aaai.org},
year = {2020}
}
@article{Alzetta2020,
annote = {Real-time systems have been given comparativelty little focus than 'narrow AI' expert systems. 

Real time systems are characterised by a plethora of algorithms ensuring compliance with strict timing constraints: they require that both the environment and the possible system's interaction with it are predetermined. If the environment changes in unforseen ways, or is too complex to be predictable, RTS is not able to adapt.},
author = {Alzetta, F and Giorgini, P and Najjar, A},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Alzetta, Giorgini, Najjar - 2020 - In-Time Explainability in Multi-Agent Systems Challenges, Opportunities, and Roadmap.pdf:pdf},
journal = {Springer Nature Switzerland: EXRAAMAS 2020},
pages = {39--53},
title = {{In-Time Explainability in Multi-Agent Systems: Challenges, Opportunities, and Roadmap}},
url = {https://link.springer.com/chapter/10.1007/978-3-030-51924-7_3},
year = {2020}
}
@misc{Winston,
author = {Winston, P H},
title = {{The Strong Story Hypothesis and the Directed Perception Hypothesis}},
url = {https://dspace.mit.edu/handle/1721.1/67693}
}
@inproceedings{Winfield_internal_models_ethics,
address = {Cham},
author = {Winfield, A F T and Blum, C and Liu, W},
booktitle = {Advances in Autonomous Robotics Systems},
editor = {Mistry, M and Leonardis, A and Witkowski, M and Melhuish, C},
pages = {85--96},
publisher = {Springer International Publishing},
title = {{Towards an Ethical Robot: Internal Models, Consequences and Ethical Action Selection}},
year = {2014}
}
@article{lastota&fong&shah_survey_humanrobot_interaction,
author = {Lasota, P A and Fong, T and Shah, J A},
doi = {10.1561/2300000052},
journal = {Foundations and Trends in Robotics},
number = {4},
pages = {261--349},
title = {{A Survey of Methods for Safe Human-Robot Interaction}},
url = {https://doi.org/10.1561/2300000052},
volume = {5},
year = {2017}
}
@inproceedings{Pietro_robocup,
author = {{Di Pietro}, A and {Lyndon While}, R and Barone, L},
booktitle = {{GECCO} 2002: Proceedings of the Genetic and Evolutionary Computation Conference, New York, USA, 9-13 July 2002},
pages = {1065--1072},
title = {{Learning In RoboCup Keepaway Using Evolutionary Algorithms}},
year = {2002}
}
@misc{Lerer2019,
abstract = {Recent superhuman results in games have largely been achieved in a variety of zero-sum settings, such as Go and Poker, in which agents need to compete against others. However, just like humans, real-world AI systems have to coordinate and communicate with other agents in cooperative partially observable environments as well. These settings commonly require participants to both interpret the actions of others and to act in a way that is informative when being interpreted. Those abilities are typically summarized as theory of mind and are seen as crucial for social interactions. In this paper we propose two different search techniques that can be applied to improve an arbitrary agreed-upon policy in a cooperative partially observable game. The first one, single-agent search, effectively converts the problem into a single agent setting by making all but one of the agents play according to the agreed-upon policy. In contrast, in multi-agent search all agents carry out the same common-knowledge search procedure whenever doing so is computationally feasible, and fall back to playing according to the agreed-upon policy otherwise. We prove that these search procedures are theoretically guaranteed to at least maintain the original performance of the agreed-upon policy (up to a bounded approximation error). In the benchmark challenge problem of Hanabi, our search technique greatly improves the performance of every agent we tested and when applied to a policy trained using RL achieves a new state-of-the-art score of 24.61 / 25 in the game, compared to a previous-best of 24.08 / 25.},
archivePrefix = {arXiv},
arxivId = {1912.02318},
author = {Lerer, Adam and Hu, Hengyuan and Foerster, Jakob and Brown, Noam},
booktitle = {arXiv},
doi = {10.1609/aaai.v34i05.6208},
eprint = {1912.02318},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Lerer et al. - 2019 - Improving policies via search in cooperative partially observable games.pdf:pdf},
issn = {23318422},
title = {{Improving policies via search in cooperative partially observable games}},
url = {https://github.com/facebookresearch/},
year = {2019}
}
@article{ROSENSCHEIN_Kaelbling,
abstract = {Intelligent agents are systems that have a complex, ongoing interaction with an environment that is dynamic and imperfectly predictable. Agents are typically difficult to program because the correctness of a program depends on the details of how the agent is situated in its environment. In this paper, we present a methodology for the design of situated agents that is based on situated-automata theory. This approach allows designers to describe the informational content of an agent's computational states in a semantically rigorous way without requiring a commitment to conventional run-time symbolic processing. We start by outlining this situated view of representation, then show how it contributes to design methodologies for building systems that track perceptual conditions and take purposeful actions in their environments.},
annote = {Computational Research on Interaction and Agency, Part 2},
author = {Rosenschein, S J and Kaelbling, L P},
issn = {0004-3702},
journal = {Artificial Intelligence},
number = {1},
pages = {149--173},
title = {{A situated view of representation and control}},
volume = {73},
year = {1995}
}
@misc{Brown2017,
abstract = {In imperfect-information games, the optimal strategy in a subgame may depend on the strategy in other, unreached subgames. Thus a subgame cannot be solved in isolation and must instead consider the strategy for the entire game as a whole, unlike perfect-information games. Nevertheless, it is possible to first approximate a solution for the whole game and then improve it by solving individual subgames. This is referred to as subgame solving. We introduce subgame-solving techniques that outperform prior methods both in theory and practice. We also show how to adapt them, and past subgame-solving techniques, to respond to opponent actions that are outside the original action abstraction; this significantly outperforms the prior state-of-the-art approach, action translation. Finally, we show that subgame solving can be repeated as the game progresses down the game tree, leading to far lower exploitability. These techniques were a key component of Libratus, the first AI to defeat top humans in heads-up no-limit Texas hold'em poker.},
author = {Brown, Noam and Sandholm, Tuomas},
booktitle = {arXiv},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Sandholm - 2017 - Safe and nested subgame solving for imperfect-information games.pdf:pdf},
issn = {23318422},
title = {{Safe and nested subgame solving for imperfect-information games}},
year = {2017}
}
@article{Albrecht2017,
abstract = {Much research in artificial intelligence is concerned with the development of autonomous agents that can interact effectively with other agents. An important aspect of such agents is the ability to reason about the behaviours of other agents, by constructing models which make predictions about various properties of interest (such as actions, goals, beliefs) of the modelled agents. A variety of modelling approaches now exist which vary widely in their methodology and underlying assumptions, catering to the needs of the different sub-communities within which they were developed and reflecting the different practical uses for which they are intended. The purpose of the present article is to provide a comprehensive survey of the salient modelling methods which can be found in the literature. The article concludes with a discussion of open problems which may form the basis for fruitful future research.},
author = {Albrecht, S V and Stone, P},
doi = {10.1016/j.artint.2018.01.002},
journal = {Artificial Intelligence},
keywords = {Autonomous agents,Modelling other agents,Multiagent systems,Opponent modelling},
month = {nov},
pages = {66--95},
publisher = {Elsevier B.V.},
title = {{Autonomous Agents Modelling Other Agents: A Comprehensive Survey and Open Problems}},
url = {http://arxiv.org/abs/1709.08071 http://dx.doi.org/10.1016/j.artint.2018.01.002},
volume = {258},
year = {2017}
}
@article{Koller2003,
abstract = {The traditional representations of games using the extensive form or the strategic form obscure much of the structure of real-world games. In this paper, we propose a graphical representation for noncooperative games - multi-agent influence diagrams (MAIDs). The basic elements in the MAID representation are variables, allowing an explicit representation of dependence, or relevance, relationships among variables. We define a decision variable D′ as strategically relevant to D if, to optimize the decision rule at D, the decision maker needs to consider the decision rule at D′. We provide a sound and complete graphical criterion for determining strategic relevance. We then show how strategic relevance can be used to decompose large games into a set of interacting smaller games, which can be solved in sequence. We show that this decomposition can lead to substantial savings in the computational cost of finding Nash equilibria in these games. {\textcopyright} 2003 Elsevier Inc. All rights reserved.},
author = {Koller, D and Milch, B},
doi = {10.1016/S0899-8256(02)00544-4},
issn = {08998256},
journal = {Games and Economic Behavior},
month = {oct},
number = {1},
pages = {181--221},
publisher = {Academic Press Inc.},
title = {{Multi-agent influence diagrams for representing and solving games}},
volume = {45},
year = {2003}
}
@inproceedings{Rabinowitz_TomNet,
address = {Stockholmsm{\"{a}}ssan, Stockholm Sweden},
annote = {Raises and interesting point: our ability to model other agents by mental states is still measured by predictive accuracy in real life, not by any ground truth... 

Focus on the problem of how to learn anothers mental state given limited data: ephasis on the how (meta-learning), rather than focusing directly on a generative form (as with most methods of opponent modelling.

Also aims not to impose any strict-assumptions on the way others are represented: e.g. Plan-recogniton assumes the presence of a plan, Foersters learning with OL awareness assumes a learning agent. 

Efficient/swift learning is also useful for alignment, flexible cooperation and mediating human understanding of AI agents: bluntly, there are number of uses outside of strict opponent modelling. 

NB: deep mind tends to make the assumption that most complex environments and situations are best-left to machine learning with little-no assumptions. Hence, no model-mismatch. 

SO --- the basic idea is to build an effective prior: a general agent network, and then train or update this swiftly, on a recently observed agent, so have a posterior (i.e. posterior over POMDP). 

They do this with a character net, a mental-state net and a prediction net. 

The character net parses the previous episodes into a character embedding.

The mental-state net parses the previous trajector to infer the mental-state: i.e. beliefs

The prediction net then uses the mental-state and character embeddings to attempt to predict the next actions of the agent. 

So, tested in a number of ways, on firstly simple action-frequency style agents, and then on more complex RL trained agents. 

Performed a Sally-Anne test, (third agent removes an object outside of the field of view of an agent). 

So, while hard-observation boundaries weren't directly established, they found that generally when swaps happened further away from agents, ToMNet would correctly predict the observed agent wouldn't react to the movement of the desired object.},
author = {Rabinowitz, N and Perbet, F and Song, F and Zhang, C and Eslami, S M A and Botvinick, M},
booktitle = {Proceedings of the 35th International Conference on Machine Learning},
editor = {Dy, J and Krause, A},
pages = {4218--4227},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Machine Theory of Mind}},
volume = {80},
year = {2018}
}
@article{Mizrahi,
abstract = {In recent years collaborative robots have become major market drivers in industry 5.0, which aims to incorporate them alongside humans in a wide array of settings ranging from welding to rehabilitation. Improving human–machine collaboration entails using computational algorithms that will save processing as well as communication cost. In this study we have constructed an agent that can choose when to cooperate using an optimal strategy. The agent was designed to operate in the context of divergent interest tacit coordination games in which communication between the players is not possible and the payoff is not symmetric. The agent's model was based on a behavioral model that can predict the probability of a player converging on prominent solutions with salient features (e.g., focal points) based on the player's Social Value Orientation (SVO) and the specific game features. The SVO theory pertains to the preferences of decision makers when allocating joint resources between themselves and another player in the context of behavioral game theory. The agent selected stochastically between one of two possible policies, a greedy or a cooperative policy, based on the probability of a player to converge on a focal point. The distribution of the number of points obtained by the autonomous agent incorporating the SVO in the model was better than the results obtained by the human players who played against each other (i.e., the distribution associated with the agent had a higher mean value). Moreover, the distribution of points gained by the agent was better than any of the separate strategies the agent could choose from, namely, always choosing a greedy or a focal point solution. To the best of our knowledge, this is the first attempt to construct an intelligent agent that maximizes its utility by incorporating the belief system of the player in the context of tacit bargaining. This reward-maximizing strategy selection process based on the SVO can also be potentially applied in other human–machine contexts, including multiagent systems.},
author = {Mizrahi, Dor and Zuckerman, Inon and Laufer, Ilan},
doi = {10.3390/s20247026},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Mizrahi, Zuckerman, Laufer - 2020 - Using a stochastic agent model to optimize performance in divergent interest tacit coordination game.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Autonomous agent,Cognitive modeling,Decision-making,Divergent interest,Social value orientation (SVO),Tacit coordination},
number = {24},
pages = {1--15},
pmid = {33302476},
title = {{Using a stochastic agent model to optimize performance in divergent interest tacit coordination games}},
url = {www.mdpi.com/journal/sensors},
volume = {20},
year = {2020}
}
@article{mealing_shapiro_OM_by_expectation_maximisation,
annote = {Playing computerised poker: challenging to learn a strategy against a person when the information changes, and the hidden information isn't revealed.

So, test in a couple of variants of texas-hold'em poker. 

1) Expectation-maximisation algorithm to infer hidden information. 

2) Sequence prediction to predict actions, and note when a player has changed strategy.
---
So given an information set (esimated by a sample of the estimation maximiation process before), it predicts the likely action of an opponent, conditioned on previous moves by that opponent in similar situations. 

------------

3) Outcome Sampling Counter-factual regret mimisation, trained through self-play, to compute a (possibly local) best response. This is shown to approximate (eventually through many iterations of self-play) to approximate a Nash equilibrium strategy profile. 

So, basically, in between moves, it simulates the games and various outcomes based on the opponent model. It then selects the best outcome...
As it takes the opponent model into account, as the opponet gets better, it starts to automatically (via the nature of counter-factual regret minimisation) revert towards a best-response (equilibrium) strategy which isn't exploitable. I.e. if the opponent starts to learn to play optimally, it starts to play optimally. 

Interesting because it doesn't assume a stationary strategy --- quoite the opposite. It assumes a weak opponent who improves (as with most learning algos) and starts by exploiting depending on the OM. 

In terms of scale, separate OM strategies can be computed, with a single policy --- however, computing the OM assumes policies won't necessarily compound --- i.e. OMs are relatively simple to the point of inferring hidden information and sequence prediction.},
author = {Mealing, R and Shapiro, J},
doi = {10.1109/TCIAIG.2015.2491611},
journal = {{IEEE} Trans. Comput. Intellig. and {AI} in Games},
number = {1},
pages = {11--24},
title = {{Opponent Modeling by Expectation-Maximization and Sequence Prediction in Simplified Poker}},
url = {https://doi.org/10.1109/TCIAIG.2015.2491611},
volume = {9},
year = {2017}
}
@article{Weber2017,
abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
archivePrefix = {arXiv},
arxivId = {1707.06203},
author = {Weber, T and Racani{\`{e}}re, S and Reichert, D P and Buesing, L and Guez, A and Rezende, D J and Badia, A P and Vinyals, O and Heess, N and Li, Y and Pascanu, R and Battaglia, P and Hassabis, D and Silver, D and Wierstra, D},
eprint = {1707.06203},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Weber et al. - 2017 - Imagination-Augmented Agents for Deep Reinforcement Learning.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {jul},
pages = {5691--5702},
publisher = {Neural information processing systems foundation},
title = {{Imagination-Augmented Agents for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1707.06203},
volume = {2017-Decem},
year = {2017}
}
@inproceedings{brooks1991challenges,
author = {Brooks, R A},
booktitle = {First International Conference on Simulation of Adaptive Behavior},
organization = {Citeseer},
pages = {434--443},
title = {{Challenges for complete creature architectures}},
year = {1991}
}
@inproceedings{Iglesias2006,
abstract = {The main goal of agent modelling is to extract and represent the knowledge about the behaviour of other agents. Nowadays, modelling an agent in multi-agent systems is increasingly becoming more complex and significant. Also, robotic soccer domain is an interesting environment where agent modelling can be used. In this paper, we present an approach to classify and compare the behaviour of a multi-agent system using a coach in the soccer simulation domain of the RoboCup. {\textcopyright} Springer-Verlag Berlin Heidelberg 2006.},
author = {Iglesias, J and Ledezma, A and Sanchis, A},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/11681960_13},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Iglesias, Ledezma, Sanchis - 2006 - A comparing method of two team behaviours in the simulation coach competition.pdf:pdf},
isbn = {3540327800},
issn = {03029743},
pages = {117--128},
title = {{A comparing method of two team behaviours in the simulation coach competition}},
volume = {3885 LNAI},
year = {2006}
}
@phdthesis{HelgassonAGIAttention,
author = {Helgason, Helgi},
title = {{General Attention Mechanism for Artificial Intelligence Systems}},
year = {2013}
}
@inproceedings{Kuipers05,
author = {Kuipers, B},
booktitle = {Proceedings, The Twentieth National Conference on Artificial Intelligence and the Seventeenth Innovative Applications of Artificial Intelligence Conference, July 9-13, 2005, Pittsburgh, Pennsylvania, {USA}},
pages = {1298--1305},
title = {{Consciousness: Drinking from the Firehose of Experience}},
year = {2005}
}
@inproceedings{8432177,
author = {Manjunath, A and Bhat, M and Shumaiev, K and Biesdorf, A and Matthes, F},
booktitle = {2018 IEEE International Conference on Software Architecture Companion (ICSA-C)},
pages = {52--55},
title = {{Decision Making and Cognitive Biases in Designing Software Architectures}},
year = {2018}
}
@techreport{Wang2020,
abstract = {Creating open-ended algorithms, which generate their own never-ending stream of novel and appropriately challenging learning opportunities, could help to automate and accelerate progress in machine learning. A recent step in this direction is the Paired Open-Ended Trailblazer (POET), an algorithm that generates and solves its own challenges , and allows solutions to goal-switch between challenges to avoid local optima. However, the original POET was unable to demonstrate its full creative potential because of limitations of the algorithm itself and because of external issues including a limited problem space and lack of a universal progress measure. Importantly, both limitations pose impediments not only for POET, but for the pursuit of open-endedness in general. Here we introduce and empirically validate two new innovations to the original algorithm, as well as two external innovations designed to help elucidate its full potential. Together, these four advances enable the most open-ended algorithmic demonstration to date. The algorithmic innovations are (1) a domain-general measure of how meaningfully novel new challenges are, enabling the system to potentially create and solve interesting challenges endlessly, and (2) an efficient heuristic for determining when agents should goal-switch from one problem to another (helping open-ended search better scale). Outside the algorithm itself, to enable a more definitive demonstration of open-endedness, we introduce (3) a novel, more flexible way to encode environmental challenges, and (4) a generic measure of the extent to which a system continues to exhibit open-ended innovation. Enhanced POET produces a diverse range of so-* Co-senior authors 1 Uber AI 2 OpenAI. Work done at Uber AI. Correspondence to: Rui Wang <ruiwang@uber.com>, Jeff Clune},
author = {Wang, Rui and Lehman, Joel and Rawal, Aditya and Zhi, Jiale and Li, Yulun and Clune, Jeff and Stanley, Kenneth O},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2020 - Enhanced POET Open-ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Soluti.pdf:pdf},
issn = {2640-3498},
month = {nov},
pages = {9940--9951},
publisher = {PMLR},
title = {{Enhanced POET: Open-ended Reinforcement Learning through Unbounded Invention of Learning Challenges and their Solutions}},
url = {http://proceedings.mlr.press/v119/wang20l.html},
year = {2020}
}
@article{Rubin&Watson_poker_review,
author = {Rubin, J and Watson, I},
doi = {10.1016/j.artint.2010.12.005},
journal = {Artif. Intell.},
number = {5-6},
pages = {958--987},
title = {{Computer poker: {A} review}},
url = {https://doi.org/10.1016/j.artint.2010.12.005},
volume = {175},
year = {2011}
}
@article{InteractivePOMDPs_panella_gmytrasiewicz,
annote = {Panella and Gmytraseiwicz propsed initally an intentional interactive partially observable decision process, in whic there were nested levels of POMdPs. 

Sovling it meant solving the deepends PomDP and working outwards (I presume). 

They augment this work by developing a subintentioanl model, and instead structuring agents as probibalistic deterministic finite controllers. PDFCs. 

They compute a bayesian non-parametric prior over the theoretically limitless allocation of agent models for an opponent --- as this is computationally impossible, they use a samplingbased method, Markov Chain Monte Carlo method, which is a non-parametric method which eventually converges to the correct belief over the set of models. 

Because PDFCs are built based on data, they can be as complex or as simple as the data observes.},
author = {Panella, A and Gmytrasiewicz, P},
doi = {10.1007/s10458-016-9359-z},
journal = {Autonomous Agents and Multi-Agent Systems},
number = {4},
pages = {861--904},
title = {{Interactive POMDPs with finite-state models of other agents}},
url = {https://doi.org/10.1007/s10458-016-9359-z},
volume = {31},
year = {2017}
}
@inproceedings{Carmel_Markovitch1996,
annote = {Building on their seminal work (1993) they show that given common utility functions in repeated two-player normal-form games, agents represented by DFAs, best-response strategies can be inferred. 

Defined an unsupervised learning algorithm for constructing minimal DFAs which are consistent with an opponents strategy. They show you can learn a minimal opponent model effectively using their US-L* algorithm. 

General intuition: perfect recall --- record the history, start with a randomly generated DFA, use that, then rebuild based on observations, and tune DFA to opponent.},
author = {Carmel, D and Markovitch, S},
booktitle = {Proceedings of the Thirteenth National Conference on Artificial Intelligence and Eighth Innovative Applications of Artificial Intelligence Conference, {AAAI} 96, {IAAI} 96, Portland, Oregon, USA, August 4-8, 1996, Volume 1},
pages = {62--67},
title = {{Learning Models of Intelligent Agents}},
year = {1996}
}
@misc{Albrecht2019,
abstract = {This paper is concerned with evaluating different multiagent learning (MAL) algorithms in problems where individual agents may be heterogenous, in the sense of utilising different learning strategies, without the opportunity for prior agreements or information regarding coordination. Such a situation arises in ad hoc team problems, a model of many practical multiagent systems applications. Prior work in multiagent learning has often been focussed on homogeneous groups of agents, meaning that all agents were identical and a priori aware of this fact. Also, those algorithms that are specifically designed for ad hoc team problems are typically evaluated in teams of agents with fixed behaviours, as opposed to agents which are adapting their behaviours. In this work, we empirically evaluate five MAL algorithms, representing major approaches to multiagent learning but originally developed with the homogeneous setting in mind, to understand their behaviour in a set of ad hoc team problems. All teams consist of agents which are continuously adapting their behaviours. The algorithms are evaluated with respect to a comprehensive characterisation of repeated matrix games, using performance criteria that include considerations such as attainment of equilibrium, social welfare and fairness. Our main conclusion is that there is no clear winner. However, the comparative evaluation also highlights the relative strengths of different algorithms with respect to the type of performance criteria, e.g., social welfare vs. attainment of equilibrium.},
archivePrefix = {arXiv},
arxivId = {1907.09189},
author = {Albrecht, Stefano V and Ramamoorthy, Subramanian},
booktitle = {arXiv},
eprint = {1907.09189},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Albrecht, Ramamoorthy - 2019 - Comparative Evaluation of Multiagent Learning Algorithms in a Diverse Set of Ad Hoc Team Problems.pdf:pdf},
issn = {23318422},
keywords = {Ad Hoc Teams,Agent Coordination,Multiagent Learning},
title = {{Comparative Evaluation of Multiagent Learning Algorithms in a Diverse Set of Ad Hoc Team Problems}},
url = {www.ifaamas.org},
year = {2019}
}
@inproceedings{Chen2019a,
abstract = {In an ad hoc teamwork setting, the team needs to coordinate their activities to perform a task without prior agreement on how to achieve it. The ad hoc agent cannot communicate with its teammates but it can observe their behaviour and plan accordingly. To do so, the existing approaches rely on the teammates' behaviour models. However, the models may not be accurate, which can compromise teamwork. For this reason, we present Ad Hoc Teamwork by Sub-task Inference and Selection (ATSIS) algorithm that uses a sub-task inference without relying on teammates' models. First, the ad hoc agent observes its teammates to infer which sub-tasks they are handling. Based on that, it selects its own sub-task using a partially observable Markov decision process that handles the uncertainty of the sub-task inference. Last, the ad hoc agent uses the Monte Carlo tree search to find the set of actions to perform the sub-task. Our experiments show the benefits of ATSIS for robust teamwork.},
author = {Chen, Shuo and Andrejczuk, Ewa and Irissappane, Athirai A and Zhang, Jie},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.24963/ijcai.2019/25},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2019 - ATSIS Achieving the Ad hoc Teamwork by Sub-task Inference and Selection.pdf:pdf},
isbn = {9780999241141},
issn = {10450823},
keywords = {Agent-based and Multi-agent Systems: Coordination,Uncertainty in AI: Sequential Decision Making},
pages = {172--179},
title = {{ATSIS: Achieving the ad hoc teamwork by sub-task inference and selection}},
volume = {2019-Augus},
year = {2019}
}
@inproceedings{Leibo2017,
abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We char-Acterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
archivePrefix = {arXiv},
arxivId = {1702.03037},
author = {Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
eprint = {1702.03037},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Leibo et al. - 2017 - Multi-agent reinforcement learning in sequential social dilemmas.pdf:pdf},
isbn = {9781510855076},
issn = {15582914},
keywords = {Agent-based social simulation,Cooperation,Keywords Social dilemmas, cooperation, Markov game,Markov games,Non-cooperative games,Social dilemmas},
pages = {464--473},
title = {{Multi-agent reinforcement learning in sequential social dilemmas}},
url = {www.ifaamas.org},
volume = {1},
year = {2017}
}
@inproceedings{Mller1993TheAA,
author = {M{\"{u}}ller, J P and Pischel, M},
title = {{The agent architecture InteRRaP : concept and application}},
year = {1993}
}
@article{Iglesias_classification,
author = {Iglesias, J A and Angelov, P P and Ledezma, A and Sanchis, A},
journal = {Evolving Systems},
number = {3},
pages = {161--171},
title = {{Evolving classification of agents' behaviors: a general approach}},
volume = {1},
year = {2010}
}
@techreport{Painter2020,
abstract = {This work investigates Monte-Carlo planning for agents in stochastic environments, with multiple objectives. We propose the Convex Hull Monte-Carlo Tree-Search (CHMCTS) framework, which builds upon Trial Based Heuristic Tree Search and Convex Hull Value Iteration (CHVI), as a solution to multi-objective planning in large environments. Moreover, we consider how to pose the problem of approximating multi-objective planning solutions as a contextual multi-armed bandits problem, giving a principled motivation for how to select actions from the view of contextual regret. This leads us to the use of Contextual Zooming for action selection, yielding Zooming CHMCTS. We evaluate our algorithm using the Gen-eralised Deep Sea Treasure environment, demonstrating that Zooming CHMCTS can achieve a sublinear contextual regret and scales better than CHVI on a given computational budget.},
author = {Painter, Michael and Lacerda, Bruno and Hawes, Nick},
booktitle = {Proceedings of the International Conference on Automated Planning and Scheduling},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Painter, Lacerda, Hawes - 2020 - Convex Hull Monte-Carlo Tree-Search.pdf:pdf},
issn = {2334-0843},
keywords = {ICAPS Main},
month = {jun},
pages = {217--225},
title = {{Convex Hull Monte-Carlo Tree-Search}},
url = {www.aaai.org},
volume = {2020},
year = {2020}
}
@misc{Franceschelli2019,
abstract = {In this technical note we consider a class of multi-agent network systems that we refer to as Open Multi-Agent Systems (OMAS): in these multi-agent systems, an indefinite number of agents may join or leave the network at any time. Focusing on discrete-time evolutions of scalar agents, we provide a novel theoretical framework to study the dynamical properties of OMAS: specifically, we propose a suitable notion of stability and derive sufficient conditions to ensure stability in this sense. These sufficient conditions regard the arrival/departure of an agent as a disturbance: consistently, they require the effect of arrivals/departures to be bounded (in a precise sense) and the OMAS to be contractive in the absence of arrivals/departures. In order to provide an example of application for this theory, we re-formulate the well-known Proportional Dynamic Consensus for Open Multi-Agent Systems and we characterize the stability properties of the resulting Open Proportional Dynamic Consensus algorithm.},
archivePrefix = {arXiv},
arxivId = {1906.00890},
author = {Franceschelli, Mauro and Frasca, Paolo},
booktitle = {arXiv},
eprint = {1906.00890},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Franceschelli, Frasca - 2019 - Stability of open multi-agent systems and applications to dynamic consensus.pdf:pdf},
issn = {23318422},
month = {jun},
publisher = {arXiv},
title = {{Stability of open multi-agent systems and applications to dynamic consensus}},
url = {http://arxiv.org/abs/1906.00890},
year = {2019}
}
@misc{Papoudakis2020,
abstract = {Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we evaluate and compare three different classes of MARL algorithms (independent learners, centralised training with decentralised execution, and value decomposition) in a diverse range of multi-agent learning tasks. Our results show that (1) algorithm performance depends strongly on environment properties and no algorithm learns efficiently across all learning tasks; (2) independent learners often achieve equal or better performance than more complex algorithms; (3) tested algorithms struggle to solve multi-agent tasks with sparse rewards. We report detailed empirical data, including a reliability analysis, and provide insights into the limitations of the tested algorithms.},
archivePrefix = {arXiv},
arxivId = {2006.07869},
author = {Papoudakis, Georgios and Christianos, Filippos and Sch{\"{a}}fer, Lukas and Albrecht, Stefano V},
booktitle = {arXiv},
eprint = {2006.07869},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Papoudakis et al. - 2020 - Comparative Evaluation of Multi-Agent Deep Reinforcement Learning Algorithms.pdf:pdf},
issn = {23318422},
title = {{Comparative Evaluation of Multi-Agent Deep Reinforcement Learning Algorithms}},
url = {www.github.com/uoe-agents/robotic-warehouse},
year = {2020}
}
@inproceedings{Minds_of_manyBurkhard,
annote = {Model multiple opponents. They do this by stero-typing. 
Opponents are stereo-typed into categories, which are then modelled as a single opponent. 

Shown in the common pool resource game: it depends on the actions of all agents, and one agent's ability to predict the actions of all other agents. 
Presents non-determinsitic game dynamics, partial observability and unknown adversaries. 

Show the effects of using different orders of ToM (i.e. different levels of nested beliefs.)

They use type-based reasoning, but learn the types: types are not pre-fabricated. 

When environment dynamics are unknown, they incorporate Q learning as an estimate of the relative magnitude of payoffs for a certain action given the incorporated beliefs (Boltzmann eqn).

As they can hold first order (limited to first order) beliefs about each other, and all other agents present, this leads to an exponential rise in complexity. 

As this gets complex, agents are stereotyped into groups. 
As the agents are limited to first order beliefs, the space complexity is limited to O(n^2). This is the same in all cases. 
Time complexity is nm^n.... where N is the number of actions.

Interesting result: when numbers of agents increase, and partial observability is introduced first order TOM agents under-perform, compared to zero-order TOM agents.
Perhaps suggesting that higher-order epistemic reasoning is unneccessary when playing against unknown agents/ but also when uncertainty is greater. The segmentation into groups also followed a similar pattern: under uncertainty, higher granularity (smaller groups) performed worse than larger segmentations. f},
author = {von der Osten, F and Kirley, M and Miller, T},
booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, {IJCAI} 2017, Melbourne, Australia, August 19-25, 2017},
doi = {10.24963/ijcai.2017/537},
editor = {Sierra, C},
pages = {3845--3851},
publisher = {ijcai.org},
title = {{The Minds of Many: Opponent Modeling in a Stochastic Game}},
url = {https://doi.org/10.24963/ijcai.2017/537},
year = {2017}
}
@inproceedings{Katt2017,
abstract = {The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult. Bayes-Adaptive Partially Observable Markov Decision Processes (BAPOMDPs) extend POMDPs to allow the model to be learned during execution. BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration. Unfortunately, BAPOMDPs are currently impractical to solve for any non-trivial domain. In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve. Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.},
author = {Katt, Sammie and Oliehoek, Frans A and Amato, Christopher},
booktitle = {International Conference on Machine Learning},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Katt, Oliehoek, Amato - 2017 - Learning in POMDPs with monte carlo tree search.pdf:pdf},
issn = {23318422},
title = {{Learning in POMDPs with monte carlo tree search}},
year = {2017}
}
@techreport{He2016,
abstract = {Opponent modeling is necessary in multi-agent settings where secondary agents with competing goals also adapt their strategies, yet it remains challenging because strategies interact with each other and change. Most previous work focuses on developing probabilistic models or parameterized strategies for specific applications. Inspired by the recent success of deep reinforcement learning, we present neural-based models that jointly learn a policy and the behavior of opponents. Instead of explicitly predicting the opponent's action, we encode observation of the opponents into a deep Q-Network (DQN); however, we retain explicit modeling (if desired) using multitasking. By using a Mixture-of-Experts architecture, our model automatically discovers different strategy patterns of opponents without extra supervision. We evaluate our models on a simulated soccer game and a popular trivia game, showing superior performance over DQN and its variants.},
annote = {Uses machine learning to allow for a wider-application. 
Basically deep-RL for opponent modelling to allow the rules of the game and the environment complexity to be marginalised over. 

So --- Q learning uses a q table to select the action with the highest discounted reward. In Complex environments, a deep-q network (a nn with samples from a memory) can be used to approximate this function. 
They use deep q learning to train an agent. 

This works in the single-agent case, but in multi-agent cases, the environment is non-stationary due to the actions of other agents. Hence, the joint action-transition probabilities need to be computed. 
If the agent follows a stationary strategy, the environment will too be stationary, hence it reverts back to normal Q learning, which should be sufficient. 

It is however unrealistic to assume agents used fixed strategies. E.g. if agents are learning. Also, treating agents as part of the environment can lead to slow adaptation: their changes are masked by the environment. 

They moddel opponents separately using a NN. 
They model and combine the networks in 2 different ways. 

DRON-concat --- concatenates the two networks
DRON-MOE --- uses a mixture of experts model. 

THey incorporate multi-task learning in DRONMOE in order to ensure the encoding of latent variables about the oppoent which might not seem relevant in earlier stages.},
author = {He, H and Boyd-Graber, J and Kwok, K},
booktitle = {jmlr.org},
title = {{Opponent Modeling in Deep Reinforcement Learning}},
url = {https://github.com/hhexiy/},
year = {2016}
}
@book{Bach2012,
doi = {10.1007/978-3-642-35506-6},
editor = {Bach, Joscha and Goertzel, Ben and Ikl{\'{e}}, Matthew},
isbn = {978-3-642-35505-9},
publisher = {Springer Berlin Heidelberg},
title = {{Artificial General Intelligence}},
url = {http://link.springer.com/10.1007/978-3-642-35506-6},
volume = {7716},
year = {2012}
}
@inproceedings{Fischer2020,
abstract = {Planning in Partially Observable Markov Decision Processes (POMDPs) inherently gathers the information necessary to act optimally under uncertainties. The framework can be extended to model pure information gathering tasks by considering belief-based rewards. This allows us to use reward shaping to guide POMDP planning to informative beliefs by using a weighted combination of the original reward and the expected information gain as the objective. In this work we propose a novel online algorithm, Information Particle Filter Tree (IPFT), to solve problems with belief-dependent rewards on continuous domains. It simulates particle-based belief trajectories in a Monte Carlo Tree Search (MCTS) approach to construct a search tree in the belief space. The evaluation shows that the consideration of information gain greatly improves the performance in problems where information gathering is an essential part of the optimal policy.},
annote = {Solving POMDPs requires approximation. 

Online planning for large and continuous state spaces are derived from MCTS, the most prominent ones being POMCP, DESPOT, and ABT. 

Potential based reward shaping can be used to address sparse rewards.
Potentials are functions over beliefs and reflect how much reward can potentially be gained in a certain situation.
This considerably speeds up planning... 

This results in a belief-based reward functions which can be modelled within a rho-POMDP framework. 

They suggest a new algo- Information-Particle Filter Tree for solving r-POMDPs...

They approximate beliefs with small particle sets and solve an MDP on the belief space with MCTS...},
author = {Fischer, Johannes and Tas, {\"{O}}mer Sahin},
booktitle = {Proceedings of the 37th International Conference on Machine Learning},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Fischer, Tas - 2020 - Information Particle Filter Tree An Online Algorithm for {POMDP}s with Belief-Based Rewards on Continuous Domains.pdf:pdf},
keywords = {IPFT,Index Terms-Continuous POMDP,Information Gathering,Monte Carlo Tree Search,Particle Filter Tree,Planning,Reward Shaping},
pages = {3177--3187},
title = {{Information Particle Filter Tree: An Online Algorithm for {POMDP}s with Belief-Based Rewards on Continuous Domains}},
url = {http://proceedings.mlr.press/v119/fischer20a.html},
volume = {119},
year = {2020}
}
@inproceedings{Evalulation_of_adhoc_teamwork_Barrett_Stone_Kraus,
annote = {Evaluated the performance of an ad-hoc learning agent in the simulated pursuit domain. 
Strict type-based reasoning at the start, but showed that they could learn to cooperate over time usiing decison trees and adapt on the fly, and in the meantime using incomplete or incorrect models. 

Had two types of companions: teammate aware or greedy predator. 

Showed that MCTS with UCT is an effective algo even under uncertainty, and is robust to strong-branching environments, and advocates for its use over value iteration.},
author = {Barrett, S and Stone, P and Kraus, S},
booktitle = {10th International Conference on Autonomous Agents and Multiagent Systems {(AAMAS} 2011), Taipei, Taiwan, May 2-6, 2011, Volume 1-3},
pages = {567--574},
title = {{Empirical evaluation of ad hoc teamwork in the pursuit domain}},
url = {http://portal.acm.org/citation.cfm?id=2031698&CFID=54178199&CFTOKEN=61392764},
year = {2011}
}
@techreport{Racaniere2017,
abstract = {We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.},
author = {Racani{\`{e}}re, S{\'{e}}bastien and Weber, Th{\'{e}}ophane and Reichert, David P and Buesing, Lars and Guez, Arthur and Rezende, Danilo and {Puigdom{\`{e}}nech Badia}, Adria and Vinyals, Oriol and Heess, Nicolas and Li, Yujia and Pascanu, Razvan and Battaglia, Peter and Hassabis, Demis and Daan, David Silver and Deepmind, Wierstra},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Racani{\`{e}}re et al. - 2017 - Imagination-Augmented Agents for Deep Reinforcement Learning.pdf:pdf},
pages = {5690--5701},
title = {{Imagination-Augmented Agents for Deep Reinforcement Learning}},
volume = {30},
year = {2017}
}
@article{Moran2019,
abstract = {To study open-ended coevolution, we define a complexity metric over interacting finite state machines playing formal language prediction games, and study the dynamics of populations under competitive and cooperative interactions. In the past purely competitive and purely cooperative interactions have been studied extensively, but neither can successfully and continuously drive an arms race. We present quantitative results using this complexity metric and analyze the causes of varying rates of complexity growth across different types of interactions. We find that while both purely competitive and purely cooperative coevolution are able to drive complexity growth above the rate of genetic drift, mixed systems with both competitive and cooperative interactions achieve significantly higher evolved complexity.},
author = {Moran, Nick and Pollack, Jordan},
doi = {10.1162/artl_a_00281},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Moran, Pollack - 2019 - Evolving complexity in prediction games.pdf:pdf},
issn = {15309185},
journal = {Artificial Life},
keywords = {Coevolution,Competition,Complexity,Cooperation,Ecosystems},
month = {apr},
number = {1},
pages = {74--91},
pmid = {30933627},
publisher = {MIT Press Journals},
title = {{Evolving complexity in prediction games}},
url = {http://direct.mit.edu/artl/article-pdf/25/1/74/1667106/artl_a_00281.pdf},
volume = {25},
year = {2019}
}
@article{Zeng2012,
abstract = {We focus on the problem of sequential decision making in partially observable environments shared with other agents of uncertain types having similar or conflicting objectives. This problem has been previously formalized by multiple frameworks one of which is the interactive dynamic influence diagram (I-DID), which generalizes the well-known influence diagram to the multiagent setting. I-DIDs are graphical models and may be used to compute the policy of an agent given its belief over the physical state and others' models, which changes as the agent acts and observes in the multiagent setting. As we may expect, solving I-DIDs is computationally hard. This is predominantly due to the large space of candidate models ascribed to the other agents and its exponential growth over time. We present two methods for reducing the size of the model space and stemming its exponential growth. Both these methods involve aggregating individual models into equivalence classes. Our first method groups together behaviorally equivalent models and selects only those models for updating which will result in predictive behaviors that are distinct from others in the updated model space. The second method further compacts the model space by focusing on portions of the behavioral predictions. Specifically, we cluster actionally equivalent models that prescribe identical actions at a single time step. Exactly identifying the equivalences would require us to solve all models in the initial set. We avoid this by selectively solving some of the models, thereby introducing an approximation. We discuss the error introduced by the approximation, and empirically demonstrate the improved efficiency in solving I-DIDs due to the equivalences. {\textcopyright} 2012 AI Access Foundation.},
annote = {Use IDIDs Interactive dynamic influence diagrams. In effect, a graphical representation of a IPOMDP. 

Made up of chance nodes, value nodes and decision nodes, but rather than decisoin nodes, they contain model nodes, which in turn can encapsulate nested levels of decison making which allow for reasoning over other agents. 

The se to of beliefs over the correct IDID is large, hence in order to minisie this they suggest two alterations, they group behvariouslly similar and actionally similar models, meaning limited updates. 

As nested levels of beliefs and states need to be solved it is computationally very hard, and so an approximation must be used, which can cause an error. Even then, generalising this framework beyond N>2 agents could post a problem.

However, it does show merit in the concept of grouping similar models in order to limit necessary updates. However, technically speaking, in the multi-agent setting, the time saved by clustering on action, and clustering based on behvaiour (i.e. intrinsic similarity) can be noted...},
author = {Zeng, Y and Doshi, P},
doi = {10.1613/jair.3461},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Zeng, Doshi - 2012 - Exploiting model equivalences for solving interactive dynamic influence diagrams.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {jan},
pages = {211--255},
title = {{Exploiting model equivalences for solving interactive dynamic influence diagrams}},
volume = {43},
year = {2012}
}
@inproceedings{Hadjinikolis,
abstract = {A strategy is used by a participant in a persuasion dialogue to select locutions most likely to achieve its objective of persuading its opponent. Such strategies often assume that the participant has a model of its opponents, which may be constructed on the basis of a participant's accumulated dialogue experience. However in most cases the fact that an agent's experience may encode additional information which if appropriately used could increase a strategy's efficiency, is neglected. In this work, we rely on an agent's experience to define a mechanism for augmenting an opponent model with information likely to be dialectally related to information already contained in it. Precise computation of this likelihood is exponential in the volume of related information. We thus describe and evaluate an approximate approach for computing these likelihoods based on Monte-Carlo simulation.},
author = {Hadjinikolis, Christos and Siantos, Yiannis and Modgil, Sanjay and Black, Elizabeth and McBurney, Peter},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Hadjinikolis et al. - 2013 - Opponent modelling in persuasion dialogues.pdf:pdf},
isbn = {9781577356332},
issn = {10450823},
pages = {164--170},
title = {{Opponent modelling in persuasion dialogues}},
year = {2013}
}
@article{goertzel2007adaptive,
author = {Goertzel, B and Wang, P},
journal = {Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms},
pages = {94},
title = {{Adaptive algorithmic hybrids for human-level artificial intelligence}},
year = {2007}
}
@article{Jensen2011,
abstract = {This paper provides a survey on probabilistic decision graphs for modeling and solving decision problems under uncertainty. We give an introduction to influence diagrams, which is a popular framework for representing and solving sequential decision problems with a single decision maker. As the methods for solving influence diagrams can scale rather badly in the length of the decision sequence, we present a couple of approaches for calculating approximate solutions. The modeling scope of the influence diagram is limited to so-called symmetric decision problems. This limitation has motivated the development of alternative representation languages, which enlarge the class of decision problems that can be modeled efficiently. We present some of these alternative frameworks and demonstrate their expressibility using several examples. Finally, we provide a list of software systems that implement the frameworks described in the paper.},
author = {Jensen, F V and Thomas, {\textperiodcentered} and Nielsen, D},
doi = {10.1007/s10288-011-0159-7},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Jensen, Thomas, Nielsen - 2011 - Probabilistic decision graphs for optimization under uncertainty.pdf:pdf},
journal = {Springer},
keywords = {Influence diagrams,Probabilistic decision graphs {\textperiodcentered},Survey {\textperiodcentered}},
number = {1},
pages = {1--28},
publisher = {Springer Verlag},
title = {{Probabilistic decision graphs for optimization under uncertainty}},
url = {https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s10288-011-0159-7&casa_token=AqVd7Y5xzfsAAAAA:xGw_664Qs2TO1CQxvwlOBjW1f_cHdNZ03Var7dzq0-BHmJA0iSeShqedNw-eDGLFWAiMiLpkkUaCwLeFxlc},
volume = {9},
year = {2011}
}
@article{Tan2011,
abstract = {This paper presents a hybrid agent architecture that integrates the behaviours of BDI agents, specifically desire and intention, with a neural network based reinforcement learner known as Temporal Difference-Fusion Architecture for Learning and COgNition (TD-FALCON). With the explicit maintenance of goals, the agent performs reinforcement learning with the awareness of its objectives instead of relying on external reinforcement signals. More importantly, the intention module equips the hybrid architecture with deliberative planning capabilities, enabling the agent to purposefully maintain an agenda of actions to perform and reducing the need of constantly sensing the environment. Through reinforcement learning, plans can also be learned and evaluated without the rigidity of user-defined plans as used in traditional BDI systems. For intention and reinforcement learning to work cooperatively, two strategies are presented for combining the intention module and the reactive learning module for decision making in a real time environment. Our case study based on a minefield navigation domain investigates how the desire and intention modules may cooperatively enhance the capability of a pure reinforcement learner. The empirical results show that the hybrid architecture is able to learn plans efficiently and tap both intentional and reactive action execution to yield a robust performance. {\textcopyright} 2011 Elsevier Ltd. All rights reserved.},
author = {Tan, Ah Hwee and Ong, Yew Soon and Tapanuj, Akejariyawong},
doi = {10.1016/j.eswa.2011.01.045},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Tan, Ong, Tapanuj - 2011 - A hybrid agent architecture integrating desire, intention and reinforcement learning.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {BDI architecture,Minefield navigation,Plan learning,Reinforcement learning,Self-organizing neural networks},
month = {jul},
number = {7},
pages = {8477--8487},
publisher = {Pergamon},
title = {{A hybrid agent architecture integrating desire, intention and reinforcement learning}},
volume = {38},
year = {2011}
}
@article{DInverno2004a,
abstract = {The Procedural Reasoning System (PRS) is the best established agent architecture currently available. It has been deployed in many major industrial applications, ranging from fault diagnosis on the space shuttle to air traffic management and business process control. The theory of PRS-like systems has also been widely studied: within the intelligent agents research community, the belief-desire-intention (BDI) model of practical reasoning that underpins PRS is arguably the dominant force in the theoretical foundations of rational agency. Despite the interest in PRS and BDI agents, no complete attempt has yet been made to precisely specify the behaviour of real PRS systems. This has led to the development of a range of systems that claim to conform to the PRS model, but which differ from it in many important respects. Our aim in this paper is to rectify this omission. We provide an abstract formal model of an idealised dMARS system (the most recent implementation of the PRS architecture), which precisely defines the key data structures present within the architecture and the operations that manipulate these structures. We focus in particular on dMARS plans, since these are the key tool for programming dMARS agents. The specification we present will enable other implementations of PRS to be easily developed, and will serve as a benchmark against which future architectural enhancements can be evaluated.},
author = {D'Inverno, Mark and Luck, Michael and Georgeff, Michael and Kinny, David and Wooldridge, Michael},
doi = {10.1023/B:AGNT.0000019688.11109.19},
issn = {13872532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Agent architectures,BDI,Formal specification,Procedural reasoning system},
month = {nov},
number = {1-2},
pages = {5--53},
title = {{The dMARS architecture: A specification of the distributed multi-agent reasoning system}},
volume = {9},
year = {2004}
}
@article{Everett2018,
abstract = {Humans, like all animals, both cooperate and compete with each other. Through these interactions we learn to observe, act, and manipulate to maximise our utility function, and continue doing so as others learn with us. This is a decentralised non-stationary learning problem, where to survive and flourish an agent must adapt to the gradual changes of other agents as they learn, as well as capitalise on sudden shifts in their behaviour. To learn in the presence of such non-stationarity, we introduce the Switching Agent Model (SAM) that combines traditional deep reinforcement learning-which typically performs poorly in such settings-with opponent modelling, using uncertainty estimations to robustly switch between multiple policies. We empirically show the success of our approach in a multi-agent continuous-action environment, demonstrating SAM's ability to identify, track, and adapt to gradual and sudden changes in the behaviour of non-stationary agents.},
author = {Everett, Richard and Roberts, Stephen},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Everett, Roberts - 2018 - Learning Against Non-Stationary Agents with Opponent Modelling and Deep Reinforcement Learning(2).pdf:pdf},
journal = {{AAAI} Spring Symposia},
keywords = {Inference,Learning,and Control of Multi-Agent Systems},
pages = {621--626},
title = {{Learning Against Non-Stationary Agents with Opponent Modelling and Deep Reinforcement Learning}},
url = {www.aaai.org},
year = {2018}
}
@article{safety_to_storytelling_winfield,
author = {Winfield, A},
journal = {Frontiers in Robotics and {AI}},
title = {{Experiments in Artificial Theory of Mind: From Safety to Story-Telling}},
volume = {2018},
year = {2018}
}
@inproceedings{Fagundes2014,
abstract = {Plan recognition has been widely used in agents that need to infer which plans are being executed or which activities are being performed by others. In many applications, reasoning and acting in response to plan recognition requires time. In such systems, plan recognition is expected to be made not only with precision, but also in a timely fashion. When recognition cannot be made in time, an agent attempting to recognize plans can interact with the observed agents to disambiguate multiple hypotheses. However, such an intrusive behavior is often not possible, very costly, or undesirable. In this paper, we focus on the problem of deciding when to interact with the observed agents in order to determine their plans under execution. To tackle this problem, we develop a plan recognizer that, on one hand is the least intrusive possible, and on the other hand attempts to recognize the plans of the observed agents with precision as soon as possible and no later than it is viable to respond to the recognized plan.},
author = {Fagundes, M S and Meneguzzi, F and Bordini, R H. and Vieira},
booktitle = {13th International Conference on Autonomous Agents and Multiagent Systems, AAMAS 2014},
isbn = {9781634391313},
keywords = {Multiagent systems,Plan disambiguation,Plan recognition},
pages = {389--396},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems (IFAAMAS)},
title = {{Dealing with ambiguity in plan recognition under time constraints}},
volume = {1},
year = {2014}
}
@inproceedings{Situational_awareness_cars,
abstract = {Assuring safety of autonomous vehicles operating in an open environment requires reliable situation awareness, action planning and prediction of actions of other vehicles and objects. Factors that also have to be considered are certainty and completeness of available information and trust in information sources and other entities. The paper discusses the problem of autonomous vehicle safety assurance and proposes dynamic situation assessment to cope with the problem of environment dynamics and incomplete and uncertain situation knowledge. The approach is presented for a simple example of a simulated autonomous vehicle. The situation awareness model and autonomous vehicle control system architecture is presented. The problems of justifying system safety are discussed.},
address = {Berlin, Heidelberg},
author = {Wardzi{\'{n}}ski, A},
booktitle = {Computer Safety, Reliability, and Security},
editor = {G{\'{o}}rski, J},
isbn = {978-3-540-45763-3},
pages = {205--218},
publisher = {Springer Berlin Heidelberg},
title = {{The Role of Situation Awareness in Assuring Safety of Autonomous Vehicles}},
year = {2006}
}
@misc{Deka2020,
abstract = {—Multi agent strategies in mixed cooperative-competitive environments can be hard to craft by hand because each agent needs to coordinate with its teammates while competing with its opponents. Learning based algorithms are appealing but many scenarios require heterogeneous agent behavior for the team's success and this increases the complexity of the learning algorithm. In this work, we develop a competitive multi agent environment called FortAttack in which two teams compete against each other. We corroborate that modeling agents with Graph Neural Networks and training them with Reinforcement Learning leads to the evolution of increasingly complex strategies for each team. We observe a natural emergence of heterogeneous behavior amongst homogeneous agents when such behavior can lead to the team's success. Such heterogeneous behavior from homogeneous agents is appealing because any agent can replace the role of another agent at test time. Finally, we propose ensemble training, in which we utilize the evolved opponent strategies to train a single policy for friendly agents.},
archivePrefix = {arXiv},
arxivId = {2007.03102},
author = {Deka, Ankur and Sycara, Katia},
booktitle = {arXiv},
eprint = {2007.03102},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Deka, Sycara - 2020 - Natural Emergence of Heterogeneous Strategies in Artificially Intelligent Competitive Teams.pdf:pdf},
issn = {23318422},
keywords = {Graph Neural Networks (GNNs),Multi Agent Reinforcement Learning (MARL)},
month = {jul},
publisher = {arXiv},
title = {{Natural Emergence of Heterogeneous Strategies in Artificially Intelligent Competitive Teams}},
url = {https://youtu.be/ltHgKYc0F-E},
year = {2020}
}
@misc{Rosbach2020,
abstract = {General-purpose trajectory planning algorithms for automated driving utilize complex reward functions to perform a combined optimization of strategic, behavioral, and kinematic features. The specification and tuning of a single reward function is a tedious task and does not generalize over a large set of traffic situations. Deep learning approaches based on path integral inverse reinforcement learning have been successfully applied to predict local situation-dependent reward functions using features of a set of sampled driving policies. Sample-based trajectory planning algorithms are able to approximate a spatio-temporal subspace of feasible driving policies that can be used to encode the context of a situation. However, the interaction with dynamic objects requires an extended planning horizon, which requires sequential context modeling. In this work, we are concerned with the sequential reward prediction over an extended time horizon. We present a neural network architecture that uses a policy attention mechanism to generate a low-dimensional context vector by concentrating on trajectories with a human-like driving style. Besides, we propose a temporal attention mechanism to identify context switches and allow for stable adaptation of rewards. We evaluate our results on complex simulated driving situations, including other vehicles. Our evaluation shows that our policy attention mechanisms learns to focus on collision free policies in the configuration space. Furthermore, the temporal attention mechanism learns persistent interaction with other vehicles over an extended planning horizon.},
archivePrefix = {arXiv},
arxivId = {2007.05798},
author = {Rosbach, Sascha and Li, Xing and Gro{\ss}johann, Simon and Homoceanu, Silviu and Roth, Stefan},
booktitle = {arXiv},
eprint = {2007.05798},
title = {{Planning on the fast lane: Learning to interact using attention mechanisms in path integral inverse reinforcement learning}},
url = {https://deepai.org/publication/planning-on-the-fast-lane-learning-to-interact-using-attention-mechanisms-in-path-integral-inverse-reinforcement-learning},
year = {2020}
}
@inproceedings{Russell89,
author = {Russell, S J and Wefald, E},
booktitle = {Proceedings of the 1st International Conference on Principles of Knowledge Representation and Reasoning (KR'89). Toronto, Canada, May 15-18 1989},
pages = {400--411},
title = {{Principles of Metareasoning}},
year = {1989}
}
@inproceedings{tefanSarkadi,
abstract = {Applying Theory of Mind to multi-agent systems enables agents to model and reason about other agents' minds. Recent work shows that this ability could increase the performance of agents, making them more efficient than agents that lack this ability. However, modelling others agents' minds is a difficult task, given that it involves many factors of uncertainty, e.g., the uncertainty of the communication channel, the uncertainty of reading other agents correctly, and the uncertainty of trust in other agents. In this paper, we explore how agents acquire and update Theory of Mind under conditions of uncertainty. To represent uncertain Theory of Mind, we add probability estimation on a formal semantics model for agent communication based on the BDI architecture and agent communication languages.},
author = {Sarkadi, Ştefan and Panisson, Alison R and Bordini, Rafael H and McBurney, Peter and Parsons, Simon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-17294-7_1},
isbn = {9783030172930},
issn = {16113349},
keywords = {Multi-agent systems,Socially-aware AI,Theory of Mind,Uncertainty},
pages = {3--17},
title = {{Towards an Approach for Modelling Uncertain Theory of Mind in Multi-Agent Systems}},
url = {https://doi.org/10.1007/978-3-030-17294-7_1},
volume = {11327 LNAI},
year = {2019}
}
@article{Adam2020,
abstract = {Modelling and simulation have long been dominated by equation-based approaches, until the recent advent of agent-based approaches. To curb the resulting complexity of models, Axelrod promoted the KISS principle: 'Keep It Simple, Stupid'. But the community is divided and a new principle appeared: KIDS, 'Keep It Descriptive, Stupid'. Richer models were thus developed for a variety of phenomena, while agent cognition still tends to be modelled with simple reactive particle-like agents. This is not always appropriate, in particular in the social sciences trying to account for the complexity of human behaviour. One solution is to model humans as belief, desire and intention (BDI) agents, an expressive paradigm using concepts from folk psychology, making it easier for modellers and users to understand the simulation. This paper provides a methodological guide to the use of BDI agents in social simulations , and an overview of existing methodologies and tools for using them.},
author = {Adam, C and Gaudou, B},
doi = {10.1017/S0269888916000096},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Adam, Gaudou - 2020 - BDI agents in social simulations a survey.pdf:pdf},
title = {{BDI agents in social simulations: a survey}},
url = {https://doi.org/10.1017/S0269888916000096},
year = {2020}
}
@article{Weitz2020,
abstract = {While the research area of artificial intelligence benefited from increasingly sophisticated machine learning techniques in recent years, the resulting systems suffer from a loss of transparency and comprehensibility, especially for end-users. In this paper, we explore the effects of incorporating virtual agents into explainable artificial intelligence (XAI) designs on the perceived trust of end-users. For this purpose, we conducted a user study based on a simple speech recognition system for keyword classification. As a result of this experiment, we found that the integration of virtual agents leads to increased user trust in the XAI system. Furthermore, we found that the user's trust significantly depends on the modalities that are used within the user-agent interface design. The results of our study show a linear trend where the visual presence of an agent combined with a voice output resulted in greater trust than the output of text or the voice output alone. Additionally, we analysed the participants' feedback regarding the presented XAI visualisations. We found that increased human-likeness of and interaction with the virtual agent are the two most common mention points on how to improve the proposed XAI interaction design. Based on these results, we discuss current limitations and interesting topics for further research in the field of XAI. Moreover, we present design recommendations for virtual agents in XAI systems for future projects.},
author = {Weitz, K and Schiller, D and Schlagowski, R and Huber, T and Andr{\'{e}}, E},
doi = {10.1007/s12193-020-00332-0},
issn = {17838738},
journal = {Journal on Multimodal User Interfaces},
keywords = {Deep learning,Explainable artificial intelligence,Human-agent interaction,Interpretable artificial intelligence,Trust,Virtual agents},
publisher = {Springer},
title = {{“Let me explain!”: exploring the potential of virtual agents in explainable AI interaction design}},
year = {2020}
}
@inproceedings{luck_inverno_agency,
author = {Luck, M and D'Inverno, M and Others},
booktitle = {Proceedings of the first international conference on multi-agent systems ({ICMAS})},
pages = {254--260},
title = {{A Formal Framework for Agency and Autonomy.}},
volume = {95},
year = {1995}
}
@misc{Brown,
abstract = {In recent years there have been great strides in artificial intelligence (AI), with games often serving as challenge problems, benchmarks, and milestones for progress. Poker has served for decades as such a challenge problem. Past successes in such benchmarks, including poker, have been limited to two-player games. However, poker in particular is traditionally played with more than two players. Multiplayer games present fundamental additional issues beyond those in two-player games, and multiplayer poker is a recognized AI milestone. In this paper we present Pluribus, an AI that we show is stronger than top human professionals in six-player no-limit Texas hold'em poker, the most popular form of poker played by humans.},
author = {Brown, Noam and Sandholm, Tuomas},
booktitle = {Science},
doi = {10.1126/science.aay2400},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Brown, Sandholm - 2019 - Superhuman AI for multiplayer poker.pdf:pdf},
issn = {10959203},
number = {6456},
pages = {885--890},
pmid = {31296650},
title = {{Superhuman AI for multiplayer poker}},
url = {http://science.sciencemag.org/},
volume = {365},
year = {2019}
}
@article{Shoham_agent_oriented_programming,
author = {Shoham, Y},
doi = {10.1016/0004-3702(93)90034-9},
journal = {Artificial Intelligence},
number = {1},
pages = {51--92},
title = {{Agent-Oriented Programming}},
url = {https://doi.org/10.1016/0004-3702(93)90034-9},
volume = {60},
year = {1993}
}
@misc{Chen2019,
abstract = {We introduce Deep Reasoning Networks (DRNets), an end-to-end framework that combines deep learning with reasoning for solving complex tasks, typically in an unsupervised or weakly-supervised setting. DRNets exploit problem structure and prior knowledge by tightly combining logic and constraint reasoning with stochastic-gradient-based neural network optimization. We illustrate the power of DRNets on de-mixing overlapping hand-written Sudokus (Multi-MNIST-Sudoku) and on a substantially more complex task in scientific discovery that concerns inferring crystal structures of materials from X-ray diffraction data under thermodynamic rules (Crystal-Structure-Phase-Mapping). At a high level, DRNets encode a structured latent space of the input data, which is constrained to adhere to prior knowledge by a reasoning module. The structured latent encoding is used by a generative decoder to generate the targeted output. Finally, an overall objective combines responses from the generative decoder (thinking fast) and the reasoning module (thinking slow), which is optimized using constraint-aware stochastic gradient descent. We show how to encode different tasks as DRNets and demonstrate DRNets' effectiveness with detailed experiments: DRNets significantly outperform the state of the art and experts' capabilities on Crystal-Structure-Phase-Mapping, recovering more precise and physically meaningful crystal structures. On MultiMNIST-Sudoku, DRNets perfectly recovered the mixed Sudokus' digits, with 100% digit accuracy, outperforming the supervised state-of-the-art MNIST de-mixing models. Finally, as a proof of concept, we also show how DRNets can solve standard combinatorial problems - 9-by-9 Sudoku puzzles and Boolean satisfiability problems (SAT), outperforming other specialized deep learning models. DRNets are general and can be adapted and expanded to tackle other tasks.},
archivePrefix = {arXiv},
arxivId = {1906.00855},
author = {Chen, Di and Bai, Yiwei and Zhao, Wenting and Ament, Sebastian and Gregoire, John M and Gomes, Carla P},
booktitle = {arXiv},
eprint = {1906.00855},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2019 - Deep reasoning networks Thinking fast and slow.pdf:pdf},
issn = {23318422},
title = {{Deep reasoning networks: Thinking fast and slow}},
year = {2019}
}
@book{Lockett2007,
abstract = {Opponent models are necessary in games where the game state is only partially known to the player, since the player must infer the state of the game based on the opponent's actions. This paper presents an architecture and a process for developing neural network game players that utilize explicit opponent models in order to improve game play against unseen opponents. The model is constructed as a mixture over a set of cardinal opponents, i.e. opponents that represent maximally distinct game strategies. The model is trained to estimate the likelihood that the opponent will make the same move as each of the cardinal opponents would in a given game situation. Experiments were performed in the game of Guess It, a simple game of imperfect information that has no optimal strategy for defeating specific opponents. Opponent modeling is therefore crucial to play this game well. Both opponent modeling and game-playing neural networks were trained using NeuroEvolution of Augmenting Topologies (NEAT). The results demonstrate that game-playing provided with the model outperform networks not provided with the model when played against the same previously unseen opponents. The "cardinal mixture" architecture therefore constitutes a promising approach for general and dynamic opponent modeling in game-playing.},
annote = {Lockett make clear the distinction between implicit (simply trained nets on some environment) and explicit nets, which are trained to predict certain characteristics of an agent. 

An explicit model, he argues, generalises far better than an implicit one, when exposed to unknown or unexpected circumstances. 

They argue action-prediction with complex and unseen opponents isn't a good metric, a better approach is to notice characteristics, and tailor that into the response. 

They define a set of four cardinal opponents which generate an opponent space. Any opponent would then fall somewhere in this space. 

A reasonable opponent model would then correctly identify where in this space the opponent falls, and then combine aspects of the four cardinal styles of play to tailor a strategy. 

They do this by generating a mixture-opponent. 

They test it in a game of guess it, where mixed strategies, played by a rational opponent with no knoweldge of anoter agent tends to be beaten by less rational strategies.... basically it is a game which requires good opponewnt modelling, whereas equilibrium based strategies tend not to occur. 

So, basically, train a network to identifdy the correct mixture given an opponents actions, and then train a player to act optimally given the opponent and the game-state.},
author = {Lockett, Alan J and Chen, Charles L and Miikkulainen, Risto},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Lockett, Chen, Miikkulainen - 2007 - Evolving Explicit Opponent Models in Game Playing.pdf:pdf},
isbn = {9781595936974},
keywords = {AI,Experimentation,F11 [Theory of Computation]: Models of Computation,Theory Keywords Opponent modeling,artificial intelligence,evolutionary computation,games of imperfect information,neural networks,neuroevolution},
title = {{Evolving Explicit Opponent Models in Game Playing}},
year = {2007}
}
@inproceedings{OM_negotiation,
author = {Tunali, O and Aydougan, R and Sanchez-Anguix, V},
booktitle = {International Conference on Principles and Practice of Multi-Agent Systems},
organization = {Springer},
pages = {263--279},
title = {{Rethinking frequency opponent modeling in automated negotiation}},
year = {2017}
}
@inproceedings{open_systems_learning_classification,
abstract = {Open systems are becoming increasingly important in a variety of distributed, networked computer applications. Their characteristics, such as agent diversity, heterogeneity and fluctuation, confront multiagent learning with new challenges. This paper presents the interaction learning meta-architecture InFFrA as one possible answer to these challenges, and introduces the opponent classification heuristic ADHOC as a concrete multiagent learning method that has been designed on the basis of InFFrA. Extensive experimental validation proves the adequacy of ADHOC in a scenario of iterated multiagent games and underlines the usefulness of schemas such as InFFrA specifically tailored for open multiagent learning environments. At the same time, limitations in the performance of ADHOC suggest further improvements to the methods used here. Also, the results obtained from this study allow more general conclusions regarding the problems of learning in open systems to be drawn. {\textcopyright} Springer-Verlag Berlin Heidelberg 2003.},
annote = {Major paper for arguing that swift is necessary. 

In an open system, the cost of acquiring an opponent model is often not worth it due to sparse (and lack of repeated) interactions. 
The cognition-based view of OM in MAS is perhaps infeasible. 

Need to focus on social learning: i.e. learning different classes of interactions, and transferring data from one interaction to another. 

They propose InFFrA -- meta model/framework for learning. Can be used with a number of machine learning algorithms. 

They present AdHOC -- adaptive heuristic for opponent classification. 

Agent is goal-rational (not=-utility rational): deliberative agent. 
In order to acheive goals, must learn a model of the environment, in some form of cause-effect model...

Argues that in open systems, agents are:
Behaviorally Diverse: 
they are granted more degrees of freedom in their behaviour (i.e. cooperative/competeive/ malicious) and a wider choice of moves. (E.. in a poker game it is clear the scenario is competetive, in an open system, the agent might be looking to cooperate or not...
Agents heterogeneous: 
larger variety of agent types and models.... i.e. lots of different types to maintain, and different types of learning necessary to facilitate modelling them. 
Agent fluctuation: 
agents can enter and leave, and can cause problems. Also traditional exploration exploitation devides are harder to compute: point of exploring and exploitin if likely to bugger off. 

They refer to this as the contingent acquantances problem. 

They frame InFFRa as a social view of learning, in which they view the world as frames: frames are basically indicating what is going on in a certain context, which enables the agent to act in a competent manner. 

Frames describe recurring categories of interaction, rather than the properties of individual agents. 

In InFFRA, a frame is a data structure which describes;
1) information trajectories (where it is likely to go)
2) roles and relationships between the parties involved. 
3) contexts (start and end states)
4) beliefs --- epistemic information regarding each party... 


Has a control flow for causal reasoning:
active frame, 
the percieved frame, 
the difference model (highlighting differences between the active and percieved frame)
trial-frame --- the current hypothesis when alternatives for the current frame a sought,
the frame-repo: database of frames. 

control model has the following steps:
matching (comparse active with percieved)
assessment (assess usability of current frame)
framing decision (retain active frame, or switch)
re-framing (search for another frame)
adaptation (modify frames in frame repo if no candidate can be found). 

It seems to be based off a modified BDI style architecture...

Very thorough framework... but relies on underlying machine learning stuff... 


---------
AD hoc used Carmel and Markovic's US * Ls algo which modelles opponents from scratch as a DFA. 

----

In practice this looks to be like type-based reasoning --- basically it learns opponent models, and then stores them in frames (types), and then uses case-based reasoning to type agents, and then apply optimal policies learned using Q learning on their opponent models...

So, it is a sophisticated form of type-based reasoning (framing as types) with encapsulated elements of policy reconstruction.

---
They employ a heuristic which creates new classes for unknown agents, or matches them to known classes. After every encounter, the best candidate classes for the currently encountered agent are those that are able to best predict past encounters with it. At the same time, good candidates have to be models which have been reliable in the past and have low computational cost.


As expected, in games of the iterated prisoners dilemma, it performed faster and better than most. Over the course of the interactions, the unbounded rational agent, which learns models for each agent it meets, was growing, however performance was slower --- social view is shown to be more effective than single-agent/cognitoin based approach. 

However --- learning about classes which are fixed, especially when there are only four classes, means that realistically, after four interactions, one will have learned all models... and then is simply an application of type-based reasoning... shows empirical results for},
author = {Rovatsos, M and Wei{\ss}, G and Wolf, M},
booktitle = {Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)},
doi = {10.1007/3-540-44826-8_5},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Rovatsos, Wei{\ss}, Wolf - 2003 - Multiagent learning for open systems A study in opponent classification.pdf:pdf},
issn = {03029743},
pages = {66--87},
publisher = {Springer Verlag},
title = {{Multiagent learning for open systems: A study in opponent classification}},
volume = {2636},
year = {2003}
}
@article{Bosse2011,
abstract = {This article discusses a formal belief, desire, intention (BDI)-based agent model for theory of mind (ToM). The model uses BDI concepts to describe the reasoning process of an agent that reasons about the reasoning process of another agent, which is also based on BDI concepts. We discuss three different application areas and illustrate how the model can be applied to each of them. We explore a case study for each of the application areas and apply our model to it. For each case study, a number of simulation experiments are described, and their results are discussed. Copyright {\textcopyright} 2011 Taylor & Francis Group, LLC.},
author = {Bosse, Tibor and Memon, Zulfiqar A. and Treur, Jan},
doi = {10.1080/08839514.2010.529259},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Bosse, Memon, Treur - 2011 - A recursive BDI agent model for theory of mind and its applications.pdf:pdf},
issn = {08839514},
journal = {Applied Artificial Intelligence},
keywords = {Applications,BDI-agents,Mindreading,Theory of Mind},
number = {1},
pages = {1--44},
title = {{A recursive BDI agent model for theory of mind and its applications}},
volume = {25},
year = {2011}
}
@inproceedings{Best_response_method_albrecht_ramamoorthy2013,
annote = {Introduces the problems by saying they are looking at flexibility and efficiency: desired qualities for cooperation with unknown agents in unknown domains.
Felxibility: heterogenous agents
Adaptability: swiftly. 

Adapting game-theoretic notions like Bayesian game approaches have drawbacks to cooperateive approaches: finding an equilibrium is the focus, not on efficiency. 

They combine them together: cooperation and game-theoretic constructs. 

stochastic bayesian game: players behaviour is based on it's type. Turns into a constrained optimisaiton problem: need to optimise flexibility and efficiency constrinaed by an agents unknown type. 

So Bellman-Harsyani ad hoc coordination was born..... 
Basically, combining type-based reasoning about agents into a stateful (stochastic) environment, which one can then use some plannin procedure to solve... it seems...also relies on the assumption that the possible type-space of all agents is common knowledge. 

They include conceptual types if the user-defined type-space is incorrect (based on a similarity metric).},
author = {Albrecht, S V and Ramamoorthy, S},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
keywords = {ad hoc coordination,harsanyi-bellman ad hoc coordination (HBA),stochastic bayesian games (SBG)},
pages = {1155--1156},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '13},
title = {{A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc Coordination in Multiagent Systems}},
year = {2013}
}
@inproceedings{Rovatsos2002,
address = {New York, New York, USA},
author = {Rovatsos, M and Weiss, G and Wolf, M},
booktitle = {Proceedings of the first international joint conference on Autonomous agents and multiagent systems part 2 - AAMAS '02},
doi = {10.1145/544862.544906},
pages = {682},
publisher = {Association for Computing Machinery (ACM)},
title = {{An approach to the analysis and design of multiagent systems based on interaction frames}},
url = {http://portal.acm.org/citation.cfm?doid=544862.544906},
year = {2002}
}
@book{AI:modern_approach_2003,
author = {Russell, S J and Norvig, P},
isbn = {0130803022},
publisher = {Prentice Hall},
series = {Prentice Hall series in artificial intelligence},
title = {{Artificial intelligence - a modern approach, 2nd Edition}},
url = {http://www.worldcat.org/oclc/314283679},
year = {2003}
}
@article{Hernandez_leal_opponent_switches,
author = {Hernandez-Leal, P and Zhan, Y and Taylor, M and {Enrique Sucar}, L and de Cote, E},
doi = {10.1007/s10458-016-9352-6},
journal = {Autonomous Agents and Multi-Agent Systems},
number = {4},
pages = {767--789},
title = {{Efficiently detecting switches against non-stationary opponents}},
url = {https://doi.org/10.1007/s10458-016-9352-6},
volume = {31},
year = {2017}
}
@article{Ahmed2019,
author = {Ahmed, M and Sriram, A and Economics, S Singh},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Ahmed, Sriram, Economics - 2019 - Short term firm-specific stock forecasting with BDI framework.pdf:pdf},
journal = {Springer: Computational Economics},
keywords = {Stock Forecasting},
mendeley-tags = {Stock Forecasting},
pages = {745----778},
title = {{Short term firm-specific stock forecasting with BDI framework}},
url = {https://link.springer.com/article/10.1007/s10614-019-09911-0},
volume = {55},
year = {2019}
}
@article{Ferguson1992,
author = {Ferguson, I A},
doi = {10.1109/2.144395},
issn = {00189162},
journal = {Computer},
number = {5},
pages = {51--55},
title = {{Touring Machines: Autonomous Agents with Attitudes}},
volume = {25},
year = {1992}
}
@incollection{brown_1951,
author = {Brown, G},
booktitle = {Proceedings of the Conference on Activity Analysis of Production and Allocation},
pages = {374--376},
publisher = {Cowles Commission Monograph},
title = {{No Title}},
volume = {13},
year = {1951}
}
@article{Maes_ana,
author = {Maes, P},
doi = {10.1145/122344.122367},
journal = {{SIGART} Bullitin},
number = {4},
pages = {115--120},
title = {{The Agent Network Architecture {(ANA)}}},
url = {https://doi.org/10.1145/122344.122367},
volume = {2},
year = {1991}
}
@article{autonomy_internet_of_things,
author = {Janiesch, C and Fischer, M and Winkelmann, A and Nentwich, V},
doi = {10.1007/s10257-018-0379-x},
journal = {Inf. Syst. E-Business Management},
number = {1},
pages = {159--194},
title = {{Specifying autonomy in the Internet of Things: the autonomy model and notation}},
url = {https://doi.org/10.1007/s10257-018-0379-x},
volume = {17},
year = {2019}
}
@misc{Konighofer,
abstract = {In this paper, we propose a method to develop trustworthy reinforcement learning systems. To ensure safety especially during exploration, we automatically synthesize a correct-by-construction runtime enforcer, called a shield, that blocks all actions that are unsafe with respect to a temporal logic specification from the agent. Our main contribution is a new synthesis algorithm for computing the shield online. Existing offline shielding approaches compute exhaustively the safety of all states-action combinations ahead-of-time, resulting in huge offline computation times, large memory consumption, and significant delays at run-time due to the look-ups in a huge database. The intuition behind online shielding is to compute during run-time the set of all states that could be reached in the near future. For each of these states, the safety of all available actions is analysed and used for shielding as soon as one of the considered states is reached. Our proposed method is general and can be applied to a wide range of planning problems with stochastic behavior. For our evaluation, we selected a 2-player version of the classical computer game Snake. The game requires fast decisions and the multiplayer setting induces a large state space, computationally expensive to analyze exhaustively. The safety objective of collision avoidance is easily transferable to a variety of planning tasks.},
archivePrefix = {arXiv},
arxivId = {2012.09539},
author = {K{\"{o}}nighofer, Bettina and Rudolf, Julian and Palmisano, Alexander and Tappler, Martin and Bloem, Roderick},
booktitle = {arXiv},
eprint = {2012.09539},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/K{\"{o}}nighofer et al. - 2020 - Online shielding for stochastic systems.pdf:pdf},
issn = {23318422},
title = {{Online shielding for stochastic systems}},
year = {2020}
}
@inproceedings{Argerich2020,
abstract = {We introduce Tutor4RL, a method to improve reinforcement learning (RL) performance during training, using external knowledge to guide the agents' decisions and experience. Current approaches of RL need extensive experience to deliver good performance, something that is not acceptable in many real systems when no simulation environment or considerable previous data are available. In Tutor4RL, external knowledge– such as expert or domain knowledge– is expressed as programmable functions that are fed to the RL agent. During its first steps, the agent uses these knowledge functions to decide the best action, guiding its exploration and providing better performance from the start. As the agent gathers experience, it increasingly exploits its learned policy, eventually leaving its tutor behind. We demonstrate Tutor4RL with a DQN agent. In our tests, Tutor4RL achieves more than 3 times higher reward in the beginning of its training than an agent with no external knowledge.},
author = {Argerich, Mauricio Fadel and F{\"{u}}rst, Jonathan and Cheng, Bin},
booktitle = {CEUR Workshop Proceedings},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Argerich, F{\"{u}}rst, Cheng - 2020 - Tutor4RL Guiding reinforcement learning with external knowledge.pdf:pdf},
issn = {16130073},
title = {{Tutor4RL: Guiding reinforcement learning with external knowledge}},
volume = {2600},
year = {2020}
}
@article{Karpinskyj_video_game_personalisation_survey,
author = {Karpinskyj, S and Zambetta, F and Cavedon, L},
doi = {10.1016/j.entcom.2014.09.002},
journal = {Entertain. Comput.},
number = {4},
pages = {211--218},
title = {{Video game personalisation techniques: {A} comprehensive survey}},
url = {https://doi.org/10.1016/j.entcom.2014.09.002},
volume = {5},
year = {2014}
}
@inproceedings{Samvelyan2019,
abstract = {In the last few years, deep multi-agent reinforcement learning (RL) has become a highly active area of research. A particularly challenging class of problems in this area is partially observable, cooperative, multi-agent learning, in which teams of agents must learn to coordinate their behaviour while conditioning only on their private observations. This is an attractive research area since such problems are relevant to a large number of real-world systems and are also more amenable to evaluation than general-sum problems. Standardised environments such as the ALE and MuJoCo have allowed single-agent RL to move beyond toy domains, such as grid worlds. However, there is no comparable benchmark for cooperative multi-agent RL. As a result, most papers in this field use one-off toy problems, making it difficult to measure real progress. In this paper, we propose the StarCraft Multi-Agent Challenge (SMAC) as a benchmark problem to fill this gap.1 SMAC is based on the popular real-time strategy game StarCraft II and focuses on micromanagement challenges where each unit is controlled by an independent agent that must act based on local observations. We offer a diverse set of challenge maps and recommendations for best practices in benchmarking and evaluations. We also open-source a deep multi-agent RL learning framework including state-of-theart algorithms.2 We believe that SMAC can provide a standard benchmark environment for years to come. Videos of our best agents for several SMAC scenarios are available at: https://youtu.be/VZ7zmQ-obZ0.},
archivePrefix = {arXiv},
arxivId = {1902.04043},
author = {Samvelyan, Mikayel and Rashid, Tabish and {De Witt}, Christian Schroeder and Farquhar, Gregory and Nardelli, Nantas and Rudner, Tim G.J. and Hung, Chia Man and Torr, Philip H.S. and Foerster, Jakob and Whiteson, Shimon},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
eprint = {1902.04043},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Samvelyan et al. - 2019 - The StarCraft multi-agent challenge(2).pdf:pdf},
isbn = {9781510892002},
issn = {15582914},
keywords = {Multi-agent learning,Reinforcement learning,StarCraft},
pages = {2186--2188},
title = {{The StarCraft multi-agent challenge}},
url = {https://youtu.be/VZ7zmQ_obZ0.},
volume = {4},
year = {2019}
}
@article{Howard2005,
abstract = {No abstract available.},
author = {Howard, R A and Matheson, J E},
doi = {10.1287/deca.1050.0020},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Howard, Matheson - 2005 - Influence Diagrams.pdf:pdf},
journal = {Decisions Analysis},
keywords = {Bayes,Bayesian network,Decision Analysis 2005.2:127-143,The Principles and Applications of Decision Analys,arrow reversal,belief network,decision network,decision tree,decision-tree network,decision-tree order,expansion,expansion order,influence diagram,knowledge map,value of clairvoyance},
month = {sep},
number = {3},
pages = {127--143},
publisher = {Institute for Operations Research and the Management Sciences (INFORMS)},
title = {{Influence Diagrams}},
url = {https://pubsonline.informs.org/doi/abs/10.1287/deca.1050.0020},
volume = {2},
year = {2005}
}
@article{Rosbach2020a,
abstract = {General-purpose trajectory planning algorithms for automated driving utilize complex reward functions to perform a combined optimization of strategic, behavioral, and kinematic features. The specification and tuning of a single reward function is a tedious task and does not generalize over a large set of traffic situations. Deep learning approaches based on path integral inverse reinforcement learning have been successfully applied to predict local situation-dependent reward functions using features of a set of sampled driving policies. Sample-based trajectory planning algorithms are able to approximate a spatio-temporal subspace of feasible driving policies that can be used to encode the context of a situation. However, the interaction with dynamic objects requires an extended planning horizon, which depends on sequential context modeling. In this work, we are concerned with the sequential reward prediction over an extended time horizon. We present a neural network architecture that uses a policy attention mechanism to generate a low-dimensional context vector by concentrating on trajectories with a human-like driving style. Apart from this, we propose a temporal attention mechanism to identify context switches and allow for stable adaptation of rewards. We evaluate our results on complex simulated driving situations, including other moving vehicles. Our evaluation shows that our policy attention mechanism learns to focus on collision-free policies in the configuration space. Furthermore, the temporal attention mechanism learns persistent interaction with other vehicles over an extended planning horizon.},
author = {Rosbach, Sascha and Li, Xing and Gro{\ss}johann, Simon and Homoceanu, Silviu and Roth, Stefan},
month = {nov},
title = {{Planning on the fast lane: Learning to interact using attention mechanisms in path integral inverse reinforcement learning}},
url = {http://arxiv.org/abs/2007.05798},
year = {2020}
}
@inproceedings{Sarkadi2019,
abstract = {Applying Theory of Mind to multi-agent systems enables agents to model and reason about other agents' minds. Recent work shows that this ability could increase the performance of agents, making them more efficient than agents that lack this ability. However, modelling others agents' minds is a difficult task, given that it involves many factors of uncertainty, e.g., the uncertainty of the communication channel, the uncertainty of reading other agents correctly, and the uncertainty of trust in other agents. In this paper, we explore how agents acquire and update Theory of Mind under conditions of uncertainty. To represent uncertain Theory of Mind, we add probability estimation on a formal semantics model for agent communication based on the BDI architecture and agent communication languages.},
author = {Sarkadi, Ştefan and Panisson, Alison R. and Bordini, Rafael H. and McBurney, Peter and Parsons, Simon},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-17294-7_1},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Sarkadi et al. - 2019 - Towards an Approach for Modelling Uncertain Theory of Mind in Multi-Agent Systems.pdf:pdf},
isbn = {9783030172930},
issn = {16113349},
keywords = {Multi-agent systems,Socially-aware AI,Theory of Mind,Uncertainty},
month = {dec},
pages = {3--17},
publisher = {Springer Verlag},
title = {{Towards an Approach for Modelling Uncertain Theory of Mind in Multi-Agent Systems}},
url = {https://doi.org/10.1007/978-3-030-17294-7_1},
volume = {11327 LNAI},
year = {2019}
}
@article{bowling2002multiagent,
author = {Bowling, M and Veloso, M},
journal = {Artificial Intelligence},
number = {2},
pages = {215--250},
publisher = {Elsevier},
title = {{Multiagent learning using a variable learning rate}},
volume = {136},
year = {2002}
}
@article{ALBRECHT201866,
author = {Albrecht, S V and Stone, P},
doi = {https://doi.org/10.1016/j.artint.2018.01.002},
issn = {0004-3702},
journal = {Artificial Intelligence},
pages = {66--95},
title = {{Autonomous agents modelling other agents: A comprehensive survey and open problems}},
url = {http://www.sciencedirect.com/science/article/pii/S0004370218300249},
volume = {258},
year = {2018}
}
@article{Feldman2020,
abstract = {Deep learning algorithms are well-known to have a propensity for fitting the training data very well and often fit even outliers and mislabeled data points. Such fitting requires memorization of training data labels, a phenomenon that has attracted significant research interest but has not been given a compelling explanation so far. A recent work of Feldman (2019) proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a significant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given. In this work we design experiments to test the key ideas in this theory. The experiments require estimation of the influence of each training example on the accuracy at each test example as well as memorization values of training examples. Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled influence and memorization values can be estimated much more efficiently. Our experiments demonstrate the significant benefits of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in (Feldman, 2019).},
archivePrefix = {arXiv},
arxivId = {2008.03703},
author = {Feldman, Vitaly and Zhang, Chiyuan},
eprint = {2008.03703},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Feldman, Zhang - 2020 - What Neural Networks Memorize and Why Discovering the Long Tail via Influence Estimation.pdf:pdf},
title = {{What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation}},
url = {http://arxiv.org/abs/2008.03703},
year = {2020}
}
@inproceedings{Doshi04,
author = {Gmytrasiewicz, P and Doshi, P},
booktitle = {Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence, July 25-29, 2004, San Jose, California, {USA}},
pages = {985--986},
title = {{A Framework for Optimal Sequential Planning in Multiagent Settings}},
year = {2004}
}
@inproceedings{Leibo2017a,
abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We char-Acterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
archivePrefix = {arXiv},
arxivId = {1702.03037},
author = {Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
eprint = {1702.03037},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Leibo et al. - 2017 - Multi-agent reinforcement learning in sequential social dilemmas(3).pdf:pdf},
isbn = {9781510855076},
issn = {15582914},
keywords = {Agent-based social simulation,Cooperation,Markov games,Non-cooperative games,Social dilemmas},
pages = {464--473},
title = {{Multi-agent reinforcement learning in sequential social dilemmas}},
url = {www.ifaamas.org},
volume = {1},
year = {2017}
}
@misc{DeWitt2018,
abstract = {Cooperative multi-agent reinforcement learning often requires decentralised policies, which severely limit the agents' ability to coordinate their behaviour. In this paper, we show that common knowledge between agents allows for complex decentralised coordination. Common knowledge arises naturally in a large number of decentralised cooperative multi-agent tasks, for example, when agents can reconstruct parts of each others' observations. Since agents can independently agree on their common knowledge, they can execute complex coordinated policies that condition on this knowledge in a fully decentralised fashion. We propose multiagent common knowledge reinforcement learning (MACKRL), a novel stochastic actor-critic algorithm that learns a hierarchical policy tree. Higher levels in the hierarchy coordinate groups of agents by conditioning on their common knowledge, or delegate to lower levels with smaller subgroups but potentially richer common knowledge. The entire policy tree can be executed in a fully decentralised fashion. As the lowest policy tree level consists of independent policies for each agent, MACKRL reduces to independently learnt decentralised policies as a special case. We demonstrate that our method can exploit common knowledge for superior performance on complex decentralised coordination tasks, including a stochastic matrix game and challenging problems in StarCraft II unit micromanagement.},
archivePrefix = {arXiv},
arxivId = {1810.11702},
author = {{De Witt}, Christian A.Schroeder and Foerster, Jakob N and Farquhar, Gregory and Torr, Philip H.S. and B{\"{o}}hmer, Wendelin and Whiteson, Shimon},
booktitle = {arXiv},
eprint = {1810.11702},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/De Witt et al. - 2018 - Multi-agent common knowledge reinforcement learning.pdf:pdf},
issn = {23318422},
title = {{Multi-agent common knowledge reinforcement learning}},
year = {2018}
}
@article{DBLP:journals/corr/KotserubaGT16,
archivePrefix = {arXiv},
arxivId = {1610.08602},
author = {Kotseruba, Iuliia and Gonzalez, Oscar J Avella and Tsotsos, John K},
eprint = {1610.08602},
journal = {CoRR},
title = {{A Review of 40 Years of Cognitive Architecture Research: Focus on Perception, Attention, Learning and Applications}},
url = {http://arxiv.org/abs/1610.08602},
volume = {abs/1610.0},
year = {2016}
}
@article{Chen2020,
abstract = {In the ad hoc teamwork setting, a team of agents needs to perform a task without prior coordination. The most advanced approach learns policies based on previous experiences and reuses one of the policies to interact with new teammates. However, the selected policy in many cases is sub-optimal. Switching between policies to adapt to new teammates' behaviour takes time, which threatens the successful performance of a task. In this paper, we propose AATEAM – a method that uses the attention-based neural networks to cope with new teammates' behaviour in real-time. We train one attention network per teammate type. The attention networks learn both to extract the temporal correlations from the sequence of states (i.e. contexts) and the mapping from contexts to actions. Each attention network also learns to predict a future state given the current context and its output action. The prediction accuracies help to determine which actions the ad hoc agent should take. We perform extensive experiments to show the effectiveness of our method.},
author = {Chen, Shuo and Andrejczuk, Ewa and Cao, Zhiguang and Zhang, Jie},
doi = {10.1609/aaai.v34i05.6196},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Chen et al. - 2020 - AATEAM Achieving the Ad Hoc Teamwork by Employing the Attention Mechanism.pdf:pdf},
issn = {2159-5399},
journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
keywords = {Multiagent Systems},
month = {apr},
number = {05},
pages = {7095--7102},
publisher = {Association for the Advancement of Artificial Intelligence (AAAI)},
title = {{AATEAM: Achieving the Ad Hoc Teamwork by Employing the Attention Mechanism}},
url = {www.aaai.org},
volume = {34},
year = {2020}
}
@article{DeGalland2020,
abstract = {We consider open multi-agent systems, which are systems subject to frequent arrivals and departures of agents while the process studied takes place. We study the behavior of all-to-all pairwise gossip interactions in such open systems. Arrivals and departures of agents imply that the composition and size of the system evolve with time, and in particular prevent convergence. We describe the expected behavior of the system by showing that the evolution of scale-independent quantities can be characterized exactly by a fixed size linear dynamical system. We apply this approach to characterize the evolution of the two first moments (and thus also of the variance) for open systems of both fixed and variable size. Our approach is based on the continuous time modelling of random asynchronous events, namely gossip steps, arrivals, departures, and replacements.},
archivePrefix = {arXiv},
arxivId = {2009.02970},
author = {de Galland, Charles Monnoyer and Martin, Samuel and Hendrickx, Julien M.},
eprint = {2009.02970},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/de Galland, Martin, Hendrickx - 2020 - Open Multi-Agent Systems with Variable Size the Case of Gossiping.pdf:pdf},
keywords = {Agents and au-tonomous systems,Cooperative control,Index Terms-Open multi-agent systems,Linear systems,Markov processes},
month = {sep},
title = {{Open Multi-Agent Systems with Variable Size: the Case of Gossiping}},
url = {http://arxiv.org/abs/2009.02970},
year = {2020}
}
@article{Kantert2017,
abstract = {Information and Communication technology (ICT) pervades every aspect of our daily lives to support us solving tasks and providing information. However, we are facing an increasing complexity in ICT due to interconnectedness and coupling of large-scale distributed systems. One particular challenge in this context is openness, i.e. systems and components are free to join and leave at any time, including those that are faulty or even malicious. In this article, we present a novel concept to master openness by detecting groups of similarly behaving systems in order to identify and finally isolate malicious elements. More precisely, we present a mechanism to cluster groups of systems at runtime and to estimate their contribution to the overall system utility. For evaluation and demonstration purposes, we use the Trusted Desktop Grid (TDG), where the system utility is an averaged speedup in job calculation for all benevolent participants. This TDG resembles typical Organic Computing characteristics such as self-organisation, adaptive behaviour of heterogeneous entities, and openness. We show that our concept is able to successfully identify groups of systems with undesired behaviour, ranging from freeriding to colluding attacks.},
author = {Kantert, Jan and Tomforde, Sven and Scharrer, Richard and Weber, Susanne and Edenhofer, Sarah and M{\"{u}}ller-Schloer, Christian},
doi = {10.1016/j.sysarc.2017.02.003},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Kantert et al. - 2017 - Identification and classification of agent behaviour at runtime in open, trust-based organic computing systems.pdf:pdf},
issn = {13837621},
journal = {Journal of Systems Architecture},
keywords = {Identification of agent behaviour,Organic computing,Runtime clustering,Trust-based control,Volunteer-based desktop grid},
month = {apr},
pages = {68--78},
publisher = {Elsevier B.V.},
title = {{Identification and classification of agent behaviour at runtime in open, trust-based organic computing systems}},
volume = {75},
year = {2017}
}
@article{Martin,
abstract = {We overview different approaches to safety in (semi)autonomous\r\nrobotics. Particularly, we focus on how to achieve safe behavior\r\nof a robot if it is requested to perform exploration of unknown states. Presented\r\nmethods are studied from the viewpoint of reinforcement learning,\r\na partially-supervised machine learning method. To collect training data\r\nfor this algorithm, the robot is required to freely explore the state space\r\n– which can lead to possibly dangerous situations. The role of safe exploration\r\nis to provide a framework allowing exploration while preserving\r\nsafety. The examined methods range from simple algorithms to sophisticated\r\nmethods based on previous experience or state prediction. Our\r\noverview also addresses the issues of how to define safety in the real-world\r\napplications (apparently absolute safety is unachievable in the continuous\r\nand random real world). In the conclusion we also suggest several\r\nways that are worth researching more thoroughly.},
author = {Martin, Pecka and Tomas, Svoboda},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Martin, Tomas - Unknown - Lecture Notes in Computer Science Safe Exploration Techniques for Reinforcement Learning – An Overview.pdf:pdf},
keywords = {Safe exploration,policy search,reinforcement learning},
title = {{Lecture Notes in Computer Science: Safe Exploration Techniques for Reinforcement Learning – An Overview}},
url = {http://cmp.felk.cvut.cz/∼peckama2}
}
@article{Bakkes_case-based-AI,
annote = {Opponent modelling in video games:
Argues opponent modelling was effecitvely kicked off by Carmel and Markovic in 1993: guided search trees and depths. 

In MMORPGs the goal isn't to play optimally or better, the goal is for the entertainment factor, so to make things easier, or harder, louder quieter etc. etc. Same with human computer interaction: not necessarily an equilibrium solutoin, which poses a change to most work. 

Only inexpensive forms of OM can be used in games: only about 20% of the computational resources might be available at any one time, as the rest are concerned with the game engine. 

So, they use a case-based to cluster opponent models offline based on 10 extracted features, and then devise strategies against them. 

They allow for the use of a centralised server which computes models offline, and at time is simply a clustering approach. 

Shows to be effecive compbination of case-based reasoning to allocate.},
author = {Bakkes, S C J and Spronck, P H M and van den Herik, H},
journal = {Entertainment Computing},
number = {1},
pages = {27--37},
title = {{Opponent modelling for case-based adaptive game AI}},
volume = {1},
year = {2009}
}
@article{Wang2020a,
abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1},
author = {Wang, Yaqing and Yao, Quanming and Kwok, James T and Ni, Lionel M},
doi = {10.1145/3386252},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Wang et al. - 2020 - Generalizing from a Few Examples A Survey on Few-shot Learning.pdf:pdf},
issn = {15577341},
journal = {ACM Computing Surveys},
keywords = {Few-shot learning,low-shot learning,meta-learning,one-shot learning,prior knowledge,small sample learning},
number = {3},
title = {{Generalizing from a Few Examples: A Survey on Few-shot Learning}},
url = {https://doi.org/10.1145/3386252},
volume = {53},
year = {2020}
}
@inproceedings{8460078,
author = {Chen, C T and Chen, A and Huang, S},
booktitle = {2018 IEEE International Conference on Agents (ICA)},
doi = {10.1109/AGENTS.2018.8460078},
issn = {null},
keywords = {decision making;gradient methods;investment;learni},
month = {jul},
pages = {34--37},
title = {{Cloning Strategies from Trading Records using Agent-based Reinforcement Learning Algorithm}},
year = {2018}
}
@inproceedings{Mordatch2018,
abstract = {By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of nonverbal communication such as pointing and guiding when language communication is unavailable.},
archivePrefix = {arXiv},
arxivId = {1703.04908},
author = {Mordatch, Igor and Abbeel, Pieter},
booktitle = {32nd AAAI Conference on Artificial Intelligence, AAAI 2018},
eprint = {1703.04908},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Mordatch, Abbeel - 2018 - Emergence of grounded compositional language in multi-agent populations.pdf:pdf},
isbn = {9781577358008},
pages = {1495--1502},
title = {{Emergence of grounded compositional language in multi-agent populations}},
url = {www.aaai.org},
year = {2018}
}
@article{CognitiveArchitecturesandAutonomyAComparativeReview,
address = {Berlin},
author = {Th{\'{o}}risson, K and Helgasson, H},
journal = {Journal of Artificial General Intelligence},
number = {2},
pages = {1--30},
publisher = {Sciendo},
title = {{Cognitive Architectures and Autonomy: A Comparative Review}},
url = {https://content.sciendo.com/view/journals/jagi/3/2/article-p1.xml},
volume = {3},
year = {2012}
}
@article{agency_definition,
author = {Franklin, S},
doi = {10.1080/019697297126029},
journal = {{C}ybernetics and {S}ystems},
number = {6},
pages = {499--520},
publisher = {Taylor & Francis},
title = {{Autonomous agents as embodied {AI}}},
url = {https://doi.org/10.1080/019697297126029},
volume = {28},
year = {1997}
}
@article{DARPA_Intruder_detection,
author = {Geib, C W and Goldman, R P},
journal = {In Proceedings DARPA Information Survivability Conference and Exposition II. DISCEX'01 (Vol. 1, pp. 46-55). IEEE.},
pages = {46--55},
title = {{Plan recognition in intrusion detection systems}},
url = {https://ieeexplore.ieee.org/abstract/document/932191/},
volume = {1},
year = {2001}
}
@article{article,
author = {D{\"{o}}rner, D},
doi = {10.1037/a0032947},
journal = {Review of General Psychology},
pages = {297--317},
title = {{PSI: A Computational Architecture of Cognition, Motivation, and Emotion}},
volume = {17},
year = {2013}
}
@techreport{Singh_2018_gaze,
abstract = {Intention recognition is the process of using behavioural cues to infer an agent's goals or future behaviour. People use many be-havioural cues to infer others' intentions, such as deliberative actions , facial expressions, eye gaze, and gestures. In artificial intelligence , two approaches for intention recognition, among others, are gaze-based and model-based intention recognition. Approaches in the former class use gaze to determine which parts of a space a person looks at more often to infer a person's intention. Approaches in the latter use models of possible future behaviour to rate intentions as more likely if they are a better 'fit' to observed actions. In this paper, we propose a novel model of human intention recognition that combines gaze and model-based approaches for online human intention recognition. Gaze data is used to build probability distributions over a set of possible intentions, which are then used as priors in a model-based intention recognition algorithm. In human-behavioural experiments (n = 20) involving a multi-player board game, we found that adding gaze-based priors to model-based intention recognition more accurately determined intentions (p < 0.01), determined those intentions earlier (p < 0.01), and at no additional cost; all compared to a model-based-only approach.},
author = {Singh, R and Miller, T and Newn, J and Velloso, E and Sonenberg, L and Vetere, F},
booktitle = {IFAAMAS},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Singh et al. - 2018 - Combining Planning with Gaze for Online Human Intention Recognition.pdf:pdf},
keywords = {Gaze,Intention Recognition,Planning},
title = {{Combining Planning with Gaze for Online Human Intention Recognition}},
url = {https://www.researchgate.net/publication/325528012},
volume = {9},
year = {2018}
}
@misc{Hernandez-Leal2017,
abstract = {The key challenge in multiagent learning is learning a best response to the behaviour of other agents, which may be non-stationary: if the other agents adapt their strategy as well, the learning target moves. Disparate streams of research have approached nonstationarity from several angles, which make a variety of implicit assumptions that make it hard to keep an overview of the state of the art and to validate the innovation and significance of new works. This survey presents a coherent overview of work that addresses opponent-induced non-stationarity with tools from game theory, reinforcement learning and multi-armed bandits. Further, we re ect on the principle approaches how algorithms model and cope with this non-stationarity, arriving at a new framework and five categories (in increasing order of sophistication): ignore, forget, respond to target models, learn models, and theory of mind. A wide range of state-of-the-art algorithms is classified into a taxonomy, using these categories and key characteristics of the environment (e.g., observability) and adaptation behaviour of the opponents (e.g., smooth, abrupt). To clarify even further we present illustrative variations of one domain, contrasting the strengths and limitations of each category. Finally, we discuss in which environments the different approaches yield most merit, and point to promising avenues of future research.},
archivePrefix = {arXiv},
arxivId = {1707.09183},
author = {Hernandez-Leal, Pablo and Kaisers, Michael and Baarslag, Tim and {De Cote}, Enrique Munoz},
booktitle = {arXiv},
eprint = {1707.09183},
issn = {23318422},
keywords = {Game theory,Multi-armed bandits,Multiagent learning,Reinforcement learning},
month = {jul},
publisher = {arXiv},
title = {{A survey of learning in multiagent environments: Dealing with non-stationarity}},
year = {2017}
}
@inproceedings{Steels_distributed_agents,
author = {Steels, L},
booktitle = {EEE International Workshop on Intelligent Robots and Systems, Towards a New Frontier of Applications},
pages = {8--14},
title = {{Cooperation between distributed agents through self-organisation}},
year = {1990}
}
@techreport{he_om_DRL,
abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision, econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
author = {Doucet, Arnaud and Johansen, Adam M},
booktitle = {warwick.ac.uk},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Doucet, Johansen - Unknown - A Tutorial on Particle Filtering and Smoothing Fifteen years later.pdf:pdf},
keywords = {Central Limit Theorem,Filtering,Hidden Markov Models,Markov chain Monte Carlo,Particle methods,Resampling,Sequential Monte Carlo,Smoothing,State-Space models},
title = {{A Tutorial on Particle Filtering and Smoothing: Fifteen years later}},
url = {http://www.warwick.ac.uk/fac/sci/statistics/staff/academic-research/johansen/publications/dj11.pdf}
}
@inproceedings{Bernhard,
abstract = {Predicting and planning interactive behaviors in complex traffic situations presents a challenging task. Especially in scenarios involving multiple traffic participants that interact densely, autonomous vehicles still struggle to interpret situations and to eventually achieve their own mission goal. As driving tests are costly and challenging scenarios are hard to find and reproduce, simulation is widely used to develop, test, and benchmark behavior models. However, most simulations rely on datasets and simplistic behavior models for traffic participants and do not cover the full variety of real-world, interactive human behaviors. In this work, we introduce BARK, an open-source behavior benchmarking environment designed to mitigate the shortcomings stated above. In BARK, behavior models are (re-)used for planning, prediction, and simulation. A range of models is currently available, such as MonteCarlo Tree Search and Reinforcement Learning-based behavior models. We use a public dataset and sampling-based scenario generation to show the inter-exchangeability of behavior models in BARK. We evaluate how well the models used cope with interactions and how robust they are towards exchanging behavior models. Our evaluation shows that BARK provides a suitable framework for a systematic development of behavior models.},
archivePrefix = {arXiv},
arxivId = {2003.02604},
author = {Bernhard, Julian and Esterle, Klemens and Hart, Patrick and Kessler, Tobias},
booktitle = {IEEE International Conference on Intelligent Robots and Systems},
doi = {10.1109/IROS45743.2020.9341222},
eprint = {2003.02604},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Bernhard et al. - 2020 - BARK Open behavior benchmarking in multi-agent environments.pdf:pdf},
isbn = {9781728162126},
issn = {21530866},
pages = {6201--6208},
title = {{BARK: Open behavior benchmarking in multi-agent environments}},
url = {http://carla.org/},
year = {2020}
}
@article{Rahman2020,
abstract = {Ad hoc teamwork is the challenging problem of designing an autonomous agent which can adapt quickly to collaborate with previously unknown teammates. Prior work in this area has focused on closed teams in which the number of agents is fixed. In this work, we consider open teams by allowing agents of varying types to enter and leave the team without prior notification. Our solution builds on graph neural networks to learn agent models and joint action-value decompositions under varying team sizes, which can be trained with reinforcement learning using a discounted returns objective. We demonstrate empirically that our approach effectively models the impact of other agents actions on the controlled agent's returns to produce policies which can robustly adapt to dynamic team composition and is able to effectively generalize to larger teams than were seen during training.},
archivePrefix = {arXiv},
arxivId = {2006.10412},
author = {Rahman, Arrasy and Hopner, Niklas and Christianos, Filippos and Albrecht, Stefano V.},
eprint = {2006.10412},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Rahman et al. - 2020 - Open Ad Hoc Teamwork using Graph-based Policy Learning.pdf:pdf},
journal = {arXiv},
month = {jun},
publisher = {arXiv},
title = {{Open Ad Hoc Teamwork using Graph-based Policy Learning}},
url = {http://arxiv.org/abs/2006.10412},
year = {2020}
}
@inproceedings{Georgeff_BDI,
address = {Berlin, Heidelberg},
author = {Georgeff, M and Pell, B and Pollack, M and Tambe, M and Wooldridge, M},
booktitle = {Intelligent Agents V: Agents Theories, Architectures, and Languages},
editor = {M{\"{u}}ller, J P and Rao, A S and Singh, M P},
pages = {1--10},
publisher = {Springer Berlin Heidelberg},
title = {{The Belief-Desire-Intention Model of Agency}},
year = {1999}
}
@inproceedings{Foerster_opponent_learning_awareness,
author = {Foerster, J and Chen, R and Al-Shedivat, M and Whiteson, S and P., Abbeel and Mordatch, I},
booktitle = {Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, {AAMAS} 2018, Stockholm, Sweden, July 10-15, 2018},
editor = {Andr{\'{e}}, E and Koenig, S and Dastani, M and Sukthankar, G},
pages = {122--130},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems Richland, SC, {USA} / {ACM}},
title = {{Learning with Opponent-Learning Awareness}},
url = {http://dl.acm.org/citation.cfm?id=3237408},
year = {2018}
}
@inproceedings{Drifter_powerTacOnly,
address = {Richland, SC},
author = {Hernandez-Leal, P and de Cote, E and Sucar, L E},
booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
isbn = {9781450334136},
keywords = {learning,markov decision process,non-stationary strategies,opponent modelling},
pages = {1989--1990},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
series = {AAMAS '15},
title = {{Opponent Modeling against Non-Stationary Strategies: (Doctoral Consortium)}},
year = {2015}
}
@incollection{wooldridge_2009,
author = {Wooldridge, M},
booktitle = {An Introduction to MultiAgent Systems},
edition = {2},
pages = {65--79},
publisher = {John Wiley and Sons Ltd.},
title = {{Practical Reasoning agents}},
year = {2009}
}
@inproceedings{Lazaridou_Emergence_natural_language,
author = {Lazaridou, A and Peysakhovich, A and {Baroni M.}},
booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
publisher = {OpenReview.net},
title = {{Multi-Agent Cooperation and the Emergence of (Natural) Language}},
url = {https://openreview.net/forum?id=Hk8N3Sclg},
year = {2017}
}
@inproceedings{Gazi2007,
abstract = {The field of multi-agent dynamic systems is an inter-disciplinary research field that has become very popular in the recent years in parallel with the significant interest in the practical applications of such systems in various areas including robotics. In this article we give a relatively short review of this field from the system dynamics and control perspective. We first focus on mathematical modelling of multi-agent systems paying particular attention on the agent dynamics models available in the literature. Then we present a number of problems on coordination and control of multi-agent systems which have gained significant attention recently and various approaches to these problems. Relevant to these problems and approaches, we summarize some of the recent results on stability, robustness, and performance of multi-agent dynamic systems which appeared in the literature. The article is concluded with some remarks on the implementation and application side of the control designs developed for multi-agent systems. {\textcopyright} Springer-Verlag Berlin Heidelberg 2007.},
author = {Gazi, Veysel and Fidan, Bariş},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-71541-2_6},
isbn = {9783540715405},
issn = {16113349},
pages = {71--102},
publisher = {Springer Verlag},
title = {{Coordination and control of multi-agent dynamic systems: Models and approaches}},
volume = {4433 LNCS},
year = {2007}
}
@article{Hernandez-Leal2017a,
abstract = {The success or failure of any learning algorithm is partially due to the exploration strategy it exerts. However, most exploration strategies assume that the environment is stationary and non-strategic. In this work we shed light on how to design exploration strategies in non-stationary and adversarial environments. Our proposed adversarial drift exploration (DE) is able to efficiently explore the state space while keeping track of regions of the environment that have changed. This proposed exploration is general enough to be applied in single agent non-stationary environments as well as in multiagent settings where the opponent changes its strategy in time. We use a two agent strategic interaction setting to test this new type of exploration, where the opponent switches between different behavioral patterns to emulate a non-deterministic, stochastic and adversarial environment. The agent's objective is to learn a model of the opponent's strategy to act optimally. Our contribution is twofold. First, we present DE as a strategy for switch detection. Second, we propose a new algorithm called R-max# for learning and planning against non-stationary opponent. To handle such opponents, R-max# reasons and acts in terms of two objectives: (1) to maximize utilities in the short term while learning and (2) eventually explore opponent behavioral changes. We provide theoretical results showing that R-max# is guaranteed to detect the opponent's switch and learn a new model in terms of finite sample complexity. R-max# makes efficient use of exploration experiences, which results in rapid adaptation and efficient DE, to deal with the non-stationary nature of the opponent. We show experimentally how using DE outperforms the state of the art algorithms that were explicitly designed for modeling opponents (in terms average rewards) in two complimentary domains.},
author = {Hernandez-Leal, Pablo and Zhan, Yusen and Taylor, Matthew E and Sucar, L. Enrique and {Munoz de Cote}, Enrique},
doi = {10.1007/s10458-016-9347-3},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Hernandez-Leal et al. - 2017 - An exploration strategy for non-stationary opponents.pdf:pdf},
issn = {15737454},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Exploration,Learning,Non-stationary environments,Repeated games,Switching strategies},
number = {5},
pages = {971--1002},
title = {{An exploration strategy for non-stationary opponents}},
volume = {31},
year = {2017}
}
@inproceedings{10.5555/1893313.1893352,
address = {NLD},
author = {Samsonovich, A V},
booktitle = {Proceedings of the 2010 Conference on Biologically Inspired Cognitive Architectures 2010: Proceedings of the First Annual Meeting of the BICA Society},
isbn = {9781607506607},
pages = {195--244},
publisher = {IOS Press},
title = {{Toward a Unified Catalog of Implemented Cognitive Architectures}},
year = {2010}
}
@misc{Rubin2011,
abstract = {The game of poker has been identified as a beneficial domain for current AI research because of the properties it possesses such as the need to deal with hidden information and stochasticity. The identification of poker as a useful research domain has inevitably resulted in increased attention from academic researchers who have pursued many separate avenues of research in the area of computer poker. The poker domain has often featured in previous review papers that focus on games in general, however a comprehensive review paper with a specific focus on computer poker has so far been lacking in the literature. In this paper, we present a review of recent algorithms and approaches in the area of computer poker, along with a survey of the autonomous poker agents that have resulted from this research. We begin with the first serious attempts to create strong computerised poker players by constructing knowledge-based and simulation-based systems. This is followed by the use of computational game theory to construct robust poker agents and the advances that have been made in this area. Approaches to constructing exploitive agents are reviewed and the challenging problems of creating accurate and dynamic opponent models are addressed. Finally, we conclude with a selection of alternative approaches that have received attention in previously published material and the interesting problems that they pose. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Rubin, Jonathan and Watson, Ian},
booktitle = {Artificial Intelligence},
doi = {10.1016/j.artint.2010.12.005},
issn = {00043702},
keywords = {Computational game theory,Computer poker,Imperfect information games,Nash equilibrium,Opponent modelling},
month = {apr},
number = {5-6},
pages = {958--987},
publisher = {Elsevier B.V.},
title = {{Computer poker: A review}},
volume = {175},
year = {2011}
}
@techreport{Raileanu2018,
abstract = {We consider the multi-agent reinforcement learning setting with imperfect information. The reward function depends on the hidden goals of both agents, so the agents must infer the other players' goals from their observed behavior in order to maximize their returns. We propose a new approach for learning in these domains: Self Other-Modeling (SOM), in which an agent uses its own policy to predict the other agent's actions and update its belief of their hidden goal in an on-line manner. We evaluate this approach on three different tasks and show that the agents are able to learn better policies using their estimate of the other players' goals, in both cooperative and competitive settings.},
author = {Raileanu, Roberta and Denton, Emily and Szlam, Arthur and Fergus, Rob},
booktitle = {proceedings.mlr.press},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Raileanu et al. - 2018 - Modeling Others using Oneself in Multi-Agent Reinforcement Learning.pdf:pdf},
title = {{Modeling Others using Oneself in Multi-Agent Reinforcement Learning}},
url = {http://proceedings.mlr.press/v80/raileanu18a.html},
year = {2018}
}
@book{Oliehoek_DecPOMDPs,
abstract = {This book provides an introduction to the basic ideas and tools used in mathematical analysis. It is a hybrid cross between an advanced calculus and a more advanced analysis text and covers topics in both real and complex variables. Considerable space is given to developing Riemann integration theory in higher dimensions, including a rigorous treatment of Fubini's theorem, polar coordinates and the divergence theorem. These are used in the final chapter to derive Cauchy's formula, which is then applied to prove some of the basic properties of analytic functions. Among the unusual features of this book is the treatment of analytic function theory as an application of ideas and results in real analysis. For instance, Cauchy's integral formula for analytic functions is derived as an application of the divergence theorem. The last section of each chapter is devoted to exercises that should be viewed as an integral part of the text. A Concise Introduction to Analysis should appeal to upper level undergraduate mathematics students, graduate students in fields where mathematics is used, as well as to those wishing to supplement their mathematical education on their own. Wherever possible, an attempt has been made to give interesting examples that demonstrate how the ideas are used and why it is important to have a rigorous grasp of them.},
author = {Oliehoek, F A and Amato, C},
booktitle = {Springer Briefs in Intelligent Systems},
isbn = {978-3-319-28927-4},
pages = {1--116},
publisher = {Springer International Publishing},
series = {SpringerBriefs in Intelligent Systems},
title = {{A Concise Introduction to Decentralized POMDPs}},
url = {http://link.springer.com/10.1007/978-3-319-28929-8},
year = {2016}
}
@techreport{winston2018genesis,
author = {Winston, P H and Holmes, D},
title = {{The Genesis Enterprise: Taking artificial intelligence to another level via a computational account of human story understanding}},
year = {2018}
}
@inproceedings{Powers_conditional_action_frequency,
author = {Powers, R and Shoham, Y},
booktitle = {IJCAI-05, Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence, Edinburgh, Scotland, UK, July 30 - August 5, 2005},
pages = {817--822},
title = {{Learning against opponents with bounded memory}},
url = {http://ijcai.org/Proceedings/05/Papers/1349.pdf},
year = {2005}
}
@inproceedings{Leibo2017b,
abstract = {Matrix games like Prisoner's Dilemma have guided research on social dilemmas for decades. However, they necessarily treat the choice to cooperate or defect as an atomic action. In real-world social dilemmas these choices are temporally extended. Cooperativeness is a property that applies to policies, not elementary actions. We introduce sequential social dilemmas that share the mixed incentive structure of matrix game social dilemmas but also require agents to learn policies that implement their strategic intentions. We analyze the dynamics of policies learned by multiple self-interested independent learning agents, each using its own deep Q-network, on two Markov games we introduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We char-Acterize how learned behavior in each domain changes as a function of environmental factors including resource abundance. Our experiments show how conflict can emerge from competition over shared resources and shed light on how the sequential nature of real world social dilemmas affects cooperation.},
archivePrefix = {arXiv},
arxivId = {1702.03037},
author = {Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
booktitle = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
eprint = {1702.03037},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Leibo et al. - 2017 - Multi-agent reinforcement learning in sequential social dilemmas(2).pdf:pdf},
isbn = {9781510855076},
issn = {15582914},
keywords = {Agent-based social simulation,Cooperation,Markov games,Non-cooperative games,Social dilemmas},
pages = {464--473},
title = {{Multi-agent reinforcement learning in sequential social dilemmas}},
url = {www.ifaamas.org},
volume = {1},
year = {2017}
}
@inproceedings{Nikkhoo_dynamic_field_theory,
author = {Nikkhoo, M and Menhaj, M B},
booktitle = {2016 4th International Conference on Control, Instrumentation, and Automation {(ICCIA)}},
pages = {435--438},
title = {{Extendable behavioral cognitive architecture based on dynamic field theory}},
year = {2016}
}
@inproceedings{IRMA,
abstract = {Academic and industrial system designers who consider using agent technology to solve an application problem are faced with a wide variety of agent paradigms: There are deliberative agents, reactive agents, interacting agents, hybrid agents, layered agents, believable agents, mobile agents, software agents, softbots --- the list could well be prolonged. Also, within each paradigm, the user can select between different architectures and systems, making the actual choice a complex and difficult endeavor.},
address = {Berlin, Heidelberg},
author = {M{\"{u}}ller, J P},
booktitle = {Intelligent Agents V: Agents Theories, Architectures, and Languages},
editor = {M{\"{u}}ller, J P and Rao, A S and Singh, M P},
isbn = {978-3-540-49057-9},
pages = {211--225},
publisher = {Springer Berlin Heidelberg},
title = {{The Right Agent (Architecture) to Do the Right Thing}},
year = {1999}
}
@article{Albrecht__explioting_causality,
abstract = {Dynamic Bayesian networks (DBNs) are a general model for stochastic processes with partially observed states. Belief filtering in DBNs is the task of inferring the belief state (i.e. the probability distribution over process states) based on incomplete and noisy observations. This can be a hard problem in complex processes with large state spaces. In this article, we explore the idea of accelerating the filtering task by automatically exploiting causality in the process. We consider a specific type of causal relation, called passivity, which pertains to how state variables cause changes in other variables. We present the Passivity-based Selective Belief Filtering (PSBF) method, which maintains a factored belief representation and exploits passivity to perform selective updates over the belief factors. PSBF produces exact belief states under certain assumptions and approximate belief states otherwise, where the approximation error is bounded by the degree of uncertainty in the process. We show empirically, in synthetic processes with varying sizes and degrees of passivity, that PSBF is faster than several alternative methods while achieving competitive accuracy. Furthermore, we demonstrate how passivity occurs naturally in a complex system such as a multi-robot warehouse, and how PSBF can exploit this to accelerate the filtering task.},
annote = {This article is interested in the application of dynamic bayesian networks as representations of actions in partially observed decision processes, such as POMDPs. 

These processes often exhibit high degrees of casual structure: a change in one part of the process might change another. 

As this is a partially observable process, there is a distribution of beliefs over each state. The application of belief filtering is using (noisy) observations to update beliefs over the true state of the environment. 

PSBF --- their algo --- exploits causal structures to know when, and when not to, perform costly updates. They define a notion called passivity: x is passive if a change in y causes a change in x. Consider the robot arm example in page 1: the finger joint moves regardless of the arm joint, hence moving a finfer doesnt neccesitate an update on teh arm, yet this is what current state-of-the-art filtering processes do.

By represeting things as a dynamic bayesian network, one can infer causal strucure, and use this structure to perform more efficient particle filtering/belief updates. 

While the authors note that in the robot-warehouse (simulated) their results were not significantly (statistically better) than traditional forms of filtering, it did highlight the high-degree of causal relation in state-variables in seeminly complex systems, and previous work (in a simpler environment) showed increasing performance as passivity levels scaled. 

Anyway -- the general message is elements of causality can allow for targeted updates.},
archivePrefix = {arXiv},
arxivId = {1401.7941},
author = {Albrecht, S V and Ramamoorthy, S},
doi = {10.1613/jair.5044},
eprint = {1401.7941},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Albrecht, Ramamoorthy - 2016 - Exploiting causality for selective belief filtering in dynamic Bayesian networks.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {apr},
pages = {1135--1178},
publisher = {AI Access Foundation},
title = {{Exploiting causality for selective belief filtering in dynamic Bayesian networks}},
volume = {55},
year = {2016}
}
@article{krarup2019_planning_contrastive_explanations,
author = {Krarup, B and Cashmore, M and Magazzeni, D and Miller, T},
publisher = {AAAI Press},
title = {{Model-based contrastive explanations for explainable planning}},
year = {2019}
}
@techreport{Jiang,
abstract = {Deep reinforcement learning (DRL) has achieved significant breakthroughs in various tasks. However , most DRL algorithms suffer a problem of generalising the learned policy, which makes the policy performance largely affected even by minor modifications of the training environment. Except that, the use of deep neural networks makes the learned policies hard to be inter-pretable. To address these two challenges, we propose a novel algorithm named Neural Logic Reinforcement Learning (NLRL) to represent the policies in reinforcement learning by first-order logic. NLRL is based on policy gradient methods and differentiable inductive logic programming that have demonstrated significant advantages in terms of interpretability and generalisability in supervised tasks. Extensive experiments conducted on cliff-walking and blocks manipulation tasks demonstrate that NLRL can induce inter-pretable policies achieving near-optimal performance while showing good generalisability to environments of different initial states and problem sizes.},
archivePrefix = {arXiv},
arxivId = {1904.10729v2},
author = {Jiang, Zhengyao and Luo, Shan},
eprint = {1904.10729v2},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Jiang, Luo - Unknown - Neural Logic Reinforcement Learning.pdf:pdf},
title = {{Neural Logic Reinforcement Learning}}
}
@article{dMars_dinverno_luck,
author = {D'Inverno, M and Luck, M and Georgeff, M and Kinny, D and Wooldridge, M},
journal = {Autonmous agents and Multi-agent Systems (AAMAS'04)},
pages = {5--53},
title = {{The d{MARS} architecture: A specification of the distributed multi-agent reasoning system}},
volume = {9},
year = {2004}
}
@inproceedings{Carmel_Markovic_learningOpponentStrategy,
author = {Carmel, D and Markovic, S},
booktitle = {Proceedings of the AAAI fall symposium series},
pages = {140--147},
title = {{Learning models of opponent's strategy in game playing}},
year = {1993}
}
@article{Barrett2017,
abstract = {Robots are being deployed in an increasing variety of environments for longer periods of time. As the number of robots grows, they will increasingly need to interact with other robots. Additionally, the number of companies and research laboratories producing these robots is increasing, leading to the situation where these robots may not share a common communication or coordination protocol. While standards for coordination and communication may be created, we expect that robots will need to additionally reason intelligently about their teammates with limited information. This problem motivates the area of ad hoc teamwork in which an agent may potentially cooperate with a variety of teammates in order to achieve a shared goal. This article focuses on a limited version of the ad hoc teamwork problem in which an agent knows the environmental dynamics and has had past experiences with other teammates, though these experiences may not be representative of the current teammates. To tackle this problem, this article introduces a new general-purpose algorithm, PLASTIC, that reuses knowledge learned from previous teammates or provided by experts to quickly adapt to new teammates. This algorithm is instantiated in two forms: 1) PLASTIC-Model – which builds models of previous teammates' behaviors and plans behaviors online using these models and 2) PLASTIC-Policy – which learns policies for cooperating with previous teammates and selects among these policies online. We evaluate PLASTIC on two benchmark tasks: the pursuit domain and robot soccer in the RoboCup 2D simulation domain. Recognizing that a key requirement of ad hoc teamwork is adaptability to previously unseen agents, the tests use more than 40 previously unknown teams on the first task and 7 previously unknown teams on the second. While PLASTIC assumes that there is some degree of similarity between the current and past teammates' behaviors, no steps are taken in the experimental setup to make sure this assumption holds. The teammates were created by a variety of independent developers and were not designed to share any similarities. Nonetheless, the results show that PLASTIC was able to identify and exploit similarities between its current and past teammates' behaviors, allowing it to quickly adapt to new teammates.},
author = {Barrett, Samuel and Rosenfeld, Avi and Kraus, Sarit and Stone, Peter},
doi = {10.1016/j.artint.2016.10.005},
issn = {00043702},
journal = {Artificial Intelligence},
keywords = {Ad hoc teamwork,Multiagent cooperation,Multiagent systems,Pursuit domain,Reinforcement learning,RoboCup soccer},
month = {jan},
pages = {132--171},
publisher = {Elsevier B.V.},
title = {{Making friends on the fly: Cooperating with new teammates}},
volume = {242},
year = {2017}
}
@article{Winfield_simulation_based_internal_2018,
author = {Blum, C and Winfield, A F T and Hafner, V V},
journal = {Frontiers in Robotics and {AI}},
title = {{Simulation-Based Internal Models for Safer Robots}},
year = {2018}
}
@inproceedings{Silver2010,
abstract = {This paper introduces a Monte-Carlo algorithm for online planning in large POMDPs. The algorithm combines a Monte-Carlo update of the agent's belief state with a Monte-Carlo tree search from the current belief state. The new algorithm, POMCP, has two important properties. First, Monte-Carlo sampling is used to break the curse of dimensionality both during belief state updates and during planning. Second, only a black box simulator of the POMDP is required, rather than explicit probability distributions. These properties enable POMCP to plan effectively in significantly larger POMDPs than has previously been possible. We demonstrate its effectiveness in three large POMDPs. We scale up a well-known benchmark problem, rocksample, by several orders of magnitude. We also introduce two challenging new POMDPs: 10 × 10 battleship and partially observable PacMan, with approximately 10 18 and 10 56 states respectively. Our Monte-Carlo planning algorithm achieved a high level of performance with no prior knowledge, and was also able to exploit simple domain knowledge to achieve better results with less search. POMCP is the first general purpose planner to achieve high performance in such large and unfactored POMDPs.},
author = {Silver, David and Veness, Joel},
booktitle = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010, NIPS 2010},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Silver, Veness - 2010 - Monte-Carlo planning in large POMDPs.pdf:pdf},
isbn = {9781617823800},
issn = {1049-5258},
title = {{Monte-Carlo planning in large POMDPs}},
year = {2010}
}
@article{Vanderelst_Winfield_architecture,
author = {Vanderelst, D and Winfield, A},
journal = {Cognitive Systems Research},
pages = {56--66},
publisher = {Elsevier},
title = {{An architecture for ethical robots inspired by the simulation theory of cognition}},
volume = {48},
year = {2018}
}
@article{AlphaGo,
annote = {Combine policy function network with a value-function network. 

So, typically the basic strategy is to truncate the search space at a certain depth and replace it with an approximate value function which represents the likely outcome of the game from a certain postion (i.e. the end result, based on the state of the board). 

Typical option two, we can map out the likely sequence of actions using a policy, and using a monte-carlo roll-out to evaluate the values of likely states, in order to better structure the policy. 

They use neural nets to approxmate the value functions and policy functions, in order to limit the depth and breadth of a search tree. So, basically use the normal method of MCTS, but limit the search depth and breadth using function-approximators. 

They start by training the policy network ons on expert human-play, then use a combination of deepRL in self play to train the value-function and other policy networks. Interestingly, the SL policy function was better than then RL, however the value function performed better (particularly against humans). 

Alpha-Go then combines all of them, into a MCTS-aided with complex function approximators. 

Interesting to note, far smaller state-space search than the method used in Deep-blue for Kasparov, also the value-function was hand-encoded. Basically this one was smarter but smaller, suggesting that human-players think a little more like that....},
author = {Silver, D and Huang, A and Maddison, C and Guez, A and Sifre, L and van den Driessche, G and Schrittwieser, J and Antonoglou, I and Panneershelvam, V and Lanctot, M and Dieleman, S and Grewe, D and Nham, J and Kalchbrenner, N and Sutskever, I and Lillicrap, T and Leach, M and Kavukcuoglu, K and Graepel, T and Hassabis, D},
journal = {Nature},
keywords = {Read},
mendeley-tags = {Read},
number = {7587},
pages = {484--489},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {https://doi.org/10.1038/nature16961},
volume = {529},
year = {2016}
}
@techreport{Carroll,
abstract = {While we would like agents that can coordinate with humans, current algorithms such as self-play and population-based training create agents that can coordinate with themselves. Agents that assume their partner to be optimal or similar to them can converge to coordination protocols that fail to understand and be understood by humans. To demonstrate this, we introduce a simple environment that requires challenging coordination, based on the popular game Overcooked, and learn a simple model that mimics human play. We evaluate the performance of agents trained via self-play and population-based training. These agents perform very well when paired with themselves, but when paired with our human model, they are significantly worse than agents designed to play with the human model. An experiment with a planning algorithm yields the same conclusion, though only when the human-aware planner is given the exact human model that it is playing with. A user study with real humans shows this pattern as well, though less strongly. Qualitatively, we find that the gains come from having the agent adapt to the human's gameplay. Given this result, we suggest several approaches for designing agents that learn about humans in order to better coordinate with them. Code is available at https://github.com/HumanCompatibleAI/overcooked_ai.},
author = {Carroll, M and Shah, R and Ho, M and Griffiths, T L and Seshia, S A and Abbeel, P and Dragan, A},
booktitle = {papers.nips.cc},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Carroll et al. - Unknown - On the Utility of Learning about Humans for Human-AI Coordination.pdf:pdf},
title = {{On the Utility of Learning about Humans for Human-AI Coordination}},
url = {https://github.com/HumanCompatibleAI/overcooked_ai.}
}
@inproceedings{Brown2020,
abstract = {The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of a successes in single-agent settings and perfect-information games, best exemplified by the success of AlphaZero. However, algorithms of this form have been unable to cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search for imperfect-information games. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results show ReBeL leads to low exploitability in benchmark imperfect-information games and achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI. We also prove that ReBeL converges to a Nash equilibrium in two-player zero-sum games in tabular settings.},
archivePrefix = {arXiv},
arxivId = {2007.13544},
author = {Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
booktitle = {34th Conference on Neural Information Processing Systems (NeurIPS)},
eprint = {2007.13544},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Brown et al. - 2020 - Combining deep reinforcement learning and search for imperfect-information games.pdf:pdf},
issn = {23318422},
title = {{Combining deep reinforcement learning and search for imperfect-information games}},
url = {https://github.com/facebookresearch/rebel},
year = {2020}
}
@inproceedings{Iglesias2009,
abstract = {Opponent modeling is a skill in multi-agent systems (MAS) which attempts to create a model of the behavior of the opponent. This model can be used to predict the future actions of the opponent and generate appropriate strategies to play against it. Several researches present different methods to create an opponent model in the RoboCup environment. However, how these models can impact the performance of teams is an essential aspect. This paper introduces a novel approach to use efficiently opponent models in order to improve our own team behavior. The basis of this approach is the research done by CAOS Coach Team for modeling and recognizing behaviors evaluated in the RoboCup Coach Competition 2006. For using these models, it is necessary a special agent (coach) which can model the observed opponent team (based on the previous research) and communicate a counter-strategy to the coached players (using the approach proposed in this paper). The evaluation of this approach is a hard problem, but we have conducted several experiments that can help us to know if we are going in a promising direction. {\textcopyright} 2009 Springer Berlin Heidelberg.},
author = {Iglesias, J and Fern{\'{a}}ndez, J and Villena, I and Ledezma, A and Sanchis, A},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-642-04394-9_59},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Iglesias et al. - 2009 - The winning advantage Using opponent models in robot soccer.pdf:pdf},
isbn = {3642043933},
issn = {03029743},
pages = {485--493},
title = {{The winning advantage: Using opponent models in robot soccer}},
volume = {5788 LNCS},
year = {2009}
}
@article{Hayashi_et_al2020,
annote = {Planners query world models in order to find good solutions to problems. 
In order to do this effectely, they need to overcome the lack of sufficiently effective or accurate prediction models for real-world objects, environments and other agents. 
key paper. 
Usually prediction models mismatch the situation. 

The cause for a lot of this mismatch is partially observable environments. 

Environments are typcally represented as POMDPs. Belief space planners have shown to be able to deal with partial observability and uncertainty of POMDPs, but they rely on apropriate models and scale very poorly in computation time wiht the complexiy of these models (and the resulting state/action and observation spaces). 

The solution to this is sampling based planners, which can partly allieviate this as they can handle much larger spaces, byt they still struggle with domain complexity that is encountered in real-world environments. 

Their approach is to update the sampling process to generate better and more meaningful samples for such planners. 

They aim to exploit prior-beliefs and current observations for online model adaptation, and so improve performance. 

They use a neural network-based particle reinvigoration process that can leverage past experiences encoded in the network, and a consistency-checking process (which makes sure the generated particles from the NN are consistent with all previous observations, and not just the current one). 

They use a physics engine as a simulator to obtain transition and observation functions. 

the Planning part of the POMCP is untouched. The belief over the true state of teh environment is managed by a particle filter. Each particle represents a hypothesis of the current situation, and contains two types of uncertain information: the state (positions of robots, objects etc) 2) a model (static aspects like agent types or internal parameters and capabilites. 
The state and model are then used to configure a simulator which in turn serves as a black-box transition and observation function. 

As initial models are assumed to have a mismatch to the situation, they train a NN (LSTM and MIxture Density netowrk) on prior experiences to suggest new hypothesis... i.e. model parameterisations, based on real observations. These are then added to new particles to be added to the current belief after they are checked for consistency. 
So, good models are given increasing weight, whereas poor models are increasly given less weight in action selection for the planner. 

As they use a mixture density network , the model outputs acts as an approximation of bayesian updating over model parameters (i.e. the updated posterior are the model outputs).},
author = {Hayashi, A and Ruiken, D and Hasegawa, T and Goerick, C},
doi = {10.1016/j.artint.2019.103228},
journal = {Artificial Intelligence},
pages = {103228},
title = {{Reasoning about uncertain parameters and agent behaviors through encoded experiences and belief planning}},
url = {https://doi.org/10.1016/j.artint.2019.103228},
volume = {280},
year = {2020}
}
@article{DInverno2004,
abstract = {The Procedural Reasoning System (PRS) is the best established agent architecture currently available. It has been deployed in many major industrial applications, ranging from fault diagnosis on the space shuttle to air traffic management and business process control. The theory of PRS-like systems has also been widely studied: within the intelligent agents research community, the belief-desire-intention (BDI) model of practical reasoning that underpins PRS is arguably the dominant force in the theoretical foundations of rational agency. Despite the interest in PRS and BDI agents, no complete attempt has yet been made to precisely specify the behaviour of real PRS systems. This has led to the development of a range of systems that claim to conform to the PRS model, but which differ from it in many important respects. Our aim in this paper is to rectify this omission. We provide an abstract formal model of an idealised dMARS system (the most recent implementation of the PRS architecture), which precisely defines the key data structures present within the architecture and the operations that manipulate these structures. We focus in particular on dMARS plans, since these are the key tool for programming dMARS agents. The specification we present will enable other implementations of PRS to be easily developed, and will serve as a benchmark against which future architectural enhancements can be evaluated.},
author = {D'Inverno, M and Luck, M and Georgeff, M and Kinny, D and Wooldridge, M},
doi = {10.1023/B:AGNT.0000019688.11109.19},
issn = {13872532},
journal = {Autonomous Agents and Multi-Agent Systems},
keywords = {Agent architectures,BDI,Formal specification,Procedural reasoning system},
month = {nov},
number = {1-2},
pages = {5--53},
title = {{The dMARS architecture: A specification of the distributed multi-agent reasoning system}},
volume = {9},
year = {2004}
}
@article{Bellman_optimal,
abstract = {In this paper we consider the problem of finding a near-optimal policy in a continuous space, discounted Markovian Decision Problem (MDP) by employing value-function-based methods when only a single trajectory of a fixed policy is available as the input. We study a policy-iteration algorithm where the iterates are obtained via empirical risk minimization with a risk function that penalizes high magnitudes of the Bellman-residual. Our main result is a finite-sample, high-probability bound on the performance of the computed policy that depends on the mixing rate of the trajectory, the capacity of the function set as measured by a novel capacity concept (the VC-crossing dimension), the approximation power of the function set and the controllability properties of the MDP. Moreover, we prove that when a linear parameterization is used the new algorithm is equivalent to Least-Squares Policy Iteration. To the best of our knowledge this is the first theoretical result for off-policy control learning over continuous state-spaces using a single trajectory.},
author = {Antos, A and Szepesv{\'{a}}ri, C and Munos, R},
doi = {10.1007/s10994-007-5038-2},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Antos, Szepesv{\'{a}}ri, Munos - 2008 - Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a.pdf:pdf},
journal = {Mach Learn},
keywords = {Bellman-residual minimization {\textperiodcentered},Finite-sample bounds,Least-squares regression {\textperiodcentered},Least-squares temporal difference learning {\textperiodcentered},Nonparametric regression {\textperiodcentered},Off-policy learning {\textperiodcentered},Policy iteration {\textperiodcentered},Reinforcement learning {\textperiodcentered}},
pages = {13--17},
title = {{Learning near-optimal policies with Bellman-residual minimization based fitted policy iteration and a single sample path}},
url = {https://link.springer.com/content/pdf/10.1007/s10994-007-5038-2.pdf},
volume = {71},
year = {2008}
}
@incollection{bordini_hubner_wooldridge_2007,
author = {Bordini, R and Hubner, J and Wooldridge, M},
booktitle = {Programming multi-agent systems in AgentSpeak using JasonBordini, R., Hubner, J., & Wooldridge, M. (2007). The BDI agent model. In Programming multi-agent systems in AgentSpeak using Jason (pp. 15–29). John Wiley and Sons Ltd.},
pages = {15--29},
publisher = {John Wiley and Sons Ltd.},
title = {{The BDI agent model}},
year = {2007}
}
@inproceedings{He2016a,
address = {New York, New York, USA},
author = {He, H and Boyd-Graber, J and Kwok, K and {Daum{\'{e}} III}, H},
booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
editor = {Balcan, Maria Florina and Weinberger, Kilian Q},
pages = {1804--1813},
publisher = {PMLR},
series = {Proceedings of Machine Learning Research},
title = {{Opponent Modeling in Deep Reinforcement Learning}},
volume = {48},
year = {2016}
}
@techreport{Kuhlmann2006,
abstract = {In a team-based multiagent system, the ability to construct a model of an opponent team's joint behavior can be useful for determining an agent's expected distribution over future world states, and thus can inform its planning of future actions. This paper presents an approach to team opponent modeling in the context of the RoboCup simulation coach competition. Specifically , it introduces an autonomous coach agent capable of analyzing past games of the current opponent, advising its own team how to play against this opponent, and identifying patterns or weaknesses on the part of the opponent. Our approach is fully implemented and tested within the RoboCup soccer server, and was the champion of the RoboCup 2005 simulation coach competition .},
annote = {Coach agent analyses past-play and builds an opponent model which identifies statistical patterns in player over time in order to better predict patterns of play. 

This is then communicated to agents. 

Offline phase builds models, 
Online phases involves pattern matching. 

References Kuhlmman, Riley and Veloso..},
author = {Kuhlmann, G and Knox, W and Stone, P},
file = {:Users/charleshiggins/Library/Application Support/Mendeley Desktop/Downloaded/Kuhlmann, Knox, Stone - 2006 - Know Thine Enemy A Champion RoboCup Coach Agent.pdf:pdf},
pages = {1463--68},
title = {{Know Thine Enemy: A Champion RoboCup Coach Agent}},
url = {www.aaai.org},
year = {2006}
}
@inproceedings{Gazanfried_Game_theory_based_OM,
author = {Ganzfried, S},
booktitle = {10th International Conference on Autonomous Agents and Multiagent Systems {(AAMAS} 2011), Taipei, Taiwan, May 20--6, 2011, Volume 1-3},
editor = {Sonenberg, L and P., Stone and K., Tumer and Yolum, P},
pages = {533--540},
publisher = {IFAAMAS},
title = {{Game theory-based opponent modeling in large imperfect-information games}},
year = {2011}
}
