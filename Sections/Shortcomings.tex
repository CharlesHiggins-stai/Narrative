\section{Shortcomings of related/current work}
\subsection{Shortcomings of RL + Search}
The majority of work in the RL + Search space has focused on 2-player zero-sum games. 
This allows for (implicit) opponent modelling via the use of a policy function to guide search: players' intentions are symmetric --- the reward function for a player's opponent is the inverse of that of the player, and as such self-play is a feasible and optimal method of training. 
However, when one moves to a general-sum game (with more than a single opponent), the intentions and characteristics of an opponent are harder to discern, and are likely to vary considerably. Therefore, more explicit and granular representations of a varying/heterogenous opponent types are needed in order to extend this architecture into a general sum, n-player environment.
\newline \newline
RL + Search architectures have focused on well-defined games with perfect observations, and where observations are imperfect all agent actions are public: in effect, all unknowns are known-unknowns, rather than unknown-unknowns. In more fluid games, the adaptation of a partially observable environment cannot always be quite so well translated into public-belief states, and as such, methods like type-based reasoning might still be required despite their relative simplicity and requirement for prior-knowledge.
\subsection{Shortcomings of sample-based planning}
While belief-based planners reason well about agent types, they have failed to adequately address swift action-selection despite having the capacity (and success) of computing a considerable amount of information about opponents. 
In Hayashi et al. \cite{Hayashi_et_al2020}, an opponent model is used solely in the roll-out (black-box simulator/transition simulation) as a predictive model, and not to guide play actively --- action-selection (i.e. tree traversal), and rollout direction is typically guided by a generic upper-confidence tree algorithm.
 The inclusion of a policy network to guide the search tree in an partially observable environment has yet to be truly completed, despite type-based reasoning taking place and being included within the form of parameters for a particle reinvigoration engine. In colloquial terms, information is being computed but not as efficiently and effectively used as it could be.
\newline \newline
Finally, there seems a distinct lack of work regarding actively seeking out information in action-based opponent modelling (N.B. outside of dialogue games). This is somewhat surprising as this area has been identified in several surveys as an open problem, and one worthy of consideration \cite{ALBRECHT201866,Hernandez-Leal_MAL_Survey}. The inclusion of an information theoretic reward (by way of a value function) in an opponent modelling context would be a novel contribution within itself, however it too would be a simple and strong addition to an RL + Search based opponent modelling architecture. 
\newline \newline
In brief: RL+Search architectures have yet to be applied and tested in a multi-agent domain where they must reason about uncertain parameters of opponenet. Belief-based planners have proved to reason well about opponent parameters, however have yet to best leverage learning to shape action-selection despite having shown promise in developing techniques to compute this information.