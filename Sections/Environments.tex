\subsection{Ad-hoc coordination: a challenge}
A large proportion of opponent modelling research has been directed at adversarial games. 
This allows a number of assumptions to be made about opposing agents (hence the name `opponent' modelling). The major assumption made is that of a purely adversarial opponent. From this assumption, one can assume that the reward function of an opposing agent is the inverse of their own. Given the assumption of an opponents reward function (or at a higher level, their intentions), one can employ a number of techniques to optimise actions --- for instance, pre-computing equilibria via iterative methods (e.g. fictitous play), self-play (training against oneself) and minimax pruning. These techniques have been used well previously in zero-sum games like Poker, Go and Chess. When one extends to environments which require cooperation as well as coompetetive objectives, these techniques require modification to be applicable. 
\newline \newline
The major effort behind this line of work is to extend opponent modelling techniques to more general problems, or more specifically, open environments. A growing field of research investigates ad-hoc coordination games. These are typically games which are sufficiently simple to be computed by an agent with limited computational power, but incorporate sufficient complexity as to be challenging. Rather more importantly, these games involve self-interested agents (i.e. no common world-model or shared objectives) and allow for competetive agents, however often require cooperation to acheive optimal results. They often involve more than a single other agent, and those other agents often can be deliberatly adversarial or cooperative in nature.
\newline \newline 
This environment shares a number of aspects with open domains: a number of heterogenous agents, exhibiting a variety of intents and abilities in a partially observable (uncertain) environment. Hence, this serves as a strong test-bed for an opponent modelling system designed for an open system and provides a number of state of the art systems to augment and benchmark against. 
\subsubsection{The level-based foraging domain}
A plausible environment (among a number of others) are variations of the Level-based foraging domain (LBFD) (used by \cite{Albrecht2019,Barrett2015,Hayashi_et_al2020,Papoudakis2020}\footnote{This is not an exhaustive list.}). The LBFD is represented by a two dimensional gridworld with a number of self-interested agents.
The gridworld is populated by agents and objects. Each agent and object has a \textit{level}. The game finishes after an arbitrary number of timesteps, or when all objects have been foraged. Agents have 6 discrete actions in each timestep :(north, south, east, west, stay still, load block). All agents more asynchronously, and select a single action per timestep. Agents receive a reward for foraging an object equal to the object's level. Agents cannot forage any object with a higher level than their own. In the case where an object's level is higher than an agent, agents can cooperate in order to forage the object together, providing that the sum of their levels' is greater than that of the object.  
\subsection{Baselines in ad-hoc coordination}
\begin{enumerate}
    \item Plastic Policy \cite{Barrett2015} \newline \newline
Barrett et al. make a key contribution: they learn an optimal policy for acting with teammates in a simulated robo-soccer league via fitted q learning. They argue that in a complex domain it is necessary to move from a model-based approach to a policy-based approach due to prohibitively large state-spaces. At runtime they select which policy to use by a nearest neighbour classification based on action histories. This contains no feature of learning at runtime and assumes that the types are sufficiently expressive, and that the policies are optimal. Hence, when exposed to unknown agents, this \textit{should}, in theory, perform worse than methods which have internal parameters to update. 
    \item Albrecht and Stone: reasoning about uncertain agent parameters\cite{Albrecht_stone_2019}.  \newline \newline
Barrett and Stone addressed the problem of static types by including parameters within agent types, and perform an element of policy reconstruction within type-based reasoning. They suggest 3 methods of updating parameter values (approximate gradient ascent, approximate bayesian updating, and exact global updating) and provide a heuristic for selectively updating type's parameters. They then use a MCTS algorithm to evalute the best action. 
\item Hayashi et al --- Particle filtering with DRNNs \cite{Hayashi_et_al2020}. \newline \newline
Hayashi et al. employ type-based reasoning with parameterisation. They encorporate the opponent types and parameters within types into the black-box environment simulator for the POMCP. They then select optimal actions via a MCTS planner at run-time. Their contribution, a deep-recurrent neural network architecture for particle reinvigoration, shows to perform well at updating despite an initially poorly configured model. At runtime, they use an unspecified heuristic for rollout policy. 
\end{enumerate}
Both Hayashi et al and Albrecht and Stone focus on inferring a correctly specified world model, and query this world model with a MCTS style algorithm. However, unlikey Barrett et al, they do not update their actions given this information, and rely solely on the MCTS to adapt, given the inclusion of the opponent model in the rollouts. Given that the existence of types imples an element of prior knowledge, it seems only reasonable to update actions, even if it is only to direct the rollouts, given this information, as opposed to an entire prior-learned policy.  