\section{Ad-hoc coordination: a challenge}
A large proportion of opponent modelling research has been directed at adversarial games. 
This allows a number of assumptions to be made about opposing agents (hence the name `opponent' modelling). The major assumption made is that of a purely adversarial opponent. From this assumption, one can assume that the reward function of an opposing agent is the inverse of their own. Given the assumption of an opponents reward function (or at a higher level, their intentions), one can employ a number of techniques to optimise actions --- for instance, pre-computing equilibria via iterative methods (e.g. fictitous play), self-play (training against oneself) and minimax pruning. These techniques have been used well previously in zero-sum games like Poker, Go and Chess. When one extends to environments which require cooperation as well as coompetetive objectives, these techniques require modification to be applicable. 
\newline \newline
The major effort behind this line of work is to extend opponent modelling techniques to more general problems, or more specifically, open environments. A growing field of research investigates ad-hoc coordination games. These are typically games which are sufficiently simple to be computed by an agent with limited computational power, but incorporate sufficient complexity as to be challenging. Rather more importantly, these games involve self-interested agents (i.e. no common world-model or shared objectives) and allow for competetive agents, however often require cooperation to acheive optimal results. They often involve more than a single other agent, and those other agents often can be deliberatly adversarial or cooperative in nature.
\newline \newline 
This environment shares a number of aspects with open domains: a number of heterogenous agents, exhibiting a variety of intents and abilities in a partially observable (uncertain) environment. Hence, this serves as a strong test-bed for an opponent modelling system designed for an open system and provides a number of state of the art systems to augment and benchmark against. 
\subsection{The level-based foraging domain}
A plausible environment (among a number of others) are variations of the Level-based foraging domain (LBFD) (used by \cite{Barrett2015,Papoudakis2020,Albrecht2019,Hayashi_et_al2020}\footnote{This is not an exhaustive list.}). The LBFD is represented by a two dimensional gridworld with a number of self-interested agents. The gridworld is populated by agents and objects. Each agent and object has a \textit{level}. The game finishes after an arbitrary number of timesteps, or when all objects have been foraged. Agents have 6 discrete actions in each timestep :(north, south, east, west, stay still, load block). All agents more asynchronously, and select a single action per timestep. Agents receive a reward for foraging an object equal to the object's level. Agents cannot forage any object with a higher level than their own. In the case where an object's level is higher than an agent, agents can cooperate in order to forage the object together, providing that the sum of their levels' is greater than that of the object.  
 