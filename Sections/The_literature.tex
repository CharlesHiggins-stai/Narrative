\section{Background}\label{Background}
This section provides an introduction to various basic concepts and intuitions required to understand the context and contribution of this line of work. It starts with an introduction into multi-agent environments and opponent modelling, before discussing various methods of action-selection and their associated benefits and weaknesses, before finally introducing the core algorithms upon which this piece of works builds. 

\subsection{Open environments \& opponent modelling}
As mentioned in Section 1, if the environment is stationary -- that is, the relationship between the state of the environment and the eventual rewards, is fixed, a learning agent can eventually learn an optimal policy: a mapping between state and actions, which maximises discounted future rewards. 
Hence, given enough data, an agent can learn to act optimally, and therefore safely, in such environments. However when the relationship between state and optimal actions change, converging to an optimal policy (at least by the same mechanism) is not feasible. This poses a problem as in reality, most environments exhibit a degree of non-stationarity. For a number of these environments, non-stationarity comes as a result of other agents. Take for example, the challenge of an autonomous vehicle navigating down a corridor. 
\newline \newline
In a single-agent environment, there is a simple optimal policy. Given enough iterations, an agent can learn how best to optimise actions to eventually move (in as few moves as possible) towards the exit. 
\newline \newline
Indeed even given a more complex relationship between optimal action selection and state, like partial observability, or noisy movements (i.e. the relationship between state-action pairs and the sucessive state is probabilistic), the optimal policy should adjust, and will remain fixed. Mathematically speaking, this environment exhibits the markov-property, where firstly, there exists an optimal policy, and secondly, the optimal policy is inferrable from only the current state (i.e. memoryless).
\newline \newline
Consider now, the same environment, but there exists at least one other agent. The optimal policy now depends on the state of the environment and the position of the other agent. Now,reasonably, one could image the other agent as being included in the state --- and indeed, if the other agent has a fixed policy, the environment is still stationary, however if that other agent learns, or updates its policy, or enters and leaves, the environment is fundamentally changing, and so too will the optimal policy.
\newline\newline
This non-stationarity poses problems for agents which are required to select actions safely and robustly. Simply put, if the results of an action depend on the actions taken by another agent, (i.e. the transition function which affects changes in the environment depends on more than a single agent's action), an agent must understand and predict the actions of another agent in order to preempt, or truly understand the likely resulting state. This idea fundamentally underpins opponent modelling --- containing an element of non-stationary and uncertainty within another agent model, and constructing and using these models in order to reduce uncertainty and ultimately act optimally. 

\subsection{Methods of opponent modelling}
There are two distinct parts of opponent modelling --- the first involves constructing an opponent model for a specific opponent (or set of opponents) and the second involves using that opponent model --- incorporating an opponent model into its action selection mechanism. 
\newline \newline
The first challenge is rather difficult to approach, as depending on the constraints and flow of information afforded by different environments and situations, some are better suited than others. 
\newline \newline
Arguably the simplest and most common form of opponent modelling is known as policy reconstruction. An agent attempting a form of policy reconstruction attempts to build an agent's policy, often with little former knowledge and simply building from observations alone. While this can be very effective given multiple observations, in a short period of time, this is unlikely to succeed. However, the policies can be very complex, and can contain elements of learning and constant updating (see for example \cite{Leibo2017} for learning with opponent learning awareness, or \cite{mealing_shapiro_OM_by_expectation_maximisation} for a recent and sophisticated form of policy reconstruction under imperfect information). 
\newline \newline
A common and far swifter form of opponent modelling is type-based reasoning (see \cite{Evalulation_of_adhoc_teamwork_Barrett_Stone_Kraus} for a number of recent well performing methods in a number of challenging environments). In type-based reasoning, an agent model is pre-built based on previous observations with other agents, or supplied by the user. These agent models are then attributed to an actual opponent at the moment of encountering an opponent. This reduces the task of opponent modelling to one of classification rather than one of policy reconstruction, and can allow for very sophisticated agent models to be built and used swiftly with minimal effort and observations necessary. 
\newline \newline
Both of the previous models could be subsymbolic (i.e. no real reasoning capability). In effect, one could view an agent model as a black-box of sorts, and simply apply it to certain situations. The highest level of opponent modelling starts to encroach on a concept known as theory of mind: this is the attribution of latent mental states like beliefs, desires, and intentions to other agents in order to attempt to understand and predict their actions. This has shown to be particularly effective in predicting actions in scenarios involving incomplete information and deliberate deception, however this often leads to recursive cycles of beliefs about other agents beliefs --- i.e. multiple orders of theory of mind.  This tends to be computationally draining and fails to be of practical use in scenarios with limited interaction protocols. 
\newline \newline
The most applicable form of opponent modelling to an open environment, and one which crucially has the capacity to be swift, is that of type-based reasoning. It is pertinent to note at this point that a type-based reasoning form of opponent modelling can easily (and has recently \cite{Albrecht__explioting_causality}) encapsulate a form of policy reconstruction, or even a theory-of-mind agent.
\subsubsection{Ad-hoc coordination}
As the name might suggest, the majority of opponent modelling work has been developed in purely adversarial environments: e.g. Chess, Go, Poker \cite{Brown2017,Brown2020,AlphaGo}. In two player zero sum games, opponent modelling is challenging due to large state spaces and strategy profiles, however the intentions of agents are clear, and the solutions can often be reduced to equilibria and modified MiniMax pruning (as for instance in \cite{Brown2020}). 
\newline \newline
In recent years there has been increasing interest in mixed-motive games, where agents must interact with friendly as well as adversarial agents \cite{Barrett2017}. Here solution concepts are arguably more challenging to derive due to the comparatively larger number of opponents/other agents and their mixed motives. One such challenge is referred to as AD-hoc coordination --- playing a game with mixed motives with unknown agents. In these environments, an agent must discern whether other agents are cooperative, and how they might cooperate and compete to achieve aligned or conflicting objectives.
\newline \newline
There are several similarities in testing grounds of this sort and open environments: large number of agents, agents with unknown motives and reward functions and considerable uncertainty. These ad-hoc coordination environments therefore provide a school of methods and well established challenges to adapt, develop and tune agents capable of performing well in an open environment. 

\subsection{Action-Selection}
In general terms there are two broad approaches to action-selection for autonomous agents: a learning vs a reasoning approach. The former starts with limited knowledge about the environment, and through sucessive trials and learning from actions and rewards, decides to learn a policy. The latter typically requies more domain knowledge, and reasons about actions before they are made in order to meet some objective/reach a desired state. 
\subsubsection{Learning vs Reasoning (high level)}
While this section is deliberately high-level, it makes a crucial point as to the strengths and weaknesses of data-driven learning vs reasoning approaches. 
In general, data-driven approaches can perform well, given sufficent time and resources to learn an optimal policy within a stationary environment. This policy often takes a huge amount of time to learn and develop as the agent learns the effects its actions have upon the environment. However, once learned, typically the execution is very swift --- depending on the exact architecture used times vary --- usually a single look-up in a Q table, or a single forward pass through a neural network. They also require very little expert knowledge to impliment, and given a stationary environment, can learn an optimal policy to maximise rewards. 
\newline \newline
In contrast, the reasoning approach requires very few, if any, learnig iterations before performing well. Rather than learning from scratch, reasoning agents typically have a model of the environment (hence the nominer model-based) which they query, testing out the likely effect of each possible action, and subsequent action. Typically, such agents are referred to as planners. These planning agents search for a chain of actions which take them from their initial state to a percieved goal state. As the search space expands exponentially in the number of sequential actions, this search is costly, and hence, at run time, these agents tend to be very slow. 
\newline\newline
In designing a swift agent therefore, one would require the intial performance of a reasoning agent, however the low-latency and swift response of a learning/data-driven agent. 
\newline \newline
One must also address the concept of robustness, and so the consequential qualities of safety (and subsequent trust, or lack thereof): data-driven or learning agents are hard to predict --- should the environment differ from that of their training, performance degrades significantly. 
Further, such are prone to unpredictable and inexplicable action-selection, limiting their applicability from fields which contain any real element of risk. In contrast, reasoning agents are often deemed 'safer'. 
They typically adapt well to changing circumstances, as their internal model already captures some of the environment's dymanics. 
Furthermore, as such agents they investigate the results of each action before they undertake it, given the assumption that their comprehension of the environment dynamics is correct, they act safely and robustly.



\subsubsection{Reinforcement learning augmented search} \label{RL + Search}
In recent years, the combination of model-based search methods and deep reinforcement learning (via self-play) has achieved super-human performance in a number of benchmark challenges in game playing (adversarial) AI \cite{AlphaGo,Brown2020,Lerer2019}. 
The major difficulties of these domains (Chess, Go, even Poker) is that of a truly enourmous state-space. Research which has acheived noteriety has combined local state-space search with deep neural networks to approximate complex functions. 
\newline \newline
While each of the mentioned papers contribute different novel features, the basic structure of these algorithms remains constant: a sample-based search algorithm augmented with two learning components, commonly referred to as a value network, and a policy network respectively.
\newline \newline
Briefly, these algorithms structure the environment as an extensive form game (a search tree). Nodes represent states, and actions represent transitions from one state to another. Given a simple environment, a planning algorithm can search through the possible states, and select the optimal action in order to maximise rewards. However, given a complex environment, the possible state-space expands exponentially, swiftly rendering a typical search-based solution intractible. To address the issue of an enourmous search space, a \textit{sample-based} search algorithm is employed. 
\newline \newline
A sample based search algorithm constructs a search tree of immediately reachable states (rather than all possible states), given available actions. As there are a huge number of possible sequences of actions, rather than exhaustively searching the entire search-space, a sample-based search tree expands and samples only some branches (possible options), and averages across the expected rewards of immediate actions to select the best action at a given node in the tree at a given time.
\newline \newline
Even with a sampling-based approach, the search space is often still prohibitively large, and so data-driven learning methods can be used to restrict the search space further to improve performance. Two distinct learning components (deep neural netowrks) are typically employed.  
\newline \newline 
Both networks are trained through a combination of supervised learning and reinforcement learning via self-play (playing oneself). At run-time, the policy network guides the search through the search tree (limiting the branching factor from areas of the state-space which are unlikely/unfavourable). The value network truncates the depth of the search tree by approximating the value (i.e. the expected discounted reward achievable) for a given state. In short, the learning parameters allow for a tractable approximation of vast state-space, while allowing for explicit reasoning over the short-term, or immediate search-space. 
\newline \newline
While arguably the most famous result was DeepMind's Go-playing AI AlphaGo \cite{AlphaGo}, which rather dramatically defeated Lee Sidol, both Chess and Go are perfect information games --- the state of the game is certain and visible at all times. In a development of earlier Poker-playing bots, Brown et al. \cite{Brown2020} tackled the problem of imperfect information games (namely head's up no limit texas hold 'em poker) by adapting the notion of state to a public-belief state. In short, they define a mechanism for transforming an imperfect information game into a continuous state-space perfect information game, where the state contains a probabilistic distribution over all agents beliefs (a public-belief state). Through this transformation, a similar algorithm to Silver et al's AlphaGo can be used, with policy and value networks guiding and truncating search.   
\newline \newline
Crucially, this research approaches the problem of large state-spaces rather more from the side of learning as opposed to reasoning. In contrast, a body of work has tackled partially observable environments with an explicit focus on reasoning: planning.  


\subsubsection{(Sample-based) Planning}
 Planning is, in essence, a search problem --- an agent queries an internal model which captures the dynamics of the environment to find a sequence of actions to achieve some goal state or maximise a reward. In complex domains, the search-space swiftly becomes so large as to be computationally intractable, hence the recent focus on model-free forms of learning to solve similar problems. However, model-free approaches tend to be unpredictable. As they have no model to reason about the environment, they can exhibit incorrect and even dangerous behaviour in unforeseen scenarios --- typically there is no reasoning process, but simply a mapping of a state to an action. Given this unpredictability, the application of purely model-free approaches to some domains carries risk and is therefore undesirable. Extending planning to complex domains remains an open question and a valuable and viable direction of enquiry. At present, the most promising area appears to be sample-based planning, which requires exploring only some of the search space online (i.e. at runtime).  

