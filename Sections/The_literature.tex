\section{Background}
There are two main paradigms which combine learning and reasoning in games --- 1) sample-based planning and 2) RL + Search. 
\subsection{(Sample-based) Planning}
 Planning is, in essence, a search problem --- an agent queries an internal model which captures the dynamics of the environment to find a sequence of actions to achieve some goal state or maximise a reward. In complex domains, the search-space swiftly becomes so large as to be computationally intractable, hence the recent focus on model-free forms of learning to solve similar problems. However, model-free approaches tend to be unpredictable. As they have no model to reason about the environment, they can exhibit incorrect and even dangerous behaviour in unforeseen scenarios --- typically there is no reasoning process, but simply a mapping of a state to an action. Given this unpredictability, the application of purely model-free approaches to some domains carries risk and is therefore undesirable. Extending planning to complex domains remains an open question and a valuable and viable direction of enquiry.

\subsubsection{Monte Carlo Tree Search --- an approach to search in a large state-space}
Monte Carlo Tree Search is a sample-based search algorithm which requires a simulation model of the environment to evaluate possible actions and future states in order to converge to an optimal policy online (i.e. re-evaluating from each state). 
Each node represents a possible state, and transitions (edges) are possible (probabilistic) actions. At a given state, an agent traverses a computed tree, selecting actions with a heuristic until reaching a leaf node. At the leaf node, it then randomly selects actions and receives a potential successor state from the transition model to some arbitrary depth. It then evaluates the state and the rewards achieved in the leaf node, and propagates the rewards backwards up the tree. This cycle  (traversal, simulation (rollout) and backpropagation) then continues for a determined period, until a new action is selected, and a new state observed. 
\newline \newline
This sampling-based approach is typically paired with a UCB1 algorithm (upper confidence bounds) which balances exploitable actions with under-explored alternatives, which has shown to converge (eventually) to an optimal policy. 


\subsubsection{Monte Carlo Search in a partially observable environment --- adopting MCTS to partially observable environments}
Despite the success of a sample-based approach, the extension of planning to uncertain (partially observable) environments still poses problems. 
The reason being is that in an $n$-stateful environment, an agent must compute a distribution over $n$ states representing its belief over the true state of the environment. 
It must compute this in addition to the generic planning complexities of all possible actions, transitions and resulting states --- hence unless facing a very trivial problem in an exceedingly simple environment, the exponential complexity of partially observable environments tend to render planners inapplicable. 
\newline \newline
Silver and Veness  \cite{Silver2010} combined a sampling-based approach to belief updating (particle-filtering) with a monte-carlo tree search style algorithm which allows for partially observable planning. Among a number of alterations, each node in the tree is based on an observation history rather than a state, reflects the belief over the state. In effect, Monte Carlo methods are used in a bi-directional manner: forwards to evaluate action selection; backwards to update belief over states. 
\newline \newline
In small state spaces, the belief-state (i.e. distribution over the possible state) can be perfectly calculated by applying Bayes rule --- in large state spaces this can be computationally demanding, and a compact representation of the transition model (in terms of likelhihoods) might not be available. To address this problem Silver and Veness contributed an algorithm, Partially Observable Monte Carlo Planning (POMCP), which uses a particle-filter: generating a number of small unweighted particles, each representing a possible state, and evaluating them based on expected vs received observations. They use this sampling-based approach to update the belief about the likely history at each time-step, and iteratively converge onto the true state.
\newline \newline
As with typical Monte Carlo methods, the sampling approach greatly limits the search space as only (likely) reachable states are evaluated, and the belief over states can be efficiently computed. In short, sampling based methods have shown to allow for swift approximation over complex belief spaces.

\subsubsection{Belief-based planning with MCTS --- Improving on POMCP}  
Despite the theoretical success of POMCP, belief-based planning still poses challenges. 
While known for its efficacy in dealing with large state-spaces, MCTS is limited by the accuracy of the transition model, even with the relatively relaxed requirements as to the nature of the transition model (all that is required is a black-box simulator). Simply put, if the model provided to the planner is not sufficiently expressive as to capture the environment dynamics, despite efficient sampling, the performance of an agent will be poor. 
\newline \newline
In recent years, there have been two notable works which have augmented belief-based planners with a learning capacity in order to overcome an incorrectly specified model. 
\newline \newline
Hayashi et al \cite{Hayashi_et_al2020} augmented a POMCP planner with a deep-recurrent neural network as a mechanism for particle reinvigoration (suggesting possible states to evaluate), which suggested better candidate states (particles) to be computed allowing noisy or incorrectly specified environments to be used without resulting in poor agent performance. They phrased the environment as a black-box simulator, with various parameters to be fitted. This fitting took place online, meaning that the black-box simulator could be tuned online, allowing the world-model to be iteratively updated. Crucially, they applied this to an opponent modelling task, the level-based foraging domain \cite{Papoudakis2020,Barrett2015}, in which agent types were parameters within the black-box simulator. 
\newline \newline
With a similar aim, Katt et al \cite{Katt2017} augmented the notation of a POMDP to take account of a number of extra features, to allow for a Bayesian updating of the environment model --- in effect the transition dynamics were updated as experience increased, allowing for better plans to be made. 
\newline \newline
Finally, Fischer and Tas augmented a MCTS style of planning with information theoretic rewards in a continuous domain. This means in practice increasing an internal reward for reaching states where the agent received more information about the environment. The basic idea behind this is that often as an agent's understanding of a domain increases, the convergence to optimal action selection is swifter. While this was tested in a single-agent domain, the concept of augmenting a sampling-based search procedure with information-based rewards showed promise \cite{Fischer2020}. Despite the domain being marginally different from that of opponent modelling, the concept of explicitly rewarding an agent for looking for information has parallels in opponent exploration. Ultimately, the concept of deliberately manipulating a game state to learn more about an opponent is worthy of investigation, and seems at present somewhat under-explored.

\subsection{Reinforcement learning augmented search}
In recent years, the combination of model-based search methods and deep reinforcement learning (via self-play) has achieved super-human performance in a number of benchmark challenges in game playing (adversarial) AI \cite{AlphaGo,Brown2020,Lerer2019}. These researchers approach the problem of large state-spaces rather more from the side of learning rather than reasoning. \newline \newline
While each of the mentioned papers contribute different novel features, the basic structure of the algorithm remains constant: a sample-based search algorithm augmented with two learning components, commonly referred to as a value network, and a policy network respectively.
\newline \newline
Both networks are trained through a combination of supervised learning and reinforcement learning via self-play (playing oneself). At run-time, the policy network guides the search through the search tree (limiting the branching factor from areas of the state-space which are unlikely/unfavourable). The value network truncates the depth of the search tree by approximating an value for a given state. In short, the learning parameters allow for a tractable approximation of vast state-space, while allowing for explicit reasoning over the short-term, or immediate search-space. 
\newline \newline
While arguably the most famous result was DeepMind's Go-playing AI AlphaGo \cite{AlphaGo}, which rather dramatically defeated Lee Sidol, both Chess and Go are perfect information games --- the state of the game is certain and visible at all times. In a development of earlier Poker-playing bots, Brown et al. \cite{Brown2020} tackled the problem of imperfect information games (namely head's up no limit texas hold 'em poker) by adapting the notion of state to a public-belief state. In short, they define a mechanism for transforming an imperfect information game into a continuous state-space perfect information game, where the state contains a probabilistic distribution over all agents beliefs (a public-belief state). Through this transformation, a similar algorithm to Silver et al's AlphaGo can be used, with policy and value networks guiding and truncating search.   