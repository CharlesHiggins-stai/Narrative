
\section{The building blocks: basic algorithms}
\subsection{Monte Carlo Tree Search --- an approach to search in a large state-space}
Monte Carlo Tree Search is a sample-based search algorithm which requires a simulation model of the environment to evaluate possible actions and future states in order to converge to an optimal policy online (i.e. re-evaluating from each state). 
Each node represents a possible state, and transitions (edges) are possible (probabilistic) actions. At a given state, an agent traverses a computed tree, selecting actions with a heuristic until reaching a leaf node. At the leaf node, it then randomly selects actions and receives a potential successor state from the transition model. This is referred to as a rollout, and continues to some arbitrary depth of tree depending on available resources. It then evaluates the state and the rewards achieved in the leaf node, and propagates the rewards backwards up the tree. This cycle  (traversal, simulation (rollout) and backpropagation) then continues for a determined period, until a new action is selected, and a new state observed. 
\newline \newline
This sampling-based approach is typically paired with a UCB1 algorithm (upper confidence bounds) which balances exploitable actions with under-explored alternatives, which has shown to converge (eventually) to an optimal policy \cite{Ross2011}. 
\newline \newline
This approach, typically when paired with a strong heuristic and an accurate transition/environment simulator, has shown to be an effective and powerful approach to action-selection in complex environments with large search spaces --- this algorithm forms the basis for most of the recent successes in adversarial AI (as described in the previous Section: Section \ref{RL + Search}) 


\subsubsection{Monte Carlo Search in a partially observable environment --- adopting MCTS to partially observable environments}
Despite the success of a sample-based approach, the extension of planning to uncertain (partially observable) environments still poses problems. 
In such an environemt (due to inperfect or incomplete observations), an agent is (initially) uncertain of the actual state. This casuses considerable complexity, and has ramifications for the time and space requirements for computing an optimal policy.
In an $n$-stateful environment, an agent must compute a distribution over $n$ states representing its belief over the true state of the environment. 
It must compute this in addition to the generic planning complexities of all possible actions, transitions and resulting states --- hence unless facing a very trivial problem in an exceedingly simple environment, the exponential complexity of partially observable environments tend to render planners inapplicable. 
\newline \newline
Silver and Veness  \cite{Silver2010} combined a sampling-based approach to belief updating (particle-filtering) with a monte-carlo tree search style algorithm which allows for partially observable planning. Among a number of alterations, each node in the tree is based on an observation history rather than a state, reflecting the agents belief over the state. In effect, Monte Carlo methods are used in a bi-directional manner: forwards to evaluate action selection; backwards to update belief over states. 
\newline \newline
In small state spaces, the belief-state (i.e. distribution over the possible state) can be perfectly calculated by applying Bayes rule --- in large state spaces this can be computationally demanding, and a compact representation of the transition model (in terms of likelhihoods) might not be available. To address this problem Silver and Veness contributed an algorithm, Partially Observable Monte Carlo Planning (POMCP), which uses a particle-filter: generating a number of small unweighted particles, each representing a possible state, and evaluating them based on expected vs received observations. They use this sampling-based approach to update the belief about the likely history at each time-step, and iteratively converge onto the true state.
\newline \newline
As with typical Monte Carlo methods, the sampling approach greatly limits the search space as only (likely) reachable states are evaluated, and the belief over states can be efficiently computed. In short, sampling based methods have shown to allow for swift approximation over complex belief spaces.

\subsubsection{Improving on POMCP --- updating the environment simulation}  
Despite the theoretical success of POMCP, belief-based planning still poses challenges. 
While known for its efficacy in dealing with large state-spaces, MCTS is limited by the accuracy of the transition model. Simply put, if the model of the environment provided to the planner is not sufficiently expressive as to capture the environment dynamics, despite efficient sampling, the performance of an agent will be poor. 
\newline \newline
In recent years, there have been two notable works which have augmented belief-based planners with a learning capacity in order to overcome an incorrectly specified model. 
\newline \newline
Katt et al \cite{Katt2017} augmented the notation of a POMDP to take account of a number of extra features, to allow for a Bayesian updating of the environment model --- in effect the transition dynamics were updated as experience increased in a Bayesian way, allowing for more realistic rollouts and better action selection. In short, they incorportated a learning element into the model of the environment to allow for an incorrectly or incomplete black-box environment simulation. 
\newline \newline
With a similar aim, Hayashi et al \cite{Hayashi_et_al2020} augmented a POMCP planner with a deep-recurrent neural network as a mechanism for particle reinvigoration (suggesting possible states to evaluate), which suggested better candidate states (particles) to be computed allowing noisy or incorrectly specified environments to be used without resulting in poor agent performance. They phrased the environment simulator as a black-box with various parameters which required fitting. This fitting took place as experience grew, meaning that the black-box simulator could be tuned online, allowing the world-model to be iteratively updated. Crucially, they applied this to an opponent modelling task, the level-based foraging domain \cite{Papoudakis2020,Barrett2015}, in which agent types were parameters within the black-box simulator. 

\subsubsection{Improving MCTS --- information theoretic rewards}
Finally, Fischer and Tas augmented a MCTS style of planning with information theoretic rewards in a continuous domain. This means in practice increasing an internal reward for reaching states where the agent received more information about the environment. The basic idea behind this is that often as an agent's understanding of a domain increases, the convergence to optimal action selection is swifter. While this was tested in a single-agent domain, the concept of augmenting a sampling-based search procedure with information-based rewards showed promise \cite{Fischer2020}. Despite the domain being marginally different from that of opponent modelling, the concept of explicitly rewarding an agent for looking for information has parallels in opponent exploration. Ultimately, the concept of deliberately manipulating a game state to learn more about an opponent is worthy of investigation, and seems at present somewhat under-explored.

\subsubsection{Incoporating agent models into MCTS}
As mentioned previously, a MCTS involves several stages: 1) traversing a tree via a best-first (heuristic) until reaching a leaf node; 2) Upon reaching a leaf node, it performs a rollout, which involves selecting an action, and sampling an environment simulator for a possible next state and expected reward. It continues this rollout to an arbitrary depth. 3) Finally, having reached a maximum deapth, the reward is propagated back up the tree to the root node.
\newline \newline 
In a multi-agent environment, the transition function responsible for mapping one state to another depends on a joint actions taken by all agents. Hence, in a rollout, the simulator must assume a joint action (i.e. infer/suggest likely actions taken by \textit{other} agents). In this way, agent models are integrated into a MCTS via the simulation phase. In simple terms, they are implicitly included in the environment model. In both \cite{Hayashi_et_al2020} and \cite{Albrecht_stone_2019} agent models are computed and viewed as parameters to be tuned in this environment, thus providing an interesting middle ground between a `global' approach, in which an agent simply learns a single generative model of  the environment from scratch (no opponent models), and a typical opponent modelling approach, which requires distict agent models and environment models.
\newline \newline
This method also has benefits as it translates the problem of opponent modelling into one of state-space search, allowing for the incorporation of policy/value netowrks and/or sampling-based planners to address complexities of opponent modelling, which often result in large (probabilistic) state-spaces.