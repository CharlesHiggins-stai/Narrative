\section{Introduction}
% \begin{itemize}
%     \item Opponent modelling roughly aims to compute a model of other agents in order to optimize action selection.
%     \item While a well developed field, methods have yet to extend to open environments. 
%     \item For an open environment, methods need to be swift, scalable and robust. 
%     \item This document looks at the swift component --- in short, RL agents are swift to execute, however take a while to train and can be unpredictable when dynamics shift drastically from training. Model-based reasoning approaches (planning) tends to be safer and predictable, but slow to execute. 
%     \item This document briefly discusses the state of the art in combining search and learning, and identifies gaps in the literature. 
% \end{itemize} 

It is a truth universally acknowledged that a machine-learning agent in possession of sufficient data must be in want of converging to optimal rewards. However, given a non-stationary environment, such convergence is often not possible. Any environment where the relationship between state and optimal actions change over time is deemed non-stationary, hence, any open environment where other agents might enter or leave or learn or update their policies in response to stimuli is deemed non-stationary. 
\newline \newline
Despite prodigious advances in recent years in games posing challenges through enormous state-spaces, requiring strategic thinking and planning, multi-agent systems pose a number of open questions. Crucially, a number of these problems revolve around safe and trustworthy action-selection: in short, how can an autonomous agent act in a way a human might trust? If an agent is in a non-stationary environment, an agent cannot be sure of the results of its action before making it, and hence, cannot reasonably act safely. In the pursuit of safe and trusted AI therefore methods to address this non-stationary are required. 
\newline \newline
One assumption one can reasonably make is that the non-stationary element of a multi-agent environment is (for the most part) due to other agents, as opposed to an intrinsic part of the wider world. One can then combine a model of another agent, designed to capture the non-stationary component of the environment, with a distinct (and stationary) environment model. Hence, an agent can control, or at the very least limit the negative effects of non-stationarity, and act with increasing certainty. This is the fundamental assumption made by a group of methods which are loosely referred to as opponent modelling. Opponent modelling methods construct a distinct internal model of other agents, and use this model in to adapt their actions to this other agent. 
\newline \newline
Opponent modelling, particularly within the area of game-playing AI, has had notable successes in closed environments, however open environments still remain an open question (pun intended) \cite{Albrecht_stone_2019}. In order to combat some of the challenges posed by openness, an agent must learn not only to adapt to large numbers of opponents, but an agent must be able to perform opponent modelling \textit{swiftly} and \textit{robustly}: it must be able to swiftly reason over the nature of another agent (i.e. its intentions/reward function, observations, and likely actions) and, equally swiftly, use this information to act safely, and ideally, optimally. 
\newline \newline
Traditional or regular methods of action seleciton for intelligent agents tend to fall into one of two distinct camps: the reasoning or learning approach. The reasoning approach tends to conform to a state-space search style approach, and so fails to perform well in large or dynamic environments. In contrast, model-free learning agents tend to perform well in complex environments, however require a large training time (i.e. they have a low degree of sample-efficiency). Both therefore have significant flaws, and therefore neither are able to meet the criteria of a swift and robust agent (N.B. this line of reasoning is further explained in Section \ref{Background}). 
\newline \newline
To meet this objective: a trustworthy method of action selection which is swift and robust, we propose a multi-agent extension of policy-augmented partially observable monte-carlo planning agent. This is a model-based agent which applies a simulation based approach for large uncertain environments, with an opponent-weighted policy-augmented search component. This architecture builds upon the state of the art in opponent modelling and game-playing AI, and integrates a data-driven learning component within a model-based reasoning agent architecture which proves to outperform both in isolation, and other state of the art baseline methods, given an uncertain environment. To the best of the author's knowledge, this is the first hybrid data-driven/model-based system to have been applied to an opponent modelling task. 
\newline \newline
The remainder of this paper is structured as follows: Section \ref{Background} introduces the background theory for opponent modelling, data-driven action-selection in game-playing AI and reasoning approaches. 
Section \ref{MDP/POMDP} formalises the system. 
Section introduces our contribution
Section briefly describes the simulation environment used to extract results and the heuristic opponent agents
Section lists the upcoming experiments and expected results, 
Section concludes. 