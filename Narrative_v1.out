\BOOKMARK [1][-]{section.1}{Introduction}{}% 1
\BOOKMARK [1][-]{section.2}{Background}{}% 2
\BOOKMARK [2][-]{subsection.2.1}{Open environments \046 opponent modelling}{section.2}% 3
\BOOKMARK [2][-]{subsection.2.2}{Methods of opponent modelling}{section.2}% 4
\BOOKMARK [3][-]{subsubsection.2.2.1}{Ad-hoc coordination}{subsection.2.2}% 5
\BOOKMARK [2][-]{subsection.2.3}{Action-Selection}{section.2}% 6
\BOOKMARK [3][-]{subsubsection.2.3.1}{Learning vs Reasoning \(high level\)}{subsection.2.3}% 7
\BOOKMARK [3][-]{subsubsection.2.3.2}{Reinforcement learning augmented search}{subsection.2.3}% 8
\BOOKMARK [1][-]{section.3}{Formalisation}{}% 9
\BOOKMARK [2][-]{subsection.3.1}{Partially observable Markov Decision Process POMDP}{section.3}% 10
\BOOKMARK [2][-]{subsection.3.2}{Parameterisation of }{section.3}% 11
\BOOKMARK [3][-]{subsubsection.3.2.1}{Type-based reasoning}{subsection.3.2}% 12
\BOOKMARK [1][-]{section.4}{The building blocks: basic algorithms}{}% 13
\BOOKMARK [2][-]{subsection.4.1}{Particle Filtering: iterative approximate Bayesian Updates}{section.4}% 14
\BOOKMARK [2][-]{subsection.4.2}{Monte Carlo Tree Search \204 an approach to search in a large state-space}{section.4}% 15
\BOOKMARK [3][-]{subsubsection.4.2.1}{Aadapting Monte Carlo Tree Search to partially observable environments}{subsection.4.2}% 16
\BOOKMARK [3][-]{subsubsection.4.2.2}{Improving on POMCP \204 updating the environment simulation}{subsection.4.2}% 17
\BOOKMARK [3][-]{subsubsection.4.2.3}{Incoporating agent models into MCTS}{subsection.4.2}% 18
\BOOKMARK [1][-]{section.5}{Shortcomings of related/current work}{}% 19
\BOOKMARK [2][-]{subsection.5.1}{Shortcomings of RL + Search}{section.5}% 20
\BOOKMARK [2][-]{subsection.5.2}{Shortcomings of sample-based planning}{section.5}% 21
\BOOKMARK [1][-]{section.6}{Opponent specific policy guidance}{}% 22
\BOOKMARK [2][-]{subsection.6.1}{Contribution}{section.6}% 23
\BOOKMARK [1][-]{section.7}{Experimental Evaluation}{}% 24
\BOOKMARK [2][-]{subsection.7.1}{Ad-hoc coordination: a challenge}{section.7}% 25
\BOOKMARK [3][-]{subsubsection.7.1.1}{The level-based foraging domain}{subsection.7.1}% 26
\BOOKMARK [2][-]{subsection.7.2}{Baselines in ad-hoc coordination}{section.7}% 27
\BOOKMARK [2][-]{subsection.7.3}{Experiments}{section.7}% 28
\BOOKMARK [2][-]{subsection.7.4}{Evaluation/Thoughts}{section.7}% 29
\BOOKMARK [1][-]{section.8}{Future work \046 Conclusions}{}% 30
\BOOKMARK [2][-]{subsection.8.1}{Closing comments}{section.8}% 31
\BOOKMARK [1][-]{appendix.A}{Opponent Types description}{}% 32
\BOOKMARK [1][-]{appendix.B}{Policy learning hyperparameters}{}% 33
\BOOKMARK [1][-]{appendix.C}{Proof of convergence for MCTS and PF}{}% 34
